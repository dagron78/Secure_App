# Backend Architecture Plan for CDSA (Confidential Data Steward Agent)

## Executive Summary

This document outlines the comprehensive backend architecture for the Confidential Data Steward Agent (CDSA), a secure, auditable, locally-controlled agentic interface for handling sensitive data in regulated industries.

**Technology Stack:**
- **Backend Framework**: Python 3.11+ with FastAPI
- **Database**: PostgreSQL 15+ (primary data store)
- **Cache/Queue**: Redis 7+ (caching, sessions, task queues)
- **AI Framework**: LangChain + LlamaIndex (unified LLM interface)
- **Vector DB**: pgvector (PostgreSQL extension for document embeddings)
- **Background Tasks**: Celery with Redis broker
- **API Documentation**: OpenAPI/Swagger (auto-generated by FastAPI)

---

## 1. System Architecture Overview

```mermaid
graph TB
    subgraph "Client Layer"
        UI[React Frontend]
    end
    
    subgraph "API Gateway"
        NGINX[NGINX Reverse Proxy]
        CORS[CORS Middleware]
    end
    
    subgraph "Application Layer"
        API[FastAPI Server]
        WS[WebSocket/SSE Handler]
        AUTH[Auth Middleware]
        RBAC[RBAC Engine]
    end
    
    subgraph "Business Logic"
        CHAT[Chat Service]
        TOOLS[Tool Execution Engine]
        APPROVAL[Approval Workflow]
        AUDIT[Audit Logger]
        VAULT[Secrets Manager]
        DOCS[Document Service]
        NOTIFY[Notification Service]
    end
    
    subgraph "AI/ML Layer"
        LLM[LangChain Router]
        LOCAL[Local Models - Ollama]
        CLOUD[Cloud APIs]
        RAG[RAG Pipeline]
        EMBED[Embeddings Service]
    end
    
    subgraph "Data Layer"
        PG[(PostgreSQL)]
        REDIS[(Redis)]
        VECTOR[(pgvector)]
        FILES[File Storage]
    end
    
    UI --> NGINX
    NGINX --> API
    API --> AUTH
    AUTH --> RBAC
    API --> WS
    WS --> CHAT
    CHAT --> TOOLS
    CHAT --> LLM
    TOOLS --> APPROVAL
    TOOLS --> AUDIT
    CHAT --> VAULT
    CHAT --> DOCS
    DOCS --> RAG
    RAG --> EMBED
    RAG --> VECTOR
    LLM --> LOCAL
    LLM --> CLOUD
    TOOLS --> PG
    AUDIT --> PG
    VAULT --> PG
    DOCS --> PG
    CHAT --> REDIS
    TOOLS --> REDIS
    NOTIFY --> REDIS
    APPROVAL --> NOTIFY
    DOCS --> NOTIFY
    TOOLS --> NOTIFY
    VAULT --> NOTIFY
```

---

## 2. Database Schema Design

### 2.1 Core Tables

```sql
-- Users and Authentication
CREATE TABLE users (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    username VARCHAR(255) UNIQUE NOT NULL,
    email VARCHAR(255) UNIQUE NOT NULL,
    password_hash VARCHAR(255) NOT NULL,
    role VARCHAR(50) NOT NULL CHECK (role IN ('ANALYST', 'MANAGER', 'ADMIN')),
    is_active BOOLEAN DEFAULT true,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

-- Sessions
CREATE TABLE sessions (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    user_id UUID REFERENCES users(id) ON DELETE CASCADE,
    session_token VARCHAR(255) UNIQUE NOT NULL,
    expires_at TIMESTAMP NOT NULL,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

-- Chat Messages
CREATE TABLE messages (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    session_id UUID,
    user_id UUID REFERENCES users(id),
    author VARCHAR(20) NOT NULL CHECK (author IN ('user', 'agent', 'system')),
    content TEXT,
    metadata JSONB,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    INDEX idx_messages_session (session_id),
    INDEX idx_messages_user (user_id)
);

-- Tools Registry
CREATE TABLE tools (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    name VARCHAR(255) UNIQUE NOT NULL,
    description TEXT NOT NULL,
    input_schema JSONB NOT NULL,
    output_schema JSONB NOT NULL,
    required_role VARCHAR(50),
    requires_approval BOOLEAN DEFAULT false,
    is_enabled BOOLEAN DEFAULT true,
    implementation_module VARCHAR(255) NOT NULL,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

-- Tool Execution History
CREATE TABLE tool_executions (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    tool_id UUID REFERENCES tools(id),
    user_id UUID REFERENCES users(id),
    session_id UUID,
    input_args JSONB NOT NULL,
    output_result JSONB,
    status VARCHAR(50) NOT NULL,
    execution_time_ms INTEGER,
    error_message TEXT,
    is_cached BOOLEAN DEFAULT false,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    INDEX idx_tool_exec_user (user_id),
    INDEX idx_tool_exec_tool (tool_id)
);

-- Approval Requests
CREATE TABLE approval_requests (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    requester_id UUID REFERENCES users(id),
    approver_id UUID REFERENCES users(id),
    tool_execution_id UUID REFERENCES tool_executions(id),
    tool_name VARCHAR(255) NOT NULL,
    tool_args JSONB NOT NULL,
    status VARCHAR(50) NOT NULL CHECK (status IN ('PENDING', 'APPROVED', 'REJECTED')),
    decision_reason TEXT,
    requested_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    decided_at TIMESTAMP,
    INDEX idx_approval_status (status),
    INDEX idx_approval_requester (requester_id)
);

-- Audit Events
CREATE TABLE audit_events (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    event_type VARCHAR(100) NOT NULL,
    user_id UUID REFERENCES users(id),
    session_id UUID,
    details JSONB NOT NULL,
    ip_address INET,
    user_agent TEXT,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    INDEX idx_audit_type (event_type),
    INDEX idx_audit_user (user_id),
    INDEX idx_audit_created (created_at)
);

-- Secrets Vault
CREATE TABLE secrets (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    name VARCHAR(255) UNIQUE NOT NULL,
    description TEXT,
    encrypted_value BYTEA NOT NULL,
    encryption_key_id VARCHAR(255) NOT NULL,
    created_by UUID REFERENCES users(id),
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    last_accessed_at TIMESTAMP
);

-- Documents
CREATE TABLE documents (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    title VARCHAR(500) NOT NULL,
    type VARCHAR(50) NOT NULL,
    summary TEXT,
    classification VARCHAR(50) NOT NULL,
    content TEXT,
    file_path VARCHAR(500),
    metadata JSONB,
    indexed BOOLEAN DEFAULT false,
    created_by UUID REFERENCES users(id),
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    INDEX idx_documents_type (type),
    INDEX idx_documents_classification (classification)
);

-- Document Embeddings (using pgvector)
CREATE TABLE document_embeddings (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    document_id UUID REFERENCES documents(id) ON DELETE CASCADE,
    chunk_index INTEGER NOT NULL,
    chunk_text TEXT NOT NULL,
    embedding vector(1536),
    metadata JSONB,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    INDEX idx_embeddings_document (document_id)
);

-- Create vector similarity index
CREATE INDEX ON document_embeddings USING ivfflat (embedding vector_cosine_ops) WITH (lists = 100);

-- LLM Models Registry
CREATE TABLE llm_models (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    model_id VARCHAR(255) UNIQUE NOT NULL,
    name VARCHAR(255) NOT NULL,
    provider VARCHAR(50) NOT NULL,
    family VARCHAR(100),
    quantization VARCHAR(50),
    size_gb DECIMAL(10,2),
    context_window INTEGER,
    is_local BOOLEAN DEFAULT false,
    is_enabled BOOLEAN DEFAULT true,
    configuration JSONB,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

-- Cache for tool results
CREATE TABLE tool_result_cache (
    cache_key VARCHAR(255) PRIMARY KEY,
    tool_name VARCHAR(255) NOT NULL,
    input_hash VARCHAR(64) NOT NULL,
    result JSONB NOT NULL,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    expires_at TIMESTAMP NOT NULL,
    access_count INTEGER DEFAULT 0,
    last_accessed_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    INDEX idx_cache_expires (expires_at)
);

-- Notifications for real-time updates
CREATE TABLE notifications (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    user_id UUID REFERENCES users(id) ON DELETE CASCADE,
    type VARCHAR(100) NOT NULL,
    title VARCHAR(500) NOT NULL,
    message TEXT NOT NULL,
    data JSONB,
    priority VARCHAR(20) DEFAULT 'normal' CHECK (priority IN ('low', 'normal', 'high', 'urgent')),
    is_read BOOLEAN DEFAULT false,
    read_at TIMESTAMP,
    expires_at TIMESTAMP,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    INDEX idx_notifications_user (user_id),
    INDEX idx_notifications_unread (user_id, is_read, created_at),
    INDEX idx_notifications_type (type)
);

-- User notification preferences
CREATE TABLE notification_preferences (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    user_id UUID REFERENCES users(id) ON DELETE CASCADE,
    notification_type VARCHAR(100) NOT NULL,
    enabled BOOLEAN DEFAULT true,
    delivery_method VARCHAR(50) DEFAULT 'realtime',
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    UNIQUE(user_id, notification_type)
);
```

### 2.2 Indexes and Performance Optimization

```sql
-- Composite indexes for common queries
CREATE INDEX idx_messages_session_created ON messages(session_id, created_at DESC);
CREATE INDEX idx_audit_user_created ON audit_events(user_id, created_at DESC);
CREATE INDEX idx_tool_exec_status_created ON tool_executions(status, created_at DESC);

-- Partitioning for audit_events (by month)
CREATE TABLE audit_events_partition_template (
    LIKE audit_events INCLUDING ALL
) PARTITION BY RANGE (created_at);
```

---

## 3. API Endpoints Design

### 3.1 Authentication & Authorization

```python
POST   /api/v1/auth/register          # User registration
POST   /api/v1/auth/login             # User login
POST   /api/v1/auth/logout            # User logout
POST   /api/v1/auth/refresh           # Refresh token
GET    /api/v1/auth/me                # Get current user
PUT    /api/v1/auth/me                # Update current user
```

### 3.2 Chat & Streaming

```python
POST   /api/v1/chat/stream            # Streaming chat (SSE)
POST   /api/v1/chat/continue          # Continue after approval
GET    /api/v1/chat/sessions          # List user sessions
GET    /api/v1/chat/sessions/{id}     # Get session details
DELETE /api/v1/chat/sessions/{id}     # Delete session
POST   /api/v1/chat/export            # Export session data
```

### 3.3 Tools Management

```python
GET    /api/v1/tools                  # List all tools
GET    /api/v1/tools/{id}             # Get tool details
POST   /api/v1/tools                  # Register new tool (admin)
PUT    /api/v1/tools/{id}             # Update tool (admin)
DELETE /api/v1/tools/{id}             # Delete tool (admin)
GET    /api/v1/tools/{id}/executions  # Get tool execution history
POST   /api/v1/tools/{id}/execute     # Direct tool execution
```

### 3.4 Approvals

```python
GET    /api/v1/approvals              # List pending approvals
GET    /api/v1/approvals/{id}         # Get approval details
POST   /api/v1/approvals/{id}/decide  # Approve/reject
GET    /api/v1/approvals/my-requests  # User's approval requests
```

### 3.5 Audit & Compliance

```python
GET    /api/v1/audit/events           # List audit events
GET    /api/v1/audit/events/{id}      # Get event details
POST   /api/v1/audit/report           # Generate compliance report
GET    /api/v1/audit/report/{id}      # Download report
GET    /api/v1/audit/stats            # Get audit statistics
```

### 3.6 Secrets Vault

```python
GET    /api/v1/vault/secrets          # List secrets (metadata only)
POST   /api/v1/vault/secrets          # Add secret
GET    /api/v1/vault/secrets/{id}     # Get secret value (audited)
PUT    /api/v1/vault/secrets/{id}     # Update secret
DELETE /api/v1/vault/secrets/{id}     # Delete secret
```

### 3.7 Documents & RAG

```python
GET    /api/v1/documents              # List documents
POST   /api/v1/documents              # Upload document
GET    /api/v1/documents/{id}         # Get document
PUT    /api/v1/documents/{id}         # Update document
DELETE /api/v1/documents/{id}         # Delete document
POST   /api/v1/documents/{id}/index   # Index document for RAG
POST   /api/v1/documents/search       # Search documents
POST   /api/v1/documents/query        # Query with RAG
```

### 3.8 LLM Gateway

```python
GET    /api/v1/llm/models             # List available models
GET    /api/v1/llm/models/{id}        # Get model details
POST   /api/v1/llm/models/{id}/test   # Test model connection
PUT    /api/v1/llm/config             # Update LLM config
GET    /api/v1/llm/status             # Get gateway status
```

### 3.9 Real-Time Notifications

```python
GET    /api/v1/notifications/stream   # SSE stream for real-time notifications
GET    /api/v1/notifications          # List user's notifications
GET    /api/v1/notifications/{id}     # Get notification details
PUT    /api/v1/notifications/{id}/read # Mark notification as read
PUT    /api/v1/notifications/mark-all-read # Mark all as read
DELETE /api/v1/notifications/{id}     # Delete notification
GET    /api/v1/notifications/preferences # Get notification preferences
PUT    /api/v1/notifications/preferences # Update notification preferences
```

---

## 4. Core Components Architecture

### 4.1 Chat Service (`app/services/chat_service.py`)

**Responsibilities:**
- Handle streaming chat responses
- Manage conversation context
- Route to appropriate LLM
- Handle tool calls and approvals
- Manage context window limits

**Key Methods:**
```python
async def stream_chat_response(
    user_input: str,
    user: User,
    session_id: str,
    history: List[Message],
    llm_config: LLMConfig
) -> AsyncIterator[StreamChunk]

async def continue_after_approval(
    approval: ApprovalRequest,
    user: User,
    session_id: str
) -> AsyncIterator[StreamChunk]

async def manage_context_window(
    messages: List[Message],
    limit: int
) -> Tuple[List[Message], bool]
```

### 4.2 Tool Execution Engine (`app/services/tool_engine.py`)

**Responsibilities:**
- Execute tools with validation
- Enforce RBAC policies
- Handle caching
- Manage approval workflow
- Execute tool implementations

**Architecture:**
```python
class ToolEngine:
    def __init__(self):
        self.registry = ToolRegistry()
        self.executor = ToolExecutor()
        self.cache = CacheManager()
        self.rbac = RBACEngine()
    
    async def execute_tool(
        self,
        tool_name: str,
        args: dict,
        user: User,
        session_id: str
    ) -> ToolResult:
        # 1. Validate tool exists
        # 2. Check RBAC permissions
        # 3. Check cache
        # 4. Check if approval needed
        # 5. Execute or queue for approval
        # 6. Audit logging
        pass
```

**Tool Implementation Interface:**
```python
from abc import ABC, abstractmethod

class BaseTool(ABC):
    name: str
    description: str
    input_schema: dict
    output_schema: dict
    required_role: Optional[str]
    requires_approval: bool
    
    @abstractmethod
    async def execute(self, args: dict, context: ToolContext) -> dict:
        pass
    
    async def validate_input(self, args: dict) -> bool:
        pass
    
    async def on_success(self, result: dict, context: ToolContext):
        pass
    
    async def on_error(self, error: Exception, context: ToolContext):
        pass
```

### 4.3 Approval Workflow (`app/services/approval_service.py`)

**Workflow States:**
```python
class ApprovalStatus(str, Enum):
    PENDING = "PENDING"
    APPROVED = "APPROVED"
    REJECTED = "REJECTED"
    EXPIRED = "EXPIRED"

class ApprovalService:
    async def create_approval_request(
        self,
        tool_call: ToolCall,
        requester: User
    ) -> ApprovalRequest
    
    async def decide_approval(
        self,
        request_id: UUID,
        approver: User,
        decision: ApprovalStatus,
        reason: Optional[str]
    ) -> ApprovalRequest
    
    async def get_pending_approvals(
        self,
        user: User
    ) -> List[ApprovalRequest]
    
    async def expire_old_approvals(self):
        # Background task to expire requests
        pass
```

### 4.4 Audit Logger (`app/services/audit_service.py`)

**Event Types:**
```python
class AuditEventType(str, Enum):
    USER_QUERY = "USER_QUERY"
    TOOL_CALL_INITIATED = "TOOL_CALL_INITIATED"
    TOOL_CALL_COMPLETED = "TOOL_CALL_COMPLETED"
    SECURITY_ALERT = "SECURITY_ALERT"
    APPROVAL_REQUESTED = "APPROVAL_REQUESTED"
    APPROVAL_DECISION = "APPROVAL_DECISION"
    # ... etc
```

**Implementation:**
```python
class AuditService:
    async def log_event(
        self,
        event_type: AuditEventType,
        user: User,
        details: dict,
        session_id: Optional[str] = None
    ):
        # 1. Create event record
        # 2. Store in database
        # 3. Optionally send to external SIEM
        pass
    
    async def generate_compliance_report(
        self,
        user: User,
        start_date: datetime,
        end_date: datetime
    ) -> ComplianceReport:
        pass
```

### 4.5 Secrets Manager (`app/services/vault_service.py`)

**Security Approach:**
- Encryption at rest using Fernet (symmetric encryption)
- Master key stored in environment or KMS
- Key rotation support
- Access logging for all operations

```python
class VaultService:
    def __init__(self):
        self.encryption_key = self._load_master_key()
        self.cipher = Fernet(self.encryption_key)
    
    async def store_secret(
        self,
        name: str,
        value: str,
        description: str,
        user: User
    ) -> Secret:
        encrypted_value = self.cipher.encrypt(value.encode())
        # Store in database
        pass
    
    async def retrieve_secret(
        self,
        secret_id: UUID,
        user: User
    ) -> str:
        # 1. Check permissions
        # 2. Log access
        # 3. Decrypt and return
        pass
    
    async def rotate_key(self):
        # Re-encrypt all secrets with new key
        pass
```

### 4.6 Document Service & RAG (`app/services/document_service.py`)

**Pipeline:**
```python
class DocumentService:
    def __init__(self):
        self.embeddings = OpenAIEmbeddings()  # or local model
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=1000,
            chunk_overlap=200
        )
    
    async def index_document(
        self,
        document: Document
    ) -> bool:
        # 1. Extract text
        # 2. Split into chunks
        # 3. Generate embeddings
        # 4. Store in pgvector
        pass
    
    async def semantic_search(
        self,
        query: str,
        top_k: int = 5
    ) -> List[DocumentChunk]:
        # 1. Generate query embedding
        # 2. Vector similarity search
        # 3. Return relevant chunks
        pass
    
    async def query_with_rag(
        self,
        query: str,
        llm: BaseLLM
    ) -> str:
        # 1. Retrieve relevant documents
        # 2. Construct prompt with context
        # 3. Generate response
        pass
```

### 4.7 LLM Integration (`app/services/llm_service.py`)

**Unified Interface:**
```python
from langchain.llms.base import BaseLLM
from langchain.chat_models import ChatOpenAI, ChatAnthropic
from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler

class LLMService:
    def __init__(self):
        self.models = {}
        self._initialize_models()
    
    def _initialize_models(self):
        # Local models via Ollama
        self.models['llama3-local'] = Ollama(
            model="llama3:8b",
            base_url="http://localhost:11434"
        )
        
        # Cloud APIs
        self.models['gpt-4'] = ChatOpenAI(
            model="gpt-4-turbo-preview",
            streaming=True
        )
        
        self.models['claude-3'] = ChatAnthropic(
            model="claude-3-opus-20240229"
        )
    
    async def stream_response(
        self,
        model_id: str,
        messages: List[Message],
        tools: List[Tool]
    ) -> AsyncIterator[str]:
        llm = self.models.get(model_id)
        # Stream response with tool calling support
        pass
```

### 4.8 Real-Time Notification Service (`app/services/notification_service.py`)

**Purpose**: Provide real-time, multi-user notifications for system events, approvals, document processing, and administrative updates.

**Architecture:**
```python
from typing import Dict, Set, AsyncIterator
from enum import Enum
import asyncio
from collections import defaultdict

class NotificationType(str, Enum):
    APPROVAL_REQUESTED = "APPROVAL_REQUESTED"
    APPROVAL_DECISION = "APPROVAL_DECISION"
    DOCUMENT_INDEXED = "DOCUMENT_INDEXED"
    DOCUMENT_PROCESSING_FAILED = "DOCUMENT_PROCESSING_FAILED"
    TOOL_ADDED = "TOOL_ADDED"
    TOOL_UPDATED = "TOOL_UPDATED"
    TOOL_REMOVED = "TOOL_REMOVED"
    SECURITY_ALERT = "SECURITY_ALERT"
    VAULT_SECRET_ADDED = "VAULT_SECRET_ADDED"
    VAULT_SECRET_ACCESSED = "VAULT_SECRET_ACCESSED"
    SYSTEM_MAINTENANCE = "SYSTEM_MAINTENANCE"
    LONG_RUNNING_TASK_COMPLETED = "LONG_RUNNING_TASK_COMPLETED"

class NotificationPriority(str, Enum):
    LOW = "low"
    NORMAL = "normal"
    HIGH = "high"
    URGENT = "urgent"

class NotificationService:
    def __init__(self):
        # Active SSE connections per user
        self._connections: Dict[str, Set[asyncio.Queue]] = defaultdict(set)
        self._redis = None  # Redis pub/sub for multi-instance support
    
    async def initialize_redis(self, redis_url: str):
        """Initialize Redis pub/sub for multi-instance notification delivery"""
        import redis.asyncio as aioredis
        self._redis = await aioredis.from_url(redis_url)
        # Subscribe to notification channel
        asyncio.create_task(self._redis_subscriber())
    
    async def _redis_subscriber(self):
        """Listen for notifications from other backend instances"""
        pubsub = self._redis.pubsub()
        await pubsub.subscribe("notifications")
        
        async for message in pubsub.listen():
            if message["type"] == "message":
                notification = json.loads(message["data"])
                await self._deliver_to_local_connections(notification)
    
    async def register_connection(
        self,
        user_id: str
    ) -> AsyncIterator[dict]:
        """Register an SSE connection for a user and yield notifications"""
        queue = asyncio.Queue()
        self._connections[user_id].add(queue)
        
        try:
            # Send initial connection success message
            yield {
                "event": "connected",
                "data": {
                    "message": "Notification stream connected",
                    "timestamp": datetime.utcnow().isoformat()
                }
            }
            
            # Stream notifications as they arrive
            while True:
                notification = await queue.get()
                yield {
                    "event": "notification",
                    "data": notification
                }
        finally:
            # Clean up connection
            self._connections[user_id].discard(queue)
            if not self._connections[user_id]:
                del self._connections[user_id]
    
    async def send_notification(
        self,
        user_id: str,
        notification_type: NotificationType,
        title: str,
        message: str,
        data: Optional[dict] = None,
        priority: NotificationPriority = NotificationPriority.NORMAL,
        persist: bool = True
    ) -> Notification:
        """
        Send a notification to a user
        
        Args:
            user_id: Target user ID
            notification_type: Type of notification
            title: Short notification title
            message: Detailed message
            data: Additional structured data
            priority: Notification priority level
            persist: Whether to save to database for retrieval
        """
        notification = Notification(
            id=str(uuid.uuid4()),
            user_id=user_id,
            type=notification_type,
            title=title,
            message=message,
            data=data or {},
            priority=priority,
            created_at=datetime.utcnow().isoformat()
        )
        
        # Persist to database if requested
        if persist:
            await self._save_to_database(notification)
        
        # Publish to Redis for multi-instance delivery
        if self._redis:
            await self._redis.publish(
                "notifications",
                json.dumps(notification.dict())
            )
        
        # Deliver to local connections
        await self._deliver_to_local_connections(notification.dict())
        
        return notification
    
    async def _deliver_to_local_connections(self, notification: dict):
        """Deliver notification to all active SSE connections for the user"""
        user_id = notification["user_id"]
        if user_id in self._connections:
            for queue in self._connections[user_id]:
                await queue.put(notification)
    
    async def send_broadcast_notification(
        self,
        notification_type: NotificationType,
        title: str,
        message: str,
        target_roles: Optional[List[str]] = None,
        data: Optional[dict] = None,
        priority: NotificationPriority = NotificationPriority.NORMAL
    ):
        """
        Send a notification to all users or users with specific roles
        
        Useful for system-wide announcements, new tool additions, etc.
        """
        # Get target users based on roles
        users = await self._get_users_by_roles(target_roles)
        
        # Send to each user
        for user in users:
            await self.send_notification(
                user_id=user.id,
                notification_type=notification_type,
                title=title,
                message=message,
                data=data,
                priority=priority,
                persist=True
            )
    
    async def _save_to_database(self, notification: Notification):
        """Save notification to database for later retrieval"""
        # Check user preferences first
        preferences = await self._get_user_preferences(notification.user_id)
        if not preferences.get(notification.type, {}).get("enabled", True):
            return  # User has disabled this notification type
        
        # Save to database
        async with AsyncSession() as session:
            db_notification = NotificationModel(**notification.dict())
            session.add(db_notification)
            await session.commit()
    
    async def get_user_notifications(
        self,
        user_id: str,
        limit: int = 50,
        unread_only: bool = False
    ) -> List[Notification]:
        """Retrieve user's notifications from database"""
        async with AsyncSession() as session:
            query = select(NotificationModel).where(
                NotificationModel.user_id == user_id
            )
            if unread_only:
                query = query.where(NotificationModel.is_read == False)
            
            query = query.order_by(
                NotificationModel.created_at.desc()
            ).limit(limit)
            
            result = await session.execute(query)
            return result.scalars().all()
    
    async def mark_as_read(
        self,
        notification_id: str,
        user_id: str
    ) -> bool:
        """Mark a notification as read"""
        async with AsyncSession() as session:
            result = await session.execute(
                update(NotificationModel)
                .where(
                    NotificationModel.id == notification_id,
                    NotificationModel.user_id == user_id
                )
                .values(
                    is_read=True,
                    read_at=datetime.utcnow()
                )
            )
            await session.commit()
            return result.rowcount > 0
    
    async def _get_user_preferences(self, user_id: str) -> dict:
        """Get user's notification preferences"""
        async with AsyncSession() as session:
            result = await session.execute(
                select(NotificationPreference)
                .where(NotificationPreference.user_id == user_id)
            )
            prefs = result.scalars().all()
            return {
                pref.notification_type: {
                    "enabled": pref.enabled,
                    "delivery_method": pref.delivery_method
                }
                for pref in prefs
            }

# Integration with other services
class ApprovalNotificationMixin:
    """Mixin for approval-related notifications"""
    
    async def notify_approval_requested(
        self,
        requester: User,
        approvers: List[User],
        tool_name: str,
        approval_id: str
    ):
        for approver in approvers:
            await notification_service.send_notification(
                user_id=approver.id,
                notification_type=NotificationType.APPROVAL_REQUESTED,
                title="Approval Required",
                message=f"{requester.name} is requesting approval to run '{tool_name}'",
                data={
                    "requester_id": requester.id,
                    "requester_name": requester.name,
                    "tool_name": tool_name,
                    "approval_id": approval_id,
                    "action_url": f"/approvals/{approval_id}"
                },
                priority=NotificationPriority.HIGH
            )
    
    async def notify_approval_decision(
        self,
        requester: User,
        approver: User,
        tool_name: str,
        decision: str,
        approval_id: str
    ):
        await notification_service.send_notification(
            user_id=requester.id,
            notification_type=NotificationType.APPROVAL_DECISION,
            title=f"Approval {decision}",
            message=f"{approver.name} has {decision.lower()} your request to run '{tool_name}'",
            data={
                "approver_id": approver.id,
                "approver_name": approver.name,
                "tool_name": tool_name,
                "decision": decision,
                "approval_id": approval_id
            },
            priority=NotificationPriority.HIGH
        )

class DocumentNotificationMixin:
    """Mixin for document-related notifications"""
    
    async def notify_document_indexed(
        self,
        user: User,
        document_id: str,
        document_title: str
    ):
        await notification_service.send_notification(
            user_id=user.id,
            notification_type=NotificationType.DOCUMENT_INDEXED,
            title="Document Indexed",
            message=f"'{document_title}' has been successfully indexed and is now searchable",
            data={
                "document_id": document_id,
                "document_title": document_title,
                "action_url": f"/documents/{document_id}"
            },
            priority=NotificationPriority.NORMAL
        )

class ToolNotificationMixin:
    """Mixin for tool registry notifications"""
    
    async def notify_tool_added(
        self,
        tool_name: str,
        tool_description: str,
        target_roles: Optional[List[str]] = None
    ):
        await notification_service.send_broadcast_notification(
            notification_type=NotificationType.TOOL_ADDED,
            title="New Tool Available",
            message=f"A new tool '{tool_name}' has been added: {tool_description}",
            target_roles=target_roles,
            data={
                "tool_name": tool_name,
                "tool_description": tool_description
            },
            priority=NotificationPriority.NORMAL
        )

# Global instance
notification_service = NotificationService()
```

**Key Features:**

1. **Real-Time Delivery**: SSE streams for instant notification delivery
2. **Multi-Instance Support**: Redis pub/sub for horizontal scaling
3. **Persistent Notifications**: Database storage for retrieval and history
4. **User Preferences**: Per-notification-type enable/disable settings
5. **Priority Levels**: Low, normal, high, urgent classification
6. **Targeted Broadcasting**: Send to all users or specific roles
7. **Integration Mixins**: Easy integration with other services

**Notification Types Covered:**
- Approval requests and decisions
- Document processing status
- Tool registry changes
- Security alerts
- Vault access events
- System maintenance announcements
- Long-running task completions

### 4.9 Notification Streaming Endpoint Implementation

**API Endpoint (`app/api/v1/notifications.py`):**
```python
from fastapi import APIRouter, Depends, HTTPException
from sse_starlette.sse import EventSourceResponse
from app.services.notification_service import notification_service, NotificationType
from app.core.security import get_current_user
from app.schemas.notification import NotificationPreferences

router = APIRouter(prefix="/notifications", tags=["notifications"])

@router.get("/stream")
async def stream_notifications(
    current_user: User = Depends(get_current_user)
):
    """
    SSE stream for real-time notifications.
    Client should connect to this endpoint on login and maintain the connection.
    """
    async def event_generator():
        async for notification in notification_service.register_connection(
            user_id=current_user.id
        ):
            yield notification
    
    return EventSourceResponse(
        event_generator(),
        headers={
            "Cache-Control": "no-cache",
            "X-Accel-Buffering": "no"  # Disable nginx buffering
        }
    )

@router.get("")
async def get_notifications(
    current_user: User = Depends(get_current_user),
    limit: int = 50,
    unread_only: bool = False
):
    """Get user's notification history"""
    notifications = await notification_service.get_user_notifications(
        user_id=current_user.id,
        limit=limit,
        unread_only=unread_only
    )
    return {"notifications": notifications}

@router.put("/{notification_id}/read")
async def mark_notification_read(
    notification_id: str,
    current_user: User = Depends(get_current_user)
):
    """Mark a specific notification as read"""
    success = await notification_service.mark_as_read(
        notification_id=notification_id,
        user_id=current_user.id
    )
    if not success:
        raise HTTPException(status_code=404, detail="Notification not found")
    return {"status": "success"}

@router.put("/mark-all-read")
async def mark_all_read(
    current_user: User = Depends(get_current_user)
):
    """Mark all user's notifications as read"""
    count = await notification_service.mark_all_as_read(current_user.id)
    return {"marked_read": count}

@router.get("/preferences")
async def get_notification_preferences(
    current_user: User = Depends(get_current_user)
):
    """Get user's notification preferences"""
    prefs = await notification_service.get_preferences(current_user.id)
    return {"preferences": prefs}

@router.put("/preferences")
async def update_notification_preferences(
    preferences: NotificationPreferences,
    current_user: User = Depends(get_current_user)
):
    """Update user's notification preferences"""
    await notification_service.update_preferences(
        user_id=current_user.id,
        preferences=preferences
    )
    return {"status": "success"}
```

**Frontend Integration Example:**
```typescript
// In App.tsx or a NotificationProvider component
import { useEffect, useState } from 'react';

interface Notification {
  id: string;
  type: string;
  title: string;
  message: string;
  priority: 'low' | 'normal' | 'high' | 'urgent';
  data: any;
  created_at: string;
}

export function useNotifications() {
  const [notifications, setNotifications] = useState<Notification[]>([]);
  const [unreadCount, setUnreadCount] = useState(0);

  useEffect(() => {
    const eventSource = new EventSource('/api/v1/notifications/stream', {
      withCredentials: true,
      headers: {
        'Authorization': `Bearer ${getAuthToken()}`
      }
    });

    eventSource.addEventListener('notification', (event) => {
      const notification: Notification = JSON.parse(event.data);
      
      // Add to notification list
      setNotifications(prev => [notification, ...prev]);
      setUnreadCount(prev => prev + 1);
      
      // Show toast notification
      showToast({
        title: notification.title,
        message: notification.message,
        type: notification.priority === 'urgent' ? 'error' : 
              notification.priority === 'high' ? 'warning' : 'info',
        duration: notification.priority === 'urgent' ? 0 : 5000
      });
      
      // Handle specific notification types
      handleNotificationAction(notification);
    });

    eventSource.addEventListener('connected', () => {
      console.log('Notification stream connected');
    });

    eventSource.onerror = (error) => {
      console.error('Notification stream error:', error);
      // Implement exponential backoff retry
      setTimeout(() => {
        eventSource.close();
        // Reconnect logic here
      }, 5000);
    };

    return () => {
      eventSource.close();
    };
  }, []);

  const markAsRead = async (notificationId: string) => {
    await fetch(`/api/v1/notifications/${notificationId}/read`, {
      method: 'PUT',
      headers: {
        'Authorization': `Bearer ${getAuthToken()}`
      }
    });
    setUnreadCount(prev => Math.max(0, prev - 1));
  };

  return { notifications, unreadCount, markAsRead };
}

function handleNotificationAction(notification: Notification) {
  switch (notification.type) {
    case 'APPROVAL_REQUESTED':
      // Navigate to approvals view if current user is manager
      if (currentUserRole === 'MANAGER') {
        setCurrentView('approvals');
      }
      break;
      
    case 'APPROVAL_DECISION':
      // Refresh chat if user was waiting for approval
      refreshChatMessages();
      break;
      
    case 'DOCUMENT_INDEXED':
      // Refresh documents list
      refreshDocuments();
      break;
      
    case 'TOOL_ADDED':
      // Refresh tool registry
      refreshTools();
      showBanner(`New tool available: ${notification.data.tool_name}`);
      break;
      
    case 'SECURITY_ALERT':
      // Show prominent alert
      showSecurityAlert(notification.message);
      break;
  }
}
```

**Notification Toast Component Example:**
```typescript
// components/NotificationToast.tsx
import React from 'react';

interface ToastProps {
  title: string;
  message: string;
  type: 'info' | 'warning' | 'error' | 'success';
  onClose: () => void;
}

export const NotificationToast: React.FC<ToastProps> = ({
  title,
  message,
  type,
  onClose
}) => {
  const bgColor = {
    info: 'bg-blue-500',
    warning: 'bg-yellow-500',
    error: 'bg-red-500',
    success: 'bg-green-500'
  }[type];

  return (
    <div className={`${bgColor} text-white p-4 rounded-lg shadow-lg mb-2 animate-slide-in`}>
      <div className="flex justify-between items-start">
        <div className="flex-1">
          <h4 className="font-bold mb-1">{title}</h4>
          <p className="text-sm opacity-90">{message}</p>
        </div>
        <button onClick={onClose} className="ml-4 text-white hover:text-gray-200">
          ×
        </button>
      </div>
    </div>
  );
};
```

---

## 5. Security Architecture

### 5.1 Authentication & Authorization

**JWT Token Strategy:**
```python
# Token structure
{
    "user_id": "uuid",
    "username": "string",
    "role": "ANALYST|MANAGER|ADMIN",
    "exp": timestamp,
    "iat": timestamp
}
```

**RBAC Implementation:**
```python
class RBACEngine:
    ROLE_HIERARCHY = {
        "ADMIN": 3,
        "MANAGER": 2,
        "ANALYST": 1
    }
    
    def has_permission(
        self,
        user: User,
        required_role: str
    ) -> bool:
        user_level = self.ROLE_HIERARCHY.get(user.role, 0)
        required_level = self.ROLE_HIERARCHY.get(required_role, 0)
        return user_level >= required_level
```

### 5.2 Security Middleware

```python
@app.middleware("http")
async def security_middleware(request: Request, call_next):
    # 1. Rate limiting
    # 2. CORS validation
    # 3. Request validation
    # 4. Audit logging
    # 5. Error handling
    response = await call_next(request)
    return response
```

### 5.3 Data Protection

**Encryption:**
- Secrets: Fernet symmetric encryption
- Sensitive fields: Field-level encryption
- Communication: TLS 1.3
- Database: Transparent data encryption (TDE)

**Redaction:**
```python
class DataRedactor:
    PATTERNS = {
        "ssn": r"\d{3}-\d{2}-\d{4}",
        "credit_card": r"\d{4}[\s-]?\d{4}[\s-]?\d{4}[\s-]?\d{4}",
        "email": r"\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b"
    }
    
    def redact_sensitive_data(self, text: str) -> str:
        for pattern_type, pattern in self.PATTERNS.items():
            text = re.sub(pattern, f"[REDACTED-{pattern_type.upper()}]", text)
        return text
```

---

## 6. Project Structure

```
backend/
├── app/
│   ├── __init__.py
│   ├── main.py                 # FastAPI application entry point
│   ├── config.py               # Configuration management
│   ├── dependencies.py         # Dependency injection
│   │
│   ├── api/                    # API endpoints
│   │   ├── __init__.py
│   │   ├── v1/
│   │   │   ├── __init__.py
│   │   │   ├── auth.py
│   │   │   ├── chat.py
│   │   │   ├── tools.py
│   │   │   ├── approvals.py
│   │   │   ├── audit.py
│   │   │   ├── vault.py
│   │   │   ├── documents.py
│   │   │   ├── llm.py
│   │   │   └── notifications.py
│   │
│   ├── core/                   # Core functionality
│   │   ├── __init__.py
│   │   ├── security.py        # Auth, JWT, encryption
│   │   ├── rbac.py            # RBAC engine
│   │   ├── errors.py          # Custom exceptions
│   │   └── logging.py         # Logging configuration
│   │
│   ├── models/                 # SQLAlchemy models
│   │   ├── __init__.py
│   │   ├── user.py
│   │   ├── message.py
│   │   ├── tool.py
│   │   ├── approval.py
│   │   ├── audit.py
│   │   ├── secret.py
│   │   ├── document.py
│   │   ├── llm_model.py
│   │   └── notification.py
│   │
│   ├── schemas/                # Pydantic schemas
│   │   ├── __init__.py
│   │   ├── user.py
│   │   ├── message.py
│   │   ├── tool.py
│   │   ├── approval.py
│   │   ├── audit.py
│   │   ├── secret.py
│   │   ├── document.py
│   │   ├── llm_model.py
│   │   └── notification.py
│   │
│   ├── services/               # Business logic
│   │   ├── __init__.py
│   │   ├── chat_service.py
│   │   ├── tool_engine.py
│   │   ├── approval_service.py
│   │   ├── audit_service.py
│   │   ├── vault_service.py
│   │   ├── document_service.py
│   │   ├── llm_service.py
│   │   ├── cache_service.py
│   │   └── notification_service.py
│   │
│   ├── tools/                  # Tool implementations
│   │   ├── __init__.py
│   │   ├── base.py            # BaseTool abstract class
│   │   ├── datetime_tool.py
│   │   ├── calculator_tool.py
│   │   ├── patient_data_tool.py
│   │   ├── financial_report_tool.py
│   │   ├── document_query_tool.py
│   │   ├── sales_analysis_tool.py
│   │   └── vault_retrieve_tool.py
│   │
│   ├── db/                     # Database
│   │   ├── __init__.py
│   │   ├── base.py            # Base model
│   │   ├── session.py         # Database session
│   │   └── migrations/        # Alembic migrations
│   │
│   ├── middleware/             # Custom middleware
│   │   ├── __init__.py
│   │   ├── rate_limiting.py
│   │   ├── audit_logging.py
│   │   └── error_handling.py
│   │
│   └── utils/                  # Utilities
│       ├── __init__.py
│       ├── encryption.py
│       ├── validation.py
│       ├── redaction.py
│       └── streaming.py
│
├── tests/                      # Tests
│   ├── __init__.py
│   ├── conftest.py
│   ├── unit/
│   │   ├── test_tools.py
│   │   ├── test_services.py
│   │   └── test_utils.py
│   ├── integration/
│   │   ├── test_api.py
│   │   └── test_workflows.py
│   └── e2e/
│       └── test_chat_flow.py
│
├── scripts/                    # Utility scripts
│   ├── init_db.py
│   ├── seed_data.py
│   └── run_migrations.py
│
├── alembic/                    # Database migrations
│   ├── versions/
│   └── env.py
│
├── .env.example                # Environment template
├── .gitignore
├── alembic.ini                 # Alembic config
├── requirements.txt            # Python dependencies
├── requirements-dev.txt        # Dev dependencies
├── pyproject.toml             # Project metadata
├── pytest.ini                 # Pytest config
├── docker-compose.yml         # Docker services
├── Dockerfile                 # Application container
└── README.md                  # Setup instructions
```

---

## 7. Dependencies & Requirements

### 7.1 Core Dependencies (`requirements.txt`)

```txt
# Web Framework
fastapi==0.109.0
uvicorn[standard]==0.27.0
python-multipart==0.0.6
sse-starlette==1.8.2

# Database
sqlalchemy==2.0.25
psycopg2-binary==2.9.9
alembic==1.13.1
asyncpg==0.29.0

# Redis & Caching
redis==5.0.1
hiredis==2.3.2

# Background Tasks
celery==5.3.6

# Authentication
python-jose[cryptography]==3.3.0
passlib[bcrypt]==1.7.4
python-multipart==0.0.6

# AI/ML Framework
langchain==0.1.6
langchain-openai==0.0.5
langchain-anthropic==0.0.1
llama-index==0.9.48
openai==1.12.0
anthropic==0.8.1

# Local LLM
ollama==0.1.6

# Vector Database
pgvector==0.2.4

# Document Processing
pypdf==4.0.1
python-docx==1.1.0
python-pptx==0.6.23
beautifulsoup4==4.12.3
lxml==5.1.0

# Embeddings
sentence-transformers==2.3.1
torch==2.2.0

# Data Validation
pydantic==2.6.0
pydantic-settings==2.1.0
email-validator==2.1.0

# Security & Encryption
cryptography==42.0.2
bcrypt==4.1.2

# Utilities
python-dateutil==2.8.2
pytz==2024.1
httpx==0.26.0
aiofiles==23.2.1

# Monitoring & Logging
structlog==24.1.0
python-json-logger==2.0.7
sentry-sdk==1.40.0

# Configuration
python-dotenv==1.0.1
pyyaml==6.0.1
```

### 7.2 Development Dependencies (`requirements-dev.txt`)

```txt
# Testing
pytest==8.0.0
pytest-asyncio==0.23.4
pytest-cov==4.1.0
pytest-mock==3.12.0
httpx==0.26.0

# Code Quality
black==24.1.1
isort==5.13.2
flake8==7.0.0
mypy==1.8.0
pylint==3.0.3

# Documentation
mkdocs==1.5.3
mkdocs-material==9.5.6

# Debugging
ipython==8.20.0
ipdb==0.13.13
```

---

## 8. Configuration Management

### 8.1 Environment Variables (`.env.example`)

```bash
# Application
APP_NAME="CDSA Backend"
APP_VERSION="1.0.0"
ENVIRONMENT="development"  # development, staging, production
DEBUG=true
LOG_LEVEL="INFO"

# API
API_HOST="0.0.0.0"
API_PORT=8000
API_RELOAD=true
CORS_ORIGINS="http://localhost:3000,http://localhost:5173"

# Database
DATABASE_URL="postgresql+asyncpg://cdsa:password@localhost:5432/cdsa_db"
DB_POOL_SIZE=20
DB_MAX_OVERFLOW=0
DB_ECHO=false

# Redis
REDIS_URL="redis://localhost:6379/0"
REDIS_CACHE_DB=1
REDIS_SESSION_DB=2

# Security
SECRET_KEY="your-secret-key-here-min-32-chars"
JWT_SECRET_KEY="your-jwt-secret-key"
JWT_ALGORITHM="HS256"
JWT_EXPIRATION_MINUTES=60
ENCRYPTION_KEY="your-encryption-key-base64"

# Rate Limiting
RATE_LIMIT_REQUESTS=100
RATE_LIMIT_PERIOD=60

# LLM Configuration
# OpenAI
OPENAI_API_KEY="sk-..."
OPENAI_MODEL="gpt-4-turbo-preview"
OPENAI_TEMPERATURE=0.7

# Anthropic
ANTHROPIC_API_KEY="sk-ant-..."
ANTHROPIC_MODEL="claude-3-opus-20240229"

# Google (Gemini)
GEMINI_API_KEY="..."

# Local Models (Ollama)
OLLAMA_BASE_URL="http://localhost:11434"
OLLAMA_DEFAULT_MODEL="llama3:8b"

# Embeddings
EMBEDDING_MODEL="text-embedding-3-small"
EMBEDDING_DIMENSIONS=1536

# RAG Configuration
RAG_CHUNK_SIZE=1000
RAG_CHUNK_OVERLAP=200
RAG_TOP_K=5

# Celery
CELERY_BROKER_URL="redis://localhost:6379/3"
CELERY_RESULT_BACKEND="redis://localhost:6379/3"

# Monitoring
SENTRY_DSN=""
ENABLE_METRICS=true

# File Storage
UPLOAD_DIR="./uploads"
MAX_UPLOAD_SIZE=10485760  # 10MB

# Context Window
MAX_CONTEXT_WINDOW=4096
CONTEXT_PRUNING_THRESHOLD=3500
```

### 8.2 Configuration Class (`app/config.py`)

```python
from pydantic_settings import BaseSettings
from typing import List, Optional

class Settings(BaseSettings):
    # Application
    APP_NAME: str = "CDSA Backend"
    ENVIRONMENT: str = "development"
    DEBUG: bool = False
    
    # API
    API_HOST: str = "0.0.0.0"
    API_PORT: int = 8000
    CORS_ORIGINS: List[str] = ["http://localhost:3000"]
    
    # Database
    DATABASE_URL: str
    
    # Redis
    REDIS_URL: str
    
    # Security
    SECRET_KEY: str
    JWT_SECRET_KEY: str
    JWT_ALGORITHM: str = "HS256"
    JWT_EXPIRATION_MINUTES: int = 60
    
    # LLM
    OPENAI_API_KEY: Optional[str] = None
    ANTHROPIC_API_KEY: Optional[str] = None
    GEMINI_API_KEY: Optional[str] = None
    
    class Config:
        env_file = ".env"
        case_sensitive = True

settings = Settings()
```

---

## 9. Streaming Response Implementation

### 9.1 SSE Streaming Endpoint

```python
from fastapi import APIRouter
from fastapi.responses import StreamingResponse
from sse_starlette.sse import EventSourceResponse

router = APIRouter()

@router.post("/chat/stream")
async def stream_chat(
    request: ChatRequest,
    current_user: User = Depends(get_current_user),
    chat_service: ChatService = Depends()
):
    async def event_generator():
        async for chunk in chat_service.stream_response(
            user_input=request.message,
            user=current_user,
            session_id=request.session_id,
            history=request.history
        ):
            # Serialize chunk to JSON
            data = {
                "message": chunk.message.dict() if chunk.message else None,
                "auditEvent": chunk.audit_event.dict() if chunk.audit_event else None,
                "historyRewrite": [m.dict() for m in chunk.history_rewrite] if chunk.history_rewrite else None
            }
            # Send as SSE event
            yield {
                "event": "message",
                "data": json.dumps(data)
            }
        
        # Send completion signal
        yield {
            "event": "done",
            "data": json.dumps({"status": "completed"})
        }
    
    return EventSourceResponse(event_generator())
```

### 9.2 WebSocket Alternative

```python
from fastapi import WebSocket

@app.websocket("/ws/chat")
async def websocket_chat(
    websocket: WebSocket,
    chat_service: ChatService = Depends()
):
    await websocket.accept()
    
    try:
        while True:
            # Receive message
            data = await websocket.receive_json()
            
            # Stream response
            async for chunk in chat_service.stream_response(
                user_input=data["message"],
                user=data["user"],
                session_id=data["session_id"]
            ):
                await websocket.send_json(chunk.dict())
            
            # Send completion
            await websocket.send_json({"type": "done"})
            
    except WebSocketDisconnect:
        print("Client disconnected")
```

---

## 10. Deployment Architecture

### 10.1 Docker Compose (`docker-compose.yml`)

```yaml
version: '3.8'

services:
  # PostgreSQL Database
  postgres:
    image: pgvector/pgvector:pg16
    environment:
      POSTGRES_DB: cdsa_db
      POSTGRES_USER: cdsa
      POSTGRES_PASSWORD: ${DB_PASSWORD}
    volumes:
      - postgres_data:/var/lib/postgresql/data
    ports:
      - "5432:5432"
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U cdsa"]
      interval: 10s
      timeout: 5s
      retries: 5

  # Redis Cache
  redis:
    image: redis:7-alpine
    command: redis-server --appendonly yes
    volumes:
      - redis_data:/data
    ports:
      - "6379:6379"
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 5s
      retries: 5

  # FastAPI Application
  api:
    build: .
    command: uvicorn app.main:app --host 0.0.0.0 --port 8000 --reload
    environment:
      DATABASE_URL: postgresql+asyncpg://cdsa:${DB_PASSWORD}@postgres:5432/cdsa_db
      REDIS_URL: redis://redis:6379/0
    env_file:
      - .env
    volumes:
      - ./app:/app/app
      - ./uploads:/app/uploads
    ports:
      - "8000:8000"
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy

  # Celery Worker
  celery_worker:
    build: .
    command: celery -A app.tasks worker --loglevel=info
    environment:
      DATABASE_URL: postgresql+asyncpg://cdsa:${DB_PASSWORD}@postgres:5432/cdsa_db
      REDIS_URL: redis://redis:6379/0
    env_file:
      - .env
    depends_on:
      - postgres
      - redis

  # Ollama (Local LLM)
  ollama:
    image: ollama/ollama:latest
    volumes:
      - ollama_data:/root/.ollama
    ports:
      - "11434:11434"
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]

  # NGINX Reverse Proxy
  nginx:
    image: nginx:alpine
    volumes:
      - ./nginx.conf:/etc/nginx/nginx.conf:ro
    ports:
      - "80:80"
      - "443:443"
    depends_on:
      - api

volumes:
  postgres_data:
  redis_data:
  ollama_data:
```

### 10.2 Dockerfile

```dockerfile
FROM python:3.11-slim

WORKDIR /app

# Install system dependencies
RUN apt-get update && apt-get install -y \
    gcc \
    g++ \
    postgresql-client \
    && rm -rf /var/lib/apt/lists/*

# Copy requirements
COPY requirements.txt requirements-dev.txt ./

# Install Python dependencies
RUN pip install --no-cache-dir -r requirements.txt

# Copy application code
COPY ./app ./app
COPY ./alembic ./alembic
COPY ./alembic.ini ./

# Create non-root user
RUN useradd -m -u 1000 cdsa && chown -R cdsa:cdsa /app
USER cdsa

# Expose port
EXPOSE 8000

# Run migrations and start server
CMD ["sh", "-c", "alembic upgrade head && uvicorn app.main:app --host 0.0.0.0 --port 8000"]
```

---

## 11. Testing Strategy

### 11.1 Test Structure

```python
# tests/conftest.py
import pytest
from fastapi.testclient import TestClient
from sqlalchemy.ext.asyncio import create_async_engine, AsyncSession
from app.main import app
from app.db.session import get_db

@pytest.fixture
async def db_session():
    engine = create_async_engine("postgresql+asyncpg://test:test@localhost/test_db")
    async with AsyncSession(engine) as session:
        yield session
    await engine.dispose()

@pytest.fixture
def client(db_session):
    async def override_get_db():
        yield db_session
    
    app.dependency_overrides[get_db] = override_get_db
    return TestClient(app)

@pytest.fixture
def auth_headers(client):
    # Login and get token
    response = client.post("/api/v1/auth/login", json={
        "username": "test_user",
        "password": "test_password"
    })
    token = response.json()["access_token"]
    return {"Authorization": f"Bearer {token}"}
```

### 11.2 Test Examples

```python
# tests/integration/test_chat_flow.py
import pytest

@pytest.mark.asyncio
async def test_chat_stream(client, auth_headers):
    response = client.post(
        "/api/v1/chat/stream",
        json={
            "message": "What time is it?",
            "session_id": "test-session"
        },
        headers=auth_headers
    )
    
    assert response.status_code == 200
    # Verify streaming response
    events = []
    for line in response.iter_lines():
        if line:
            events.append(json.loads(line))
    
    assert len(events) > 0
    assert any(e.get("message") for e in events)

@pytest.mark.asyncio
async def test_approval_workflow(client, auth_headers):
    # 1. Request tool that requires approval
    response = client.post(
        "/api/v1/chat/stream",
        json={
            "message": "Generate financial report",
            "session_id": "test-session"
        },
        headers=auth_headers
    )
    
    # 2. Verify approval request created
    approvals = client.get("/api/v1/approvals", headers=auth_headers).json()
    assert len(approvals) > 0
    
    # 3. Approve request
    approval_id = approvals[0]["id"]
    response = client.post(
        f"/api/v1/approvals/{approval_id}/decide",
        json={"decision": "APPROVED"},
        headers=auth_headers
    )
    assert response.status_code == 200
```

---

## 12. Monitoring & Observability

### 12.1 Logging

```python
import structlog

logger = structlog.get_logger()

# Usage
logger.info(
    "tool_executed",
    tool_name=tool.name,
    user_id=str(user.id),
    execution_time_ms=execution_time,
    success=True
)
```

### 12.2 Metrics (Prometheus)

```python
from prometheus_client import Counter, Histogram

# Metrics
tool_executions = Counter(
    'tool_executions_total',
    'Total tool executions',
    ['tool_name', 'status']
)

response_time = Histogram(
    'response_time_seconds',
    'Response time',
    ['endpoint']
)
```

### 12.3 Health Checks

```python
@app.get("/health")
async def health_check():
    return {
        "status": "healthy",
        "version": settings.APP_VERSION,
        "services": {
            "database": await check_db_connection(),
            "redis": await check_redis_connection(),
            "llm": await check_llm_availability()
        }
    }
```

---

## 13. Performance Optimizations

### 13.1 Caching Strategy

```python
# Redis caching with TTL
class CacheService:
    async def get_or_compute(
        self,
        key: str,
        compute_fn: Callable,
        ttl: int = 300
    ):
        # Check cache
        cached = await self.redis.get(key)
        if cached:
            return json.loads(cached)
        
        # Compute and cache
        result = await compute_fn()
        await self.redis.setex(key, ttl, json.dumps(result))
        return result
```

### 13.2 Database Query Optimization

```python
# Use eager loading for relationships
query = (
    select(Tool)
    .options(selectinload(Tool.executions))
    .where(Tool.is_enabled == True)
)

# Use indexes effectively
# Create in migrations
Index('idx_audit_user_time', 'user_id', 'created_at')
```

### 13.3 Connection Pooling

```python
engine = create_async_engine(
    DATABASE_URL,
    pool_size=20,
    max_overflow=0,
    pool_pre_ping=True,
    pool_recycle=3600
)
```

---

## 14. Migration Plan

### 14.1 Phase 1: Foundation (Weeks 1-2)
- [x] Set up project structure
- [ ] Configure database and Redis
- [ ] Implement authentication system
- [ ] Create base models and schemas
- [ ] Set up API routing

### 14.2 Phase 2: Core Features (Weeks 3-4)
- [ ] Implement chat streaming
- [ ] Build tool execution engine
- [ ] Create RBAC system
- [ ] Add audit logging
- [ ] Implement basic tools

### 14.3 Phase 3: Advanced Features (Weeks 5-6)
- [ ] Approval workflow
- [ ] Secrets vault
- [ ] Document indexing & RAG
- [ ] LLM integration (local & cloud)
- [ ] Caching system

### 14.4 Phase 4: Testing & Optimization (Weeks 7-8)
- [ ] Write comprehensive tests
- [ ] Performance optimization
- [ ] Security audit
- [ ] Documentation
- [ ] Deployment setup

---

## 15. Success Criteria

### 15.1 Functional Requirements
- ✅ Support streaming chat responses
- ✅ Execute tools with RBAC enforcement
- ✅ Implement approval workflow for high-risk operations
- ✅ Complete audit trail for all actions
- ✅ Secure secrets management
- ✅ Document RAG pipeline
- ✅ Support both local and cloud LLMs

### 15.2 Non-Functional Requirements
- **Performance**: 
  - API response time < 200ms (excluding LLM calls)
  - Support 100 concurrent users
  - Cache hit rate > 80% for tool results
  
- **Security**:
  - All data encrypted at rest and in transit
  - RBAC enforced on all operations
  - Complete audit logging
  - Regular security scans

- **Reliability**:
  - 99.9% uptime
  - Automatic failover for database
  - Graceful degradation when services unavailable

- **Scalability**:
  - Horizontal scaling via load balancer
  - Database read replicas
  - Redis cluster for high availability

---

## 16. Risk Management

### 16.1 Identified Risks

| Risk | Impact | Mitigation |
|------|--------|------------|
| LLM API rate limits | High | Implement caching, fallback to local models |
| Database performance | Medium | Proper indexing, query optimization, read replicas |
| Security vulnerabilities | Critical | Regular audits, penetration testing, security updates |
| Data privacy concerns | Critical | Encryption, redaction, compliance validation |
| LLM hallucinations | Medium | RAG for grounding, approval workflow for high-risk actions |

### 16.2 Contingency Plans

- **LLM Unavailable**: Fall back to cached responses or local models
- **Database Down**: Read-only mode using Redis cache
- **High Load**: Auto-scaling, rate limiting, request queueing

---

## 17. Next Steps

### Immediate Actions:
1. ✅ Review and approve architecture plan
2. [ ] Set up development environment
3. [ ] Initialize Git repository
4. [ ] Create database schema
5. [ ] Implement authentication system

### This Week:
- Set up PostgreSQL with pgvector
- Configure Redis
- Create project structure
- Implement user model and auth endpoints
- Set up testing framework

### Next Sprint:
- Build chat streaming endpoint
- Implement tool execution engine
- Create first 3-5 tools
- Add RBAC enforcement
- Begin audit logging

---

## Appendix A: API Request/Response Examples

### Chat Stream Request
```json
POST /api/v1/chat/stream
{
  "message": "Analyze sales data for Q3 2024",
  "session_id": "550e8400-e29b-41d4-a716-446655440000",
  "history": [
    {
      "id": "msg-1",
      "author": "user",
      "text": "Hello"
    }
  ],
  "userContext": {
    "id": "user-123",
    "name": "Alex",
    "role": "MANAGER"
  },
  "activeModelId": "llama3-8b"
}
```

### Stream Response (SSE)
```
event: message
data: {"message": {"id": "msg-2", "author": "agent", "text": "Analyzing sales data..."}}

event: message
data: {"auditEvent": {"type": "TOOL_CALL_INITIATED", "toolName": "analyze_sales_data"}}

event: message
data: {"message": {"id": "msg-3", "author": "system", "toolResult": {...}}}

event: done
data: {"status": "completed"}
```

---

## Appendix B: Security Checklist

- [ ] Use HTTPS only in production
- [ ] Implement rate limiting
- [ ] Validate all inputs
- [ ] Sanitize outputs
- [ ] Use parameterized queries
- [ ] Encrypt sensitive data
- [ ] Implement CSRF protection
- [ ] Use secure session management
- [ ] Regular dependency updates
- [ ] Security headers configured
- [ ] Audit logging enabled
- [ ] Access control enforced
- [ ] Secrets never in code
- [ ] Regular backups
- [ ] Incident response plan

---

**Document Version**: 1.0  
**Last Updated**: 2025-11-12  
**Author**: AI Architect  
**Status**: Ready for Implementation