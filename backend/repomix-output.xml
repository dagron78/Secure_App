This file is a merged representation of the entire codebase, combined into a single document by Repomix.

<file_summary>
This section contains a summary of this file.

<purpose>
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.
</purpose>

<file_format>
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Repository files (if enabled)
5. Multiple file entries, each consisting of:
  - File path as an attribute
  - Full contents of the file
</file_format>

<usage_guidelines>
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.
</usage_guidelines>

<notes>
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded
- Files are sorted by Git change count (files with more changes are at the bottom)
</notes>

</file_summary>

<directory_structure>
alembic/
  versions/
    001_initial_schema.py
  env.py
  script.py.mako
app/
  api/
    v1/
      audit.py
      auth.py
      chat.py
      documents.py
      llm.py
      notifications.py
      tools.py
      vault.py
  core/
    deps.py
    logging.py
    security.py
  db/
    base.py
  models/
    __init__.py
    audit.py
    chat.py
    document.py
    notification.py
    secret.py
    tool.py
    user.py
  schemas/
    audit.py
    auth.py
    chat.py
    document.py
    notification.py
    secret.py
    tool.py
  services/
    chunking_service.py
    context_manager.py
    document_processor.py
    embedding_service.py
    llm_service.py
    notification_service.py
  __init__.py
  config.py
  main.py
scripts/
  create_tables_direct.py
  seed_data.py
  setup_database.sh
  validate_setup.py
.env.example
.gitignore
.wnv
alembic.ini
create_tables.py
docker-compose.yml
Dockerfile
migration.sql
pyproject.toml
QUICKSTART.md
README.md
run.sh
test_bcrypt_direct.py
test_login.py
test_server.py
</directory_structure>

<files>
This section contains the contents of the repository's files.

<file path="alembic/versions/001_initial_schema.py">
"""initial schema

Revision ID: 001
Revises: 
Create Date: 2024-01-12 02:21:00.000000

"""
from typing import Sequence, Union

from alembic import op
import sqlalchemy as sa
from sqlalchemy.dialects import postgresql

# revision identifiers, used by Alembic.
revision: str = '001'
down_revision: Union[str, None] = None
branch_labels: Union[str, Sequence[str], None] = None
depends_on: Union[str, Sequence[str], None] = None


def upgrade() -> None:
    """Create all database tables for CDSA application."""
    
    # Create roles table
    op.create_table(
        'roles',
        sa.Column('id', sa.Integer(), nullable=False),
        sa.Column('name', sa.String(length=100), nullable=False),
        sa.Column('description', sa.String(length=500), nullable=True),
        sa.Column('created_at', sa.DateTime(), nullable=False),
        sa.Column('updated_at', sa.DateTime(), nullable=False),
        sa.PrimaryKeyConstraint('id'),
        sa.UniqueConstraint('name')
    )
    op.create_index(op.f('ix_roles_id'), 'roles', ['id'], unique=False)
    
    # Create permissions table
    op.create_table(
        'permissions',
        sa.Column('id', sa.Integer(), nullable=False),
        sa.Column('name', sa.String(length=100), nullable=False),
        sa.Column('description', sa.String(length=500), nullable=True),
        sa.Column('created_at', sa.DateTime(), nullable=False),
        sa.PrimaryKeyConstraint('id'),
        sa.UniqueConstraint('name')
    )
    op.create_index(op.f('ix_permissions_id'), 'permissions', ['id'], unique=False)
    
    # Create users table
    op.create_table(
        'users',
        sa.Column('id', sa.Integer(), nullable=False),
        sa.Column('email', sa.String(length=255), nullable=False),
        sa.Column('username', sa.String(length=100), nullable=False),
        sa.Column('hashed_password', sa.String(length=255), nullable=False),
        sa.Column('full_name', sa.String(length=255), nullable=True),
        sa.Column('is_active', sa.Boolean(), nullable=False),
        sa.Column('is_superuser', sa.Boolean(), nullable=False),
        sa.Column('is_verified', sa.Boolean(), nullable=False),
        sa.Column('created_at', sa.DateTime(), nullable=False),
        sa.Column('updated_at', sa.DateTime(), nullable=False),
        sa.Column('last_login', sa.DateTime(), nullable=True),
        sa.PrimaryKeyConstraint('id')
    )
    op.create_index(op.f('ix_users_email'), 'users', ['email'], unique=True)
    op.create_index(op.f('ix_users_id'), 'users', ['id'], unique=False)
    op.create_index(op.f('ix_users_username'), 'users', ['username'], unique=True)
    
    # Create user_roles junction table
    op.create_table(
        'user_roles',
        sa.Column('user_id', sa.Integer(), nullable=False),
        sa.Column('role_id', sa.Integer(), nullable=False),
        sa.ForeignKeyConstraint(['role_id'], ['roles.id'], ondelete='CASCADE'),
        sa.ForeignKeyConstraint(['user_id'], ['users.id'], ondelete='CASCADE'),
        sa.PrimaryKeyConstraint('user_id', 'role_id')
    )
    
    # Create role_permissions junction table
    op.create_table(
        'role_permissions',
        sa.Column('role_id', sa.Integer(), nullable=False),
        sa.Column('permission_id', sa.Integer(), nullable=False),
        sa.ForeignKeyConstraint(['permission_id'], ['permissions.id'], ondelete='CASCADE'),
        sa.ForeignKeyConstraint(['role_id'], ['roles.id'], ondelete='CASCADE'),
        sa.PrimaryKeyConstraint('role_id', 'permission_id')
    )
    
    # Create sessions table
    op.create_table(
        'sessions',
        sa.Column('id', sa.Integer(), nullable=False),
        sa.Column('user_id', sa.Integer(), nullable=False),
        sa.Column('token', sa.String(length=500), nullable=False),
        sa.Column('refresh_token', sa.String(length=500), nullable=True),
        sa.Column('ip_address', sa.String(length=50), nullable=True),
        sa.Column('user_agent', sa.String(length=500), nullable=True),
        sa.Column('is_active', sa.Boolean(), nullable=False),
        sa.Column('expires_at', sa.DateTime(), nullable=False),
        sa.Column('created_at', sa.DateTime(), nullable=False),
        sa.Column('last_activity', sa.DateTime(), nullable=False),
        sa.ForeignKeyConstraint(['user_id'], ['users.id'], ondelete='CASCADE'),
        sa.PrimaryKeyConstraint('id')
    )
    op.create_index(op.f('ix_sessions_id'), 'sessions', ['id'], unique=False)
    op.create_index(op.f('ix_sessions_token'), 'sessions', ['token'], unique=True)
    op.create_index('ix_sessions_user_active', 'sessions', ['user_id', 'is_active'], unique=False)
    
    # Create chat_sessions table
    op.create_table(
        'chat_sessions',
        sa.Column('id', sa.Integer(), nullable=False),
        sa.Column('user_id', sa.Integer(), nullable=False),
        sa.Column('title', sa.String(length=500), nullable=False),
        sa.Column('model', sa.String(length=100), nullable=True),
        sa.Column('temperature', sa.String(length=10), nullable=False),
        sa.Column('context_window_size', sa.Integer(), nullable=False),
        sa.Column('is_active', sa.Boolean(), nullable=False),
        sa.Column('created_at', sa.DateTime(), nullable=False),
        sa.Column('updated_at', sa.DateTime(), nullable=False),
        sa.Column('last_message_at', sa.DateTime(), nullable=False),
        sa.ForeignKeyConstraint(['user_id'], ['users.id'], ondelete='CASCADE'),
        sa.PrimaryKeyConstraint('id')
    )
    op.create_index(op.f('ix_chat_sessions_id'), 'chat_sessions', ['id'], unique=False)
    op.create_index('ix_chat_sessions_user_active', 'chat_sessions', ['user_id', 'is_active'], unique=False)
    
    # Create chat_messages table
    op.create_table(
        'chat_messages',
        sa.Column('id', sa.Integer(), nullable=False),
        sa.Column('session_id', sa.Integer(), nullable=False),
        sa.Column('user_id', sa.Integer(), nullable=False),
        sa.Column('role', sa.String(length=20), nullable=False),
        sa.Column('content', sa.Text(), nullable=False),
        sa.Column('tokens', sa.Integer(), nullable=True),
        sa.Column('model', sa.String(length=100), nullable=True),
        sa.Column('meta_data', sa.JSON(), nullable=True),
        sa.Column('tool_execution_id', sa.Integer(), nullable=True),
        sa.Column('created_at', sa.DateTime(), nullable=False),
        sa.ForeignKeyConstraint(['session_id'], ['chat_sessions.id'], ondelete='CASCADE'),
        sa.ForeignKeyConstraint(['user_id'], ['users.id'], ondelete='CASCADE'),
        sa.PrimaryKeyConstraint('id')
    )
    op.create_index(op.f('ix_chat_messages_id'), 'chat_messages', ['id'], unique=False)
    op.create_index('ix_chat_messages_session', 'chat_messages', ['session_id', 'created_at'], unique=False)
    
    # Create context_windows table
    op.create_table(
        'context_windows',
        sa.Column('id', sa.Integer(), nullable=False),
        sa.Column('session_id', sa.Integer(), nullable=False),
        sa.Column('max_tokens', sa.Integer(), nullable=False),
        sa.Column('current_tokens', sa.Integer(), nullable=False),
        sa.Column('message_count', sa.Integer(), nullable=False),
        sa.Column('strategy', sa.String(length=50), nullable=False),
        sa.Column('updated_at', sa.DateTime(), nullable=False),
        sa.ForeignKeyConstraint(['session_id'], ['chat_sessions.id'], ondelete='CASCADE'),
        sa.PrimaryKeyConstraint('id')
    )
    op.create_index(op.f('ix_context_windows_id'), 'context_windows', ['id'], unique=False)
    op.create_index(op.f('ix_context_windows_session_id'), 'context_windows', ['session_id'], unique=True)
    
    # Create tools table
    op.create_table(
        'tools',
        sa.Column('id', sa.Integer(), nullable=False),
        sa.Column('name', sa.String(length=100), nullable=False),
        sa.Column('display_name', sa.String(length=200), nullable=False),
        sa.Column('description', sa.Text(), nullable=False),
        sa.Column('category', sa.String(length=50), nullable=False),
        sa.Column('version', sa.String(length=20), nullable=False),
        sa.Column('python_function', sa.String(length=200), nullable=False),
        sa.Column('parameters_schema', sa.JSON(), nullable=False),
        sa.Column('return_schema', sa.JSON(), nullable=True),
        sa.Column('status', sa.String(length=20), nullable=False),
        sa.Column('requires_approval', sa.Boolean(), nullable=False),
        sa.Column('required_permission', sa.String(length=100), nullable=True),
        sa.Column('timeout_seconds', sa.Integer(), nullable=False),
        sa.Column('max_retries', sa.Integer(), nullable=False),
        sa.Column('execution_count', sa.Integer(), nullable=False),
        sa.Column('success_count', sa.Integer(), nullable=False),
        sa.Column('failure_count', sa.Integer(), nullable=False),
        sa.Column('avg_execution_time', sa.Float(), nullable=True),
        sa.Column('last_executed_at', sa.DateTime(), nullable=True),
        sa.Column('created_at', sa.DateTime(), nullable=False),
        sa.Column('updated_at', sa.DateTime(), nullable=False),
        sa.PrimaryKeyConstraint('id')
    )
    op.create_index(op.f('ix_tools_id'), 'tools', ['id'], unique=False)
    op.create_index(op.f('ix_tools_name'), 'tools', ['name'], unique=True)
    op.create_index('ix_tools_category_status', 'tools', ['category', 'status'], unique=False)
    
    # Create tool_executions table
    op.create_table(
        'tool_executions',
        sa.Column('id', sa.Integer(), nullable=False),
        sa.Column('tool_id', sa.Integer(), nullable=False),
        sa.Column('user_id', sa.Integer(), nullable=False),
        sa.Column('session_id', sa.Integer(), nullable=True),
        sa.Column('status', sa.String(length=20), nullable=False),
        sa.Column('input_data', sa.JSON(), nullable=False),
        sa.Column('output_data', sa.JSON(), nullable=True),
        sa.Column('error_message', sa.Text(), nullable=True),
        sa.Column('started_at', sa.DateTime(), nullable=True),
        sa.Column('completed_at', sa.DateTime(), nullable=True),
        sa.Column('execution_time', sa.Float(), nullable=True),
        sa.Column('retry_count', sa.Integer(), nullable=False),
        sa.Column('requires_approval', sa.Boolean(), nullable=False),
        sa.Column('created_at', sa.DateTime(), nullable=False),
        sa.ForeignKeyConstraint(['session_id'], ['chat_sessions.id'], ondelete='SET NULL'),
        sa.ForeignKeyConstraint(['tool_id'], ['tools.id'], ondelete='CASCADE'),
        sa.ForeignKeyConstraint(['user_id'], ['users.id'], ondelete='CASCADE'),
        sa.PrimaryKeyConstraint('id')
    )
    op.create_index(op.f('ix_tool_executions_id'), 'tool_executions', ['id'], unique=False)
    op.create_index('ix_tool_executions_tool_status', 'tool_executions', ['tool_id', 'status'], unique=False)
    op.create_index('ix_tool_executions_user_created', 'tool_executions', ['user_id', 'created_at'], unique=False)
    
    # Create tool_approvals table
    op.create_table(
        'tool_approvals',
        sa.Column('id', sa.Integer(), nullable=False),
        sa.Column('execution_id', sa.Integer(), nullable=False),
        sa.Column('user_id', sa.Integer(), nullable=False),
        sa.Column('status', sa.String(length=20), nullable=False),
        sa.Column('reason', sa.Text(), nullable=True),
        sa.Column('notes', sa.Text(), nullable=True),
        sa.Column('requested_at', sa.DateTime(), nullable=False),
        sa.Column('responded_at', sa.DateTime(), nullable=True),
        sa.Column('expires_at', sa.DateTime(), nullable=True),
        sa.ForeignKeyConstraint(['execution_id'], ['tool_executions.id'], ondelete='CASCADE'),
        sa.ForeignKeyConstraint(['user_id'], ['users.id'], ondelete='CASCADE'),
        sa.PrimaryKeyConstraint('id')
    )
    op.create_index(op.f('ix_tool_approvals_id'), 'tool_approvals', ['id'], unique=False)
    op.create_index('ix_tool_approvals_status_requested', 'tool_approvals', ['status', 'requested_at'], unique=False)
    op.create_index('ix_tool_approvals_user_status', 'tool_approvals', ['user_id', 'status'], unique=False)
    
    # Create tool_cache table
    op.create_table(
        'tool_cache',
        sa.Column('id', sa.Integer(), nullable=False),
        sa.Column('tool_id', sa.Integer(), nullable=False),
        sa.Column('input_hash', sa.String(length=64), nullable=False),
        sa.Column('input_data', sa.JSON(), nullable=False),
        sa.Column('output_data', sa.JSON(), nullable=False),
        sa.Column('execution_time', sa.Float(), nullable=False),
        sa.Column('hit_count', sa.Integer(), nullable=False),
        sa.Column('created_at', sa.DateTime(), nullable=False),
        sa.Column('last_accessed', sa.DateTime(), nullable=False),
        sa.Column('expires_at', sa.DateTime(), nullable=True),
        sa.ForeignKeyConstraint(['tool_id'], ['tools.id'], ondelete='CASCADE'),
        sa.PrimaryKeyConstraint('id')
    )
    op.create_index(op.f('ix_tool_cache_id'), 'tool_cache', ['id'], unique=False)
    op.create_index('ix_tool_cache_tool_hash', 'tool_cache', ['tool_id', 'input_hash'], unique=True)
    
    # Create documents table
    op.create_table(
        'documents',
        sa.Column('id', sa.Integer(), nullable=False),
        sa.Column('title', sa.String(length=500), nullable=False),
        sa.Column('source', sa.String(length=1000), nullable=False),
        sa.Column('source_type', sa.String(length=50), nullable=False),
        sa.Column('file_type', sa.String(length=50), nullable=True),
        sa.Column('file_size', sa.Integer(), nullable=True),
        sa.Column('tags', postgresql.ARRAY(sa.String()), nullable=True),
        sa.Column('meta_data', sa.JSON(), nullable=True),
        sa.Column('is_processed', sa.Boolean(), nullable=False),
        sa.Column('is_indexed', sa.Boolean(), nullable=False),
        sa.Column('processing_error', sa.Text(), nullable=True),
        sa.Column('uploaded_by', sa.Integer(), nullable=True),
        sa.Column('is_public', sa.Boolean(), nullable=False),
        sa.Column('required_permission', sa.String(length=100), nullable=True),
        sa.Column('created_at', sa.DateTime(), nullable=False),
        sa.Column('updated_at', sa.DateTime(), nullable=False),
        sa.Column('last_accessed', sa.DateTime(), nullable=True),
        sa.ForeignKeyConstraint(['uploaded_by'], ['users.id'], ondelete='SET NULL'),
        sa.PrimaryKeyConstraint('id')
    )
    op.create_index(op.f('ix_documents_id'), 'documents', ['id'], unique=False)
    op.create_index('ix_documents_indexed_public', 'documents', ['is_indexed', 'is_public'], unique=False)
    op.create_index('ix_documents_source_type', 'documents', ['source_type', 'created_at'], unique=False)
    
    # Create embedding_models table
    op.create_table(
        'embedding_models',
        sa.Column('id', sa.Integer(), nullable=False),
        sa.Column('name', sa.String(length=100), nullable=False),
        sa.Column('display_name', sa.String(length=200), nullable=False),
        sa.Column('provider', sa.String(length=50), nullable=False),
        sa.Column('model_id', sa.String(length=200), nullable=False),
        sa.Column('dimension', sa.Integer(), nullable=False),
        sa.Column('max_tokens', sa.Integer(), nullable=False),
        sa.Column('cost_per_1k_tokens', sa.Float(), nullable=True),
        sa.Column('is_active', sa.Boolean(), nullable=False),
        sa.Column('is_default', sa.Boolean(), nullable=False),
        sa.Column('avg_latency_ms', sa.Float(), nullable=True),
        sa.Column('created_at', sa.DateTime(), nullable=False),
        sa.PrimaryKeyConstraint('id')
    )
    op.create_index(op.f('ix_embedding_models_id'), 'embedding_models', ['id'], unique=False)
    op.create_index(op.f('ix_embedding_models_name'), 'embedding_models', ['name'], unique=True)
    
    # Create document_chunks table (with pgvector extension)
    op.create_table(
        'document_chunks',
        sa.Column('id', sa.Integer(), nullable=False),
        sa.Column('document_id', sa.Integer(), nullable=False),
        sa.Column('chunk_index', sa.Integer(), nullable=False),
        sa.Column('content', sa.Text(), nullable=False),
        sa.Column('embedding', postgresql.ARRAY(sa.Float()), nullable=True),
        sa.Column('embedding_model_id', sa.Integer(), nullable=True),
        sa.Column('token_count', sa.Integer(), nullable=True),
        sa.Column('char_count', sa.Integer(), nullable=True),
        sa.Column('meta_data', sa.JSON(), nullable=True),
        sa.Column('search_keywords', postgresql.ARRAY(sa.String()), nullable=True),
        sa.Column('created_at', sa.DateTime(), nullable=False),
        sa.ForeignKeyConstraint(['document_id'], ['documents.id'], ondelete='CASCADE'),
        sa.ForeignKeyConstraint(['embedding_model_id'], ['embedding_models.id'], ondelete='SET NULL'),
        sa.PrimaryKeyConstraint('id')
    )
    op.create_index(op.f('ix_document_chunks_id'), 'document_chunks', ['id'], unique=False)
    op.create_index('ix_document_chunks_doc_index', 'document_chunks', ['document_id', 'chunk_index'], unique=True)
    
    # Create search_results table
    op.create_table(
        'search_results',
        sa.Column('id', sa.Integer(), nullable=False),
        sa.Column('chunk_id', sa.Integer(), nullable=False),
        sa.Column('query', sa.Text(), nullable=False),
        sa.Column('similarity_score', sa.Float(), nullable=False),
        sa.Column('search_type', sa.String(length=50), nullable=False),
        sa.Column('user_id', sa.Integer(), nullable=True),
        sa.Column('created_at', sa.DateTime(), nullable=False),
        sa.ForeignKeyConstraint(['chunk_id'], ['document_chunks.id'], ondelete='CASCADE'),
        sa.ForeignKeyConstraint(['user_id'], ['users.id'], ondelete='SET NULL'),
        sa.PrimaryKeyConstraint('id')
    )
    op.create_index(op.f('ix_search_results_id'), 'search_results', ['id'], unique=False)
    op.create_index('ix_search_results_user_created', 'search_results', ['user_id', 'created_at'], unique=False)
    
    # Create secrets table
    op.create_table(
        'secrets',
        sa.Column('id', sa.Integer(), nullable=False),
        sa.Column('name', sa.String(length=200), nullable=False),
        sa.Column('description', sa.Text(), nullable=True),
        sa.Column('secret_type', sa.String(length=50), nullable=False),
        sa.Column('encrypted_value', sa.Text(), nullable=False),
        sa.Column('encryption_key_id', sa.String(length=100), nullable=False),
        sa.Column('version', sa.Integer(), nullable=False),
        sa.Column('is_active', sa.Boolean(), nullable=False),
        sa.Column('created_by', sa.Integer(), nullable=False),
        sa.Column('tags', postgresql.ARRAY(sa.String()), nullable=True),
        sa.Column('meta_data', sa.JSON(), nullable=True),
        sa.Column('rotation_enabled', sa.Boolean(), nullable=False),
        sa.Column('rotation_interval_days', sa.Integer(), nullable=True),
        sa.Column('last_rotated_at', sa.DateTime(), nullable=True),
        sa.Column('next_rotation_at', sa.DateTime(), nullable=True),
        sa.Column('expires_at', sa.DateTime(), nullable=True),
        sa.Column('created_at', sa.DateTime(), nullable=False),
        sa.Column('updated_at', sa.DateTime(), nullable=False),
        sa.ForeignKeyConstraint(['created_by'], ['users.id'], ondelete='RESTRICT'),
        sa.PrimaryKeyConstraint('id')
    )
    op.create_index(op.f('ix_secrets_id'), 'secrets', ['id'], unique=False)
    op.create_index(op.f('ix_secrets_name'), 'secrets', ['name'], unique=True)
    op.create_index('ix_secrets_active_type', 'secrets', ['is_active', 'secret_type'], unique=False)
    
    # Create secret_versions table
    op.create_table(
        'secret_versions',
        sa.Column('id', sa.Integer(), nullable=False),
        sa.Column('secret_id', sa.Integer(), nullable=False),
        sa.Column('version', sa.Integer(), nullable=False),
        sa.Column('encrypted_value', sa.Text(), nullable=False),
        sa.Column('encryption_key_id', sa.String(length=100), nullable=False),
        sa.Column('is_active', sa.Boolean(), nullable=False),
        sa.Column('created_by', sa.Integer(), nullable=False),
        sa.Column('created_at', sa.DateTime(), nullable=False),
        sa.Column('expires_at', sa.DateTime(), nullable=True),
        sa.ForeignKeyConstraint(['created_by'], ['users.id'], ondelete='RESTRICT'),
        sa.ForeignKeyConstraint(['secret_id'], ['secrets.id'], ondelete='CASCADE'),
        sa.PrimaryKeyConstraint('id')
    )
    op.create_index(op.f('ix_secret_versions_id'), 'secret_versions', ['id'], unique=False)
    op.create_index('ix_secret_versions_secret_version', 'secret_versions', ['secret_id', 'version'], unique=True)
    
    # Create secret_access_logs table
    op.create_table(
        'secret_access_logs',
        sa.Column('id', sa.Integer(), nullable=False),
        sa.Column('secret_id', sa.Integer(), nullable=False),
        sa.Column('user_id', sa.Integer(), nullable=False),
        sa.Column('access_type', sa.String(length=50), nullable=False),
        sa.Column('ip_address', sa.String(length=50), nullable=True),
        sa.Column('user_agent', sa.String(length=500), nullable=True),
        sa.Column('accessed_at', sa.DateTime(), nullable=False),
        sa.ForeignKeyConstraint(['secret_id'], ['secrets.id'], ondelete='CASCADE'),
        sa.ForeignKeyConstraint(['user_id'], ['users.id'], ondelete='CASCADE'),
        sa.PrimaryKeyConstraint('id')
    )
    op.create_index(op.f('ix_secret_access_logs_id'), 'secret_access_logs', ['id'], unique=False)
    op.create_index('ix_secret_access_logs_secret_accessed', 'secret_access_logs', ['secret_id', 'accessed_at'], unique=False)
    op.create_index('ix_secret_access_logs_user_accessed', 'secret_access_logs', ['user_id', 'accessed_at'], unique=False)
    
    # Create audit_logs table
    op.create_table(
        'audit_logs',
        sa.Column('id', sa.Integer(), nullable=False),
        sa.Column('user_id', sa.Integer(), nullable=True),
        sa.Column('action', sa.String(length=100), nullable=False),
        sa.Column('resource_type', sa.String(length=100), nullable=False),
        sa.Column('resource_id', sa.Integer(), nullable=True),
        sa.Column('details', sa.JSON(), nullable=True),
        sa.Column('ip_address', sa.String(length=50), nullable=True),
        sa.Column('user_agent', sa.String(length=500), nullable=True),
        sa.Column('session_id', sa.Integer(), nullable=True),
        sa.Column('tool_execution_id', sa.Integer(), nullable=True),
        sa.Column('created_at', sa.DateTime(), nullable=False),
        sa.ForeignKeyConstraint(['session_id'], ['sessions.id'], ondelete='SET NULL'),
        sa.ForeignKeyConstraint(['tool_execution_id'], ['tool_executions.id'], ondelete='SET NULL'),
        sa.ForeignKeyConstraint(['user_id'], ['users.id'], ondelete='SET NULL'),
        sa.PrimaryKeyConstraint('id')
    )
    op.create_index(op.f('ix_audit_logs_id'), 'audit_logs', ['id'], unique=False)
    op.create_index('ix_audit_logs_action_created', 'audit_logs', ['action', 'created_at'], unique=False)
    op.create_index('ix_audit_logs_resource', 'audit_logs', ['resource_type', 'resource_id'], unique=False)
    op.create_index('ix_audit_logs_user_created', 'audit_logs', ['user_id', 'created_at'], unique=False)
    
    # Create system_metrics table
    op.create_table(
        'system_metrics',
        sa.Column('id', sa.Integer(), nullable=False),
        sa.Column('metric_name', sa.String(length=100), nullable=False),
        sa.Column('metric_value', sa.Float(), nullable=False),
        sa.Column('metric_type', sa.String(length=50), nullable=False),
        sa.Column('tags', sa.JSON(), nullable=True),
        sa.Column('recorded_at', sa.DateTime(), nullable=False),
        sa.PrimaryKeyConstraint('id')
    )
    op.create_index(op.f('ix_system_metrics_id'), 'system_metrics', ['id'], unique=False)
    op.create_index('ix_system_metrics_name_recorded', 'system_metrics', ['metric_name', 'recorded_at'], unique=False)
    
    # Create notifications table
    op.create_table(
        'notifications',
        sa.Column('id', sa.Integer(), nullable=False),
        sa.Column('user_id', sa.Integer(), nullable=False),
        sa.Column('type', sa.String(length=100), nullable=False),
        sa.Column('title', sa.String(length=500), nullable=False),
        sa.Column('message', sa.Text(), nullable=False),
        sa.Column('data', sa.JSON(), nullable=True),
        sa.Column('priority', sa.String(length=20), nullable=False),
        sa.Column('is_read', sa.Boolean(), nullable=False),
        sa.Column('read_at', sa.DateTime(), nullable=True),
        sa.Column('expires_at', sa.DateTime(), nullable=True),
        sa.Column('created_at', sa.DateTime(), nullable=False),
        sa.ForeignKeyConstraint(['user_id'], ['users.id'], ondelete='CASCADE'),
        sa.PrimaryKeyConstraint('id')
    )
    op.create_index(op.f('ix_notifications_id'), 'notifications', ['id'], unique=False)
    op.create_index('idx_notifications_priority', 'notifications', ['priority', 'created_at'], unique=False)
    op.create_index('idx_notifications_type', 'notifications', ['type'], unique=False)
    op.create_index('idx_notifications_unread', 'notifications', ['user_id', 'is_read', 'created_at'], unique=False)
    op.create_index('idx_notifications_user', 'notifications', ['user_id'], unique=False)
    
    # Create notification_preferences table
    op.create_table(
        'notification_preferences',
        sa.Column('id', sa.Integer(), nullable=False),
        sa.Column('user_id', sa.Integer(), nullable=False),
        sa.Column('notification_type', sa.String(length=100), nullable=False),
        sa.Column('enabled', sa.Boolean(), nullable=False),
        sa.Column('delivery_method', sa.String(length=50), nullable=False),
        sa.Column('created_at', sa.DateTime(), nullable=False),
        sa.Column('updated_at', sa.DateTime(), nullable=False),
        sa.ForeignKeyConstraint(['user_id'], ['users.id'], ondelete='CASCADE'),
        sa.PrimaryKeyConstraint('id')
    )
    op.create_index(op.f('ix_notification_preferences_id'), 'notification_preferences', ['id'], unique=False)
    op.create_index('idx_notification_prefs_user_type', 'notification_preferences', ['user_id', 'notification_type'], unique=True)


def downgrade() -> None:
    """Drop all database tables."""
    op.drop_table('notification_preferences')
    op.drop_table('notifications')
    op.drop_table('system_metrics')
    op.drop_table('audit_logs')
    op.drop_table('secret_access_logs')
    op.drop_table('secret_versions')
    op.drop_table('secrets')
    op.drop_table('search_results')
    op.drop_table('document_chunks')
    op.drop_table('embedding_models')
    op.drop_table('documents')
    op.drop_table('tool_cache')
    op.drop_table('tool_approvals')
    op.drop_table('tool_executions')
    op.drop_table('tools')
    op.drop_table('context_windows')
    op.drop_table('chat_messages')
    op.drop_table('chat_sessions')
    op.drop_table('sessions')
    op.drop_table('role_permissions')
    op.drop_table('user_roles')
    op.drop_table('users')
    op.drop_table('permissions')
    op.drop_table('roles')
</file>

<file path="alembic/env.py">
"""Alembic environment configuration for database migrations."""
from logging.config import fileConfig
import sys
from pathlib import Path

from sqlalchemy import engine_from_config
from sqlalchemy import pool
from alembic import context

# Add parent directory to path for imports
sys.path.append(str(Path(__file__).resolve().parents[1]))

from app.config import settings
from app.db.base import Base

# Don't import models during migrations - we use explicit SQL instead
# from app.models import *  # Import all models to ensure they're registered

# Alembic Config object
config = context.config

# Interpret the config file for Python logging
if config.config_file_name is not None:
    fileConfig(config.config_file_name)

# Add your model's MetaData object here for 'autogenerate' support
target_metadata = Base.metadata

# Override sqlalchemy.url from config with our settings
# Convert asyncpg URL to psycopg2 for synchronous migrations
database_url = settings.DATABASE_URL.replace("postgresql+asyncpg://", "postgresql://")
config.set_main_option("sqlalchemy.url", database_url)


def run_migrations_offline() -> None:
    """Run migrations in 'offline' mode.

    This configures the context with just a URL
    and not an Engine, though an Engine is acceptable
    here as well. By skipping the Engine creation
    we don't even need a DBAPI to be available.

    Calls to context.execute() here emit the given string to the
    script output.
    """
    url = config.get_main_option("sqlalchemy.url")
    context.configure(
        url=url,
        target_metadata=target_metadata,
        literal_binds=True,
        dialect_opts={"paramstyle": "named"},
        compare_type=True,
        compare_server_default=True,
    )

    with context.begin_transaction():
        context.run_migrations()


def run_migrations_online() -> None:
    """Run migrations in 'online' mode.

    In this scenario we need to create an Engine
    and associate a connection with the context.
    """
    connectable = engine_from_config(
        config.get_section(config.config_ini_section, {}),
        prefix="sqlalchemy.",
        poolclass=pool.NullPool,
    )

    with connectable.connect() as connection:
        context.configure(
            connection=connection,
            target_metadata=target_metadata,
            compare_type=True,
            compare_server_default=True,
        )

        with context.begin_transaction():
            context.run_migrations()


if context.is_offline_mode():
    run_migrations_offline()
else:
    run_migrations_online()
</file>

<file path="alembic/script.py.mako">
"""${message}

Revision ID: ${up_revision}
Revises: ${down_revision | comma,n}
Create Date: ${create_date}

"""
from typing import Sequence, Union

from alembic import op
import sqlalchemy as sa
${imports if imports else ""}

# revision identifiers, used by Alembic.
revision: str = ${repr(up_revision)}
down_revision: Union[str, None] = ${repr(down_revision)}
branch_labels: Union[str, Sequence[str], None] = ${repr(branch_labels)}
depends_on: Union[str, Sequence[str], None] = ${repr(depends_on)}


def upgrade() -> None:
    ${upgrades if upgrades else "pass"}


def downgrade() -> None:
    ${downgrades if downgrades else "pass"}
</file>

<file path="app/api/v1/audit.py">
"""
Audit logging API endpoints.
Handles audit log queries, system metrics, and statistics.
"""
from datetime import datetime, timedelta
from typing import List, Optional
from fastapi import APIRouter, Depends, HTTPException, status, Query, Request
from sqlalchemy import select, func, and_, or_, desc
from sqlalchemy.ext.asyncio import AsyncSession

from app.core.deps import (
    get_current_user,
    get_optional_user,
    require_permission,
    get_db
)
from app.models.user import User
from app.models.audit import AuditLog, AuditAction, SystemMetric
from app.models.tool import ToolExecution
from app.models.chat import ChatSession
from app.schemas.audit import (
    AuditLogCreate,
    AuditLogResponse,
    AuditLogListResponse,
    AuditLogFilterParams,
    SystemMetricCreate,
    SystemMetricResponse,
    SystemMetricListResponse,
    AuditStatistics,
    SystemHealthMetrics
)

router = APIRouter(prefix="/audit", tags=["audit"])


# ============================================================================
# Audit Log Management
# ============================================================================

async def create_audit_log(
    db: AsyncSession,
    user_id: Optional[int],
    action: AuditAction,
    resource_type: str,
    resource_id: Optional[int] = None,
    details: Optional[dict] = None,
    ip_address: Optional[str] = None,
    user_agent: Optional[str] = None,
    session_id: Optional[int] = None,
    tool_execution_id: Optional[int] = None
):
    """
    Helper function to create audit log entries.
    Can be called from other parts of the application.
    """
    log = AuditLog(
        user_id=user_id,
        action=action,
        resource_type=resource_type,
        resource_id=resource_id,
        details=details,
        ip_address=ip_address,
        user_agent=user_agent,
        session_id=session_id,
        tool_execution_id=tool_execution_id
    )
    
    db.add(log)
    await db.commit()
    await db.refresh(log)
    
    return log


@router.post("/logs", response_model=AuditLogResponse, status_code=status.HTTP_201_CREATED)
async def create_audit_log_entry(
    log_data: AuditLogCreate,
    request: Request,
    db: AsyncSession = Depends(get_db),
    current_user: User = Depends(get_optional_user)
):
    """
    Create a new audit log entry.
    Can be used by authenticated users or system processes.
    """
    # Get IP address from request
    ip_address = log_data.ip_address or request.client.host if request.client else None
    user_agent = log_data.user_agent or request.headers.get("user-agent")
    
    log = await create_audit_log(
        db=db,
        user_id=current_user.id if current_user else None,
        action=log_data.action,
        resource_type=log_data.resource_type,
        resource_id=log_data.resource_id,
        details=log_data.details,
        ip_address=ip_address,
        user_agent=user_agent
    )
    
    # Get username if available
    username = current_user.username if current_user else None
    
    return AuditLogResponse(
        id=log.id,
        user_id=log.user_id,
        username=username,
        action=log.action,
        resource_type=log.resource_type,
        resource_id=log.resource_id,
        details=log.details,
        ip_address=log.ip_address,
        user_agent=log.user_agent,
        session_id=log.session_id,
        tool_execution_id=log.tool_execution_id,
        created_at=log.created_at
    )


@router.get("/logs", response_model=AuditLogListResponse)
async def list_audit_logs(
    page: int = Query(1, ge=1),
    page_size: int = Query(50, ge=1, le=100),
    user_id: Optional[int] = Query(None),
    action: Optional[AuditAction] = Query(None),
    resource_type: Optional[str] = Query(None),
    resource_id: Optional[int] = Query(None),
    start_date: Optional[datetime] = Query(None),
    end_date: Optional[datetime] = Query(None),
    search: Optional[str] = Query(None, description="Search in details"),
    db: AsyncSession = Depends(get_db),
    current_user: User = Depends(require_permission("audit.view"))
):
    """
    List audit logs with filtering and pagination.
    Requires 'audit.view' permission.
    """
    # Build query
    query = select(AuditLog)
    
    # Apply filters
    filters = []
    if user_id:
        filters.append(AuditLog.user_id == user_id)
    if action:
        filters.append(AuditLog.action == action)
    if resource_type:
        filters.append(AuditLog.resource_type == resource_type)
    if resource_id:
        filters.append(AuditLog.resource_id == resource_id)
    if start_date:
        filters.append(AuditLog.created_at >= start_date)
    if end_date:
        filters.append(AuditLog.created_at <= end_date)
    
    if filters:
        query = query.where(and_(*filters))
    
    # Get total count
    count_query = select(func.count()).select_from(AuditLog)
    if filters:
        count_query = count_query.where(and_(*filters))
    result = await db.execute(count_query)
    total = result.scalar()
    
    # Apply pagination and ordering
    query = query.offset((page - 1) * page_size).limit(page_size)
    query = query.order_by(desc(AuditLog.created_at))
    
    result = await db.execute(query)
    logs = result.scalars().all()
    
    # Get usernames
    user_ids = [log.user_id for log in logs if log.user_id]
    if user_ids:
        users_result = await db.execute(
            select(User).where(User.id.in_(user_ids))
        )
        users = {u.id: u.username for u in users_result.scalars().all()}
    else:
        users = {}
    
    log_responses = [
        AuditLogResponse(
            id=log.id,
            user_id=log.user_id,
            username=users.get(log.user_id) if log.user_id else None,
            action=log.action,
            resource_type=log.resource_type,
            resource_id=log.resource_id,
            details=log.details,
            ip_address=log.ip_address,
            user_agent=log.user_agent,
            session_id=log.session_id,
            tool_execution_id=log.tool_execution_id,
            created_at=log.created_at
        )
        for log in logs
    ]
    
    return AuditLogListResponse(
        logs=log_responses,
        total=total,
        page=page,
        page_size=page_size
    )


@router.get("/logs/{log_id}", response_model=AuditLogResponse)
async def get_audit_log(
    log_id: int,
    db: AsyncSession = Depends(get_db),
    current_user: User = Depends(require_permission("audit.view"))
):
    """
    Get a specific audit log entry.
    Requires 'audit.view' permission.
    """
    result = await db.execute(
        select(AuditLog).where(AuditLog.id == log_id)
    )
    log = result.scalar_one_or_none()
    
    if not log:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail=f"Audit log with ID {log_id} not found"
        )
    
    # Get username
    username = None
    if log.user_id:
        user_result = await db.execute(
            select(User).where(User.id == log.user_id)
        )
        user = user_result.scalar_one_or_none()
        if user:
            username = user.username
    
    return AuditLogResponse(
        id=log.id,
        user_id=log.user_id,
        username=username,
        action=log.action,
        resource_type=log.resource_type,
        resource_id=log.resource_id,
        details=log.details,
        ip_address=log.ip_address,
        user_agent=log.user_agent,
        session_id=log.session_id,
        tool_execution_id=log.tool_execution_id,
        created_at=log.created_at
    )


# ============================================================================
# Audit Statistics
# ============================================================================

@router.get("/statistics", response_model=AuditStatistics)
async def get_audit_statistics(
    days: int = Query(30, ge=1, le=365, description="Number of days to analyze"),
    db: AsyncSession = Depends(get_db),
    current_user: User = Depends(require_permission("audit.view"))
):
    """
    Get audit statistics for the specified period.
    Requires 'audit.view' permission.
    """
    start_date = datetime.utcnow() - timedelta(days=days)
    
    # Total events
    total_result = await db.execute(
        select(func.count()).select_from(AuditLog)
        .where(AuditLog.created_at >= start_date)
    )
    total_events = total_result.scalar()
    
    # Events by action
    action_result = await db.execute(
        select(
            AuditLog.action,
            func.count().label('count')
        )
        .where(AuditLog.created_at >= start_date)
        .group_by(AuditLog.action)
    )
    events_by_action = {row.action.value: row.count for row in action_result}
    
    # Events by resource type
    resource_result = await db.execute(
        select(
            AuditLog.resource_type,
            func.count().label('count')
        )
        .where(AuditLog.created_at >= start_date)
        .group_by(AuditLog.resource_type)
    )
    events_by_resource = {row.resource_type: row.count for row in resource_result}
    
    # Unique users
    users_result = await db.execute(
        select(func.count(func.distinct(AuditLog.user_id)))
        .where(and_(
            AuditLog.created_at >= start_date,
            AuditLog.user_id.is_not(None)
        ))
    )
    unique_users = users_result.scalar()
    
    # Unique IPs
    ips_result = await db.execute(
        select(func.count(func.distinct(AuditLog.ip_address)))
        .where(and_(
            AuditLog.created_at >= start_date,
            AuditLog.ip_address.is_not(None)
        ))
    )
    unique_ips = ips_result.scalar()
    
    # Events last 24h
    last_24h_result = await db.execute(
        select(func.count()).select_from(AuditLog)
        .where(AuditLog.created_at >= datetime.utcnow() - timedelta(hours=24))
    )
    events_last_24h = last_24h_result.scalar()
    
    # Events last 7d
    last_7d_result = await db.execute(
        select(func.count()).select_from(AuditLog)
        .where(AuditLog.created_at >= datetime.utcnow() - timedelta(days=7))
    )
    events_last_7d = last_7d_result.scalar()
    
    # Events last 30d
    last_30d_result = await db.execute(
        select(func.count()).select_from(AuditLog)
        .where(AuditLog.created_at >= datetime.utcnow() - timedelta(days=30))
    )
    events_last_30d = last_30d_result.scalar()
    
    # Top users
    top_users_result = await db.execute(
        select(
            User.id,
            User.username,
            func.count().label('event_count')
        )
        .join(AuditLog, AuditLog.user_id == User.id)
        .where(AuditLog.created_at >= start_date)
        .group_by(User.id, User.username)
        .order_by(desc('event_count'))
        .limit(10)
    )
    top_users = [
        {
            "user_id": row.id,
            "username": row.username,
            "event_count": row.event_count
        }
        for row in top_users_result
    ]
    
    # Recent failures (errors in last 24h)
    failures_result = await db.execute(
        select(func.count()).select_from(AuditLog)
        .where(and_(
            AuditLog.created_at >= datetime.utcnow() - timedelta(hours=24),
            or_(
                AuditLog.action == AuditAction.LOGIN_FAILED,
                AuditLog.action == AuditAction.PERMISSION_DENIED,
                AuditLog.action == AuditAction.TOOL_EXECUTION_FAILED
            )
        ))
    )
    recent_failures = failures_result.scalar()
    
    return AuditStatistics(
        total_events=total_events,
        events_by_action=events_by_action,
        events_by_resource=events_by_resource,
        unique_users=unique_users,
        unique_ips=unique_ips,
        events_last_24h=events_last_24h,
        events_last_7d=events_last_7d,
        events_last_30d=events_last_30d,
        top_users=top_users,
        recent_failures=recent_failures
    )


# ============================================================================
# System Metrics
# ============================================================================

@router.post("/metrics", response_model=SystemMetricResponse, status_code=status.HTTP_201_CREATED)
async def record_metric(
    metric_data: SystemMetricCreate,
    db: AsyncSession = Depends(get_db),
    current_user: User = Depends(require_permission("metrics.record"))
):
    """
    Record a system metric.
    Requires 'metrics.record' permission.
    """
    metric = SystemMetric(
        metric_name=metric_data.metric_name,
        metric_value=metric_data.metric_value,
        metric_type=metric_data.metric_type,
        tags=metric_data.tags
    )
    
    db.add(metric)
    await db.commit()
    await db.refresh(metric)
    
    return SystemMetricResponse(
        id=metric.id,
        metric_name=metric.metric_name,
        metric_value=metric.metric_value,
        metric_type=metric.metric_type,
        tags=metric.tags,
        recorded_at=metric.recorded_at
    )


@router.get("/metrics", response_model=SystemMetricListResponse)
async def list_metrics(
    page: int = Query(1, ge=1),
    page_size: int = Query(100, ge=1, le=1000),
    metric_name: Optional[str] = Query(None),
    metric_type: Optional[str] = Query(None),
    start_date: Optional[datetime] = Query(None),
    end_date: Optional[datetime] = Query(None),
    db: AsyncSession = Depends(get_db),
    current_user: User = Depends(require_permission("metrics.view"))
):
    """
    List system metrics with filtering.
    Requires 'metrics.view' permission.
    """
    # Build query
    query = select(SystemMetric)
    
    filters = []
    if metric_name:
        filters.append(SystemMetric.metric_name == metric_name)
    if metric_type:
        filters.append(SystemMetric.metric_type == metric_type)
    if start_date:
        filters.append(SystemMetric.recorded_at >= start_date)
    if end_date:
        filters.append(SystemMetric.recorded_at <= end_date)
    
    if filters:
        query = query.where(and_(*filters))
    
    # Get total count
    count_query = select(func.count()).select_from(SystemMetric)
    if filters:
        count_query = count_query.where(and_(*filters))
    result = await db.execute(count_query)
    total = result.scalar()
    
    # Apply pagination
    query = query.offset((page - 1) * page_size).limit(page_size)
    query = query.order_by(desc(SystemMetric.recorded_at))
    
    result = await db.execute(query)
    metrics = result.scalars().all()
    
    metric_responses = [
        SystemMetricResponse(
            id=m.id,
            metric_name=m.metric_name,
            metric_value=m.metric_value,
            metric_type=m.metric_type,
            tags=m.tags,
            recorded_at=m.recorded_at
        )
        for m in metrics
    ]
    
    return SystemMetricListResponse(
        metrics=metric_responses,
        total=total,
        page=page,
        page_size=page_size
    )


@router.get("/health", response_model=SystemHealthMetrics)
async def get_system_health(
    db: AsyncSession = Depends(get_db),
    current_user: User = Depends(get_current_user)
):
    """
    Get current system health metrics.
    Requires authentication.
    """
    # Active sessions (last 24h)
    sessions_result = await db.execute(
        select(func.count()).select_from(ChatSession)
        .where(ChatSession.last_activity >= datetime.utcnow() - timedelta(hours=24))
    )
    active_sessions = sessions_result.scalar()
    
    # Active executions (running or pending)
    from app.models.tool import ExecutionStatus
    executions_result = await db.execute(
        select(func.count()).select_from(ToolExecution)
        .where(or_(
            ToolExecution.status == ExecutionStatus.RUNNING,
            ToolExecution.status == ExecutionStatus.PENDING
        ))
    )
    active_executions = executions_result.scalar()
    
    # Get recent metrics
    cpu_metric = await db.execute(
        select(SystemMetric)
        .where(SystemMetric.metric_name == "cpu_usage")
        .order_by(desc(SystemMetric.recorded_at))
        .limit(1)
    )
    cpu = cpu_metric.scalar_one_or_none()
    
    memory_metric = await db.execute(
        select(SystemMetric)
        .where(SystemMetric.metric_name == "memory_usage")
        .order_by(desc(SystemMetric.recorded_at))
        .limit(1)
    )
    memory = memory_metric.scalar_one_or_none()
    
    disk_metric = await db.execute(
        select(SystemMetric)
        .where(SystemMetric.metric_name == "disk_usage")
        .order_by(desc(SystemMetric.recorded_at))
        .limit(1)
    )
    disk = disk_metric.scalar_one_or_none()
    
    # Calculate uptime (time since first audit log)
    first_log_result = await db.execute(
        select(AuditLog.created_at)
        .order_by(AuditLog.created_at)
        .limit(1)
    )
    first_log = first_log_result.scalar_one_or_none()
    uptime_seconds = 0
    if first_log:
        uptime_seconds = int((datetime.utcnow() - first_log).total_seconds())
    
    return SystemHealthMetrics(
        cpu_usage=cpu.metric_value if cpu else None,
        memory_usage=memory.metric_value if memory else None,
        disk_usage=disk.metric_value if disk else None,
        active_sessions=active_sessions,
        active_executions=active_executions,
        cache_hit_rate=None,  # TODO: Calculate from cache stats
        avg_response_time=None,  # TODO: Calculate from request logs
        error_rate=None,  # TODO: Calculate from error logs
        uptime_seconds=uptime_seconds
    )
</file>

<file path="app/api/v1/auth.py">
"""Authentication API endpoints."""
from datetime import datetime, timedelta
from typing import Any

from fastapi import APIRouter, Depends, HTTPException, status
from sqlalchemy.ext.asyncio import AsyncSession
from sqlalchemy import select, or_, update
from sqlalchemy.orm import selectinload

from app.config import settings
from app.core.deps import get_current_user, get_current_superuser, get_db
from app.core.security import (
    create_access_token,
    create_refresh_token,
    get_password_hash,
    validate_refresh_token,
    verify_password,
)
from app.models.user import User, Session as UserSession
from app.schemas.auth import (
    LoginRequest,
    PasswordChange,
    RefreshTokenRequest,
    TokenResponse,
    UserCreate,
    UserResponse,
    UserWithRoles,
)

router = APIRouter()


@router.post("/register", response_model=UserResponse, status_code=status.HTTP_201_CREATED)
async def register(
    user_data: UserCreate,
    db: AsyncSession = Depends(get_db),
) -> Any:
    """Register a new user.
    
    Args:
        user_data: User registration data
        db: Database session
        
    Returns:
        The created user
        
    Raises:
        HTTPException: If username or email already exists
    """
    # Check if username exists
    result = await db.execute(select(User).where(User.username == user_data.username))
    if result.scalar_one_or_none():
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail="Username already registered",
        )
    
    # Check if email exists
    result = await db.execute(select(User).where(User.email == user_data.email))
    if result.scalar_one_or_none():
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail="Email already registered",
        )
    
    # Create new user
    user = User(
        email=user_data.email,
        username=user_data.username,
        full_name=user_data.full_name,
        hashed_password=get_password_hash(user_data.password),
        is_active=True,
        is_superuser=False,
        is_verified=False,
    )
    
    db.add(user)
    await db.commit()
    await db.refresh(user)
    
    return user


@router.post("/login", response_model=TokenResponse)
async def login(
    login_data: LoginRequest,
    db: AsyncSession = Depends(get_db),
) -> Any:
    """Authenticate user and return access and refresh tokens.
    
    Args:
        login_data: Login credentials (username/email and password)
        db: Database session
        
    Returns:
        Access and refresh tokens
        
    Raises:
        HTTPException: If credentials are invalid
    """
    try:
        # Try to find user by username or email
        result = await db.execute(
            select(User).where(
                or_(User.username == login_data.username, User.email == login_data.username)
            )
        )
        user = result.unique().scalar_one_or_none()
        
        if not user:
            raise HTTPException(
                status_code=status.HTTP_401_UNAUTHORIZED,
                detail="Incorrect username or password",
                headers={"WWW-Authenticate": "Bearer"},
            )
        
        # Debug logging
        print(f"DEBUG: Password length: {len(login_data.password)}")
        print(f"DEBUG: Hash length: {len(user.hashed_password)}")
        print(f"DEBUG: Hash prefix: {user.hashed_password[:20]}")
        
        if not verify_password(login_data.password, user.hashed_password):
            raise HTTPException(
                status_code=status.HTTP_401_UNAUTHORIZED,
                detail="Incorrect username or password",
                headers={"WWW-Authenticate": "Bearer"},
            )
        
        if not user.is_active:
            raise HTTPException(
                status_code=status.HTTP_403_FORBIDDEN,
                detail="Inactive user",
            )
        
        # Store user_id before any database operations that might expire the object
        user_id = user.id
        
        # Create tokens
        access_token = create_access_token(subject=user_id)
        refresh_token = create_refresh_token(subject=user_id)
        
        # Update last login using direct SQL to avoid relationship loading issues
        await db.execute(
            update(User)
            .where(User.id == user_id)
            .values(last_login=datetime.utcnow())
        )
        await db.commit()
        
        # TODO: Create session record (temporarily disabled for testing)
        # session = UserSession(
        #     user_id=user_id,
        #     token=access_token,
        #     refresh_token=refresh_token,
        #     is_active=True,
        #     expires_at=datetime.utcnow() + timedelta(minutes=settings.ACCESS_TOKEN_EXPIRE_MINUTES),
        # )
        # db.add(session)
        # await db.commit()
        
        return {
            "access_token": access_token,
            "refresh_token": refresh_token,
            "token_type": "bearer",
            "expires_in": settings.ACCESS_TOKEN_EXPIRE_MINUTES * 60,
        }
    except HTTPException:
        raise
    except Exception as e:
        # Log the actual error for debugging
        import traceback
        print(f"Login error: {str(e)}")
        print(traceback.format_exc())
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Internal server error: {str(e)}",
        )


@router.post("/refresh", response_model=TokenResponse)
async def refresh_token(
    refresh_data: RefreshTokenRequest,
    db: AsyncSession = Depends(get_db),
) -> Any:
    """Refresh access token using refresh token.
    
    Args:
        refresh_data: Refresh token
        db: Database session
        
    Returns:
        New access and refresh tokens
        
    Raises:
        HTTPException: If refresh token is invalid
    """
    # Validate refresh token
    user_id = validate_refresh_token(refresh_data.refresh_token)
    if user_id is None:
        raise HTTPException(
            status_code=status.HTTP_401_UNAUTHORIZED,
            detail="Invalid refresh token",
            headers={"WWW-Authenticate": "Bearer"},
        )
    
    # Get user
    result = await db.execute(select(User).where(User.id == int(user_id)))
    user = result.scalar_one_or_none()
    if not user or not user.is_active:
        raise HTTPException(
            status_code=status.HTTP_401_UNAUTHORIZED,
            detail="User not found or inactive",
        )
    
    # Invalidate old session
    result = await db.execute(
        select(UserSession).where(UserSession.refresh_token == refresh_data.refresh_token)
    )
    old_session = result.scalar_one_or_none()
    if old_session:
        old_session.is_active = False
        await db.commit()
    
    # Create new tokens
    access_token = create_access_token(subject=user.id)
    new_refresh_token = create_refresh_token(subject=user.id)
    
    # Create new session
    session = UserSession(
        user_id=user.id,
        token=access_token,
        refresh_token=new_refresh_token,
        is_active=True,
        expires_at=datetime.utcnow() + timedelta(minutes=settings.ACCESS_TOKEN_EXPIRE_MINUTES),
    )
    db.add(session)
    await db.commit()
    
    return {
        "access_token": access_token,
        "refresh_token": new_refresh_token,
        "token_type": "bearer",
        "expires_in": settings.ACCESS_TOKEN_EXPIRE_MINUTES * 60,
    }


@router.post("/logout")
async def logout(
    current_user: User = Depends(get_current_user),
    db: AsyncSession = Depends(get_db),
) -> Any:
    """Logout current user by invalidating their sessions.
    
    Args:
        current_user: Current authenticated user
        db: Database session
        
    Returns:
        Success message
    """
    # Invalidate all active sessions for this user
    result = await db.execute(
        select(UserSession).where(
            UserSession.user_id == current_user.id,
            UserSession.is_active == True
        )
    )
    sessions = result.scalars().all()
    for session in sessions:
        session.is_active = False
    await db.commit()
    
    return {"message": "Successfully logged out"}


@router.get("/me", response_model=UserWithRoles)
async def get_current_user_info(
    current_user: User = Depends(get_current_user),
) -> Any:
    """Get current user information with roles and permissions.
    
    Args:
        current_user: Current authenticated user
        
    Returns:
        User information with roles and permissions
    """
    return {
        **UserResponse.from_orm(current_user).dict(),
        "roles": [role.name for role in current_user.roles],
        "permissions": current_user.permissions,
    }


@router.put("/me", response_model=UserResponse)
async def update_current_user(
    update_data: dict[str, Any],
    current_user: User = Depends(get_current_user),
    db: AsyncSession = Depends(get_db),
) -> Any:
    """Update current user information.
    
    Args:
        update_data: Fields to update
        current_user: Current authenticated user
        db: Database session
        
    Returns:
        Updated user information
        
    Raises:
        HTTPException: If username/email already exists
    """
    # Check if username is being changed and if it already exists
    if "username" in update_data and update_data["username"] != current_user.username:
        result = await db.execute(select(User).where(User.username == update_data["username"]))
        if result.scalar_one_or_none():
            raise HTTPException(
                status_code=status.HTTP_400_BAD_REQUEST,
                detail="Username already taken",
            )
    
    # Check if email is being changed and if it already exists
    if "email" in update_data and update_data["email"] != current_user.email:
        result = await db.execute(select(User).where(User.email == update_data["email"]))
        if result.scalar_one_or_none():
            raise HTTPException(
                status_code=status.HTTP_400_BAD_REQUEST,
                detail="Email already registered",
            )
    
    # Update allowed fields
    allowed_fields = ["username", "email", "full_name"]
    for field in allowed_fields:
        if field in update_data:
            setattr(current_user, field, update_data[field])
    
    current_user.updated_at = datetime.utcnow()
    await db.commit()
    await db.refresh(current_user)
    
    return current_user


@router.post("/change-password")
async def change_password(
    password_data: PasswordChange,
    current_user: User = Depends(get_current_user),
    db: AsyncSession = Depends(get_db),
) -> Any:
    """Change current user's password.
    
    Args:
        password_data: Current and new password
        current_user: Current authenticated user
        db: Database session
        
    Returns:
        Success message
        
    Raises:
        HTTPException: If current password is incorrect
    """
    # Verify current password
    if not verify_password(password_data.current_password, current_user.hashed_password):
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail="Incorrect current password",
        )
    
    # Update password
    current_user.hashed_password = get_password_hash(password_data.new_password)
    current_user.updated_at = datetime.utcnow()
    
    # Invalidate all sessions (force re-login)
    result = await db.execute(
        select(UserSession).where(
            UserSession.user_id == current_user.id,
            UserSession.is_active == True
        )
    )
    sessions = result.scalars().all()
    for session in sessions:
        session.is_active = False
    
    await db.commit()
    
    return {"message": "Password changed successfully. Please login again."}


@router.get("/users", response_model=list[UserResponse])
async def list_users(
    skip: int = 0,
    limit: int = 100,
    current_user: User = Depends(get_current_superuser),
    db: AsyncSession = Depends(get_db),
) -> Any:
    """List all users (superuser only).
    
    Args:
        skip: Number of records to skip
        limit: Maximum number of records to return
        current_user: Current authenticated superuser
        db: Database session
        
    Returns:
        List of users
    """
    result = await db.execute(select(User).offset(skip).limit(limit))
    users = result.scalars().all()
    return users


@router.get("/users/{user_id}", response_model=UserWithRoles)
async def get_user(
    user_id: int,
    current_user: User = Depends(get_current_superuser),
    db: AsyncSession = Depends(get_db),
) -> Any:
    """Get user by ID (superuser only).
    
    Args:
        user_id: User ID
        current_user: Current authenticated superuser
        db: Database session
        
    Returns:
        User information with roles
        
    Raises:
        HTTPException: If user not found
    """
    result = await db.execute(select(User).where(User.id == user_id))
    user = result.scalar_one_or_none()
    if not user:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail="User not found",
        )
    
    return {
        **UserResponse.from_orm(user).dict(),
        "roles": [role.name for role in user.roles],
        "permissions": user.permissions,
    }
</file>

<file path="app/api/v1/chat.py">
"""Chat API endpoints with SSE streaming."""
import asyncio
import json
import logging
from datetime import datetime
from typing import Any, AsyncGenerator, List

from fastapi import APIRouter, Depends, HTTPException, status
from fastapi.responses import StreamingResponse
from sqlalchemy.orm import Session

logger = logging.getLogger(__name__)

from app.core.deps import get_current_user, get_db
from app.models.chat import ChatMessage, ChatSession, ContextWindow, MessageRole
from app.models.user import User
from app.schemas.chat import (
    ChatHistoryResponse,
    ChatMessageCreate,
    ChatMessageResponse,
    ChatSessionCreate,
    ChatSessionResponse,
    ChatSessionUpdate,
    ChatStreamChunk,
    ChatStreamRequest,
    ContextWindowResponse,
)

router = APIRouter()


@router.post("/sessions", response_model=ChatSessionResponse, status_code=status.HTTP_201_CREATED)
def create_chat_session(
    session_data: ChatSessionCreate,
    current_user: User = Depends(get_current_user),
    db: Session = Depends(get_db),
) -> Any:
    """Create a new chat session.
    
    Args:
        session_data: Chat session creation data
        current_user: Current authenticated user
        db: Database session
        
    Returns:
        Created chat session
    """
    # Create chat session
    chat_session = ChatSession(
        user_id=current_user.id,
        title=session_data.title,
        model=session_data.model,
        temperature=session_data.temperature,
        context_window_size=session_data.context_window_size,
        is_active=True,
    )
    
    db.add(chat_session)
    db.commit()
    db.refresh(chat_session)
    
    # Create context window
    context_window = ContextWindow(
        session_id=chat_session.id,
        total_tokens=0,
        max_tokens=session_data.context_window_size,
        included_message_ids=[],
        strategy="sliding_window",
    )
    
    db.add(context_window)
    db.commit()
    
    return {
        **ChatSessionResponse.from_orm(chat_session).dict(),
        "message_count": 0,
    }


@router.get("/sessions", response_model=List[ChatSessionResponse])
def list_chat_sessions(
    skip: int = 0,
    limit: int = 100,
    active_only: bool = True,
    current_user: User = Depends(get_current_user),
    db: Session = Depends(get_db),
) -> Any:
    """List user's chat sessions.
    
    Args:
        skip: Number of sessions to skip
        limit: Maximum number of sessions to return
        active_only: Only return active sessions
        current_user: Current authenticated user
        db: Database session
        
    Returns:
        List of chat sessions
    """
    query = db.query(ChatSession).filter(ChatSession.user_id == current_user.id)
    
    if active_only:
        query = query.filter(ChatSession.is_active == True)
    
    sessions = query.order_by(ChatSession.last_message_at.desc()).offset(skip).limit(limit).all()
    
    # Add message counts
    result = []
    for session in sessions:
        message_count = db.query(ChatMessage).filter(ChatMessage.session_id == session.id).count()
        result.append({
            **ChatSessionResponse.from_orm(session).dict(),
            "message_count": message_count,
        })
    
    return result


@router.get("/sessions/{session_id}", response_model=ChatHistoryResponse)
def get_chat_session(
    session_id: int,
    include_messages: bool = True,
    message_limit: int = 100,
    current_user: User = Depends(get_current_user),
    db: Session = Depends(get_db),
) -> Any:
    """Get chat session with message history.
    
    Args:
        session_id: Chat session ID
        include_messages: Include message history
        message_limit: Maximum number of messages to return
        current_user: Current authenticated user
        db: Database session
        
    Returns:
        Chat session with history
        
    Raises:
        HTTPException: If session not found or not owned by user
    """
    # Get session
    session = db.query(ChatSession).filter(
        ChatSession.id == session_id,
        ChatSession.user_id == current_user.id,
    ).first()
    
    if not session:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail="Chat session not found",
        )
    
    # Get messages
    messages = []
    if include_messages:
        messages = db.query(ChatMessage).filter(
            ChatMessage.session_id == session_id
        ).order_by(ChatMessage.created_at.desc()).limit(message_limit).all()
        messages = list(reversed(messages))  # Chronological order
    
    # Get context window
    context_window = db.query(ContextWindow).filter(
        ContextWindow.session_id == session_id
    ).first()
    
    return {
        "session": {
            **ChatSessionResponse.from_orm(session).dict(),
            "message_count": len(messages),
        },
        "messages": [ChatMessageResponse.from_orm(msg) for msg in messages],
        "context_window": ContextWindowResponse.from_orm(context_window) if context_window else None,
    }


@router.put("/sessions/{session_id}", response_model=ChatSessionResponse)
def update_chat_session(
    session_id: int,
    session_update: ChatSessionUpdate,
    current_user: User = Depends(get_current_user),
    db: Session = Depends(get_db),
) -> Any:
    """Update chat session.
    
    Args:
        session_id: Chat session ID
        session_update: Update data
        current_user: Current authenticated user
        db: Database session
        
    Returns:
        Updated chat session
        
    Raises:
        HTTPException: If session not found or not owned by user
    """
    # Get session
    session = db.query(ChatSession).filter(
        ChatSession.id == session_id,
        ChatSession.user_id == current_user.id,
    ).first()
    
    if not session:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail="Chat session not found",
        )
    
    # Update fields
    update_data = session_update.dict(exclude_unset=True)
    for field, value in update_data.items():
        setattr(session, field, value)
    
    session.updated_at = datetime.utcnow()
    db.commit()
    db.refresh(session)
    
    message_count = db.query(ChatMessage).filter(ChatMessage.session_id == session.id).count()
    
    return {
        **ChatSessionResponse.from_orm(session).dict(),
        "message_count": message_count,
    }


@router.delete("/sessions/{session_id}", status_code=status.HTTP_204_NO_CONTENT)
def delete_chat_session(
    session_id: int,
    current_user: User = Depends(get_current_user),
    db: Session = Depends(get_db),
) -> None:
    """Delete chat session and all its messages.
    
    Args:
        session_id: Chat session ID
        current_user: Current authenticated user
        db: Database session
        
    Raises:
        HTTPException: If session not found or not owned by user
    """
    # Get session
    session = db.query(ChatSession).filter(
        ChatSession.id == session_id,
        ChatSession.user_id == current_user.id,
    ).first()
    
    if not session:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail="Chat session not found",
        )
    
    # Delete session (cascade will delete messages and context window)
    db.delete(session)
    db.commit()


@router.post("/sessions/{session_id}/messages", response_model=ChatMessageResponse, status_code=status.HTTP_201_CREATED)
def create_chat_message(
    session_id: int,
    message_data: ChatMessageCreate,
    current_user: User = Depends(get_current_user),
    db: Session = Depends(get_db),
) -> Any:
    """Create a chat message (non-streaming).
    
    Args:
        session_id: Chat session ID
        message_data: Message data
        current_user: Current authenticated user
        db: Database session
        
    Returns:
        Created message
        
    Raises:
        HTTPException: If session not found or not owned by user
    """
    # Verify session ownership
    session = db.query(ChatSession).filter(
        ChatSession.id == session_id,
        ChatSession.user_id == current_user.id,
    ).first()
    
    if not session:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail="Chat session not found",
        )
    
    # Create message
    message = ChatMessage(
        session_id=session_id,
        user_id=current_user.id,
        role=message_data.role,
        content=message_data.content,
    )
    
    db.add(message)
    
    # Update session last_message_at
    session.last_message_at = datetime.utcnow()
    
    db.commit()
    db.refresh(message)
    
    return message


async def stream_chat_response(
    session_id: int,
    user_message: str,
    current_user: User,
    db: Session,
    model: str = "gpt-3.5-turbo",
    temperature: float = 0.7,
) -> AsyncGenerator[str, None]:
    """Stream chat response using SSE.
    
    This is a placeholder implementation that demonstrates the streaming pattern.
    In production, this would integrate with actual LLM providers.
    
    Args:
        session_id: Chat session ID
        user_message: User's message
        current_user: Current user
        db: Database session
        model: LLM model to use
        temperature: Temperature parameter
        
    Yields:
        SSE formatted chat chunks
    """
    try:
        # Save user message
        user_msg = ChatMessage(
            session_id=session_id,
            user_id=current_user.id,
            role=MessageRole.USER,
            content=user_message,
        )
        db.add(user_msg)
        db.commit()
        db.refresh(user_msg)
        
        # Send user message confirmation
        yield f"data: {json.dumps(ChatStreamChunk(type='message', message_id=user_msg.id).dict())}\n\n"
        
        # Fetch chat history from database
        history = db.query(ChatMessage).filter(
            ChatMessage.session_id == session_id
        ).order_by(ChatMessage.created_at).all()
        
        # Stream response from LLM
        from app.services.llm_service import llm_service
        from app.services.context_manager import create_context_manager
        
        # Create context manager for model
        context_mgr = create_context_manager(model or "gpt-3.5-turbo")
        
        # Prepare messages with context window management
        llm_messages = context_mgr.prepare_messages_for_llm(history, include_system=True)
        
        # Add current user message
        llm_messages.append({
            "role": "user",
            "content": user_message
        })
        
        # Log context stats
        stats = context_mgr.get_context_stats(llm_messages)
        logger.info(
            f"Context window stats: {stats['total_tokens']}/{stats['max_tokens']} tokens "
            f"({stats['utilization_percent']}% utilization)"
        )
        
        # Stream from LLM
        assistant_response = ""
        async for llm_chunk in llm_service.stream_chat(
            model_id=model or "gpt-3.5-turbo",
            messages=llm_messages,
            temperature=temperature
        ):
            if llm_chunk["type"] == "content":
                content = llm_chunk["content"]
                assistant_response += content
                
                chunk = ChatStreamChunk(
                    type="message",
                    content=content,
                )
                yield f"data: {json.dumps(chunk.dict())}\n\n"
            
            elif llm_chunk["type"] == "error":
                error_chunk = ChatStreamChunk(
                    type="error",
                    error=llm_chunk["error"],
                )
                yield f"data: {json.dumps(error_chunk.dict())}\n\n"
                return
        
        # Save assistant message
        assistant_msg = ChatMessage(
            session_id=session_id,
            user_id=current_user.id,
            role=MessageRole.ASSISTANT,
            content=assistant_response,
            model=model,
        )
        db.add(assistant_msg)
        
        # Update session
        session = db.query(ChatSession).filter(ChatSession.id == session_id).first()
        if session:
            session.last_message_at = datetime.utcnow()
        
        db.commit()
        db.refresh(assistant_msg)
        
        # Send completion
        completion_chunk = ChatStreamChunk(
            type="done",
            message_id=assistant_msg.id,
            content=assistant_response,
        )
        yield f"data: {json.dumps(completion_chunk.dict())}\n\n"
        
    except Exception as e:
        error_chunk = ChatStreamChunk(
            type="error",
            error=str(e),
        )
        yield f"data: {json.dumps(error_chunk.dict())}\n\n"


@router.post("/stream")
async def stream_chat(
    request: ChatStreamRequest,
    current_user: User = Depends(get_current_user),
    db: Session = Depends(get_db),
) -> StreamingResponse:
    """Stream chat response using Server-Sent Events (SSE).
    
    Args:
        request: Chat stream request
        current_user: Current authenticated user
        db: Database session
        
    Returns:
        SSE stream response
        
    Raises:
        HTTPException: If session not found or not owned by user
    """
    # Verify session ownership
    session = db.query(ChatSession).filter(
        ChatSession.id == request.session_id,
        ChatSession.user_id == current_user.id,
    ).first()
    
    if not session:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail="Chat session not found",
        )
    
    # Use session model or request model
    model = request.model or session.model or "gpt-3.5-turbo"
    temperature = request.temperature if request.temperature is not None else float(session.temperature)
    
    return StreamingResponse(
        stream_chat_response(
            session_id=request.session_id,
            user_message=request.message,
            current_user=current_user,
            db=db,
            model=model,
            temperature=temperature,
        ),
        media_type="text/event-stream",
        headers={
            "Cache-Control": "no-cache",
            "Connection": "keep-alive",
            "X-Accel-Buffering": "no",  # Disable nginx buffering
        },
    )


@router.get("/sessions/{session_id}/context", response_model=ContextWindowResponse)
def get_context_window(
    session_id: int,
    current_user: User = Depends(get_current_user),
    db: Session = Depends(get_db),
) -> Any:
    """Get context window information for a session.
    
    Args:
        session_id: Chat session ID
        current_user: Current authenticated user
        db: Database session
        
    Returns:
        Context window information
        
    Raises:
        HTTPException: If session not found or not owned by user
    """
    # Verify session ownership
    session = db.query(ChatSession).filter(
        ChatSession.id == session_id,
        ChatSession.user_id == current_user.id,
    ).first()
    
    if not session:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail="Chat session not found",
        )
    
    # Get context window
    context_window = db.query(ContextWindow).filter(
        ContextWindow.session_id == session_id
    ).first()
    
    if not context_window:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail="Context window not found",
        )
    
    return context_window
</file>

<file path="app/api/v1/documents.py">
"""
Document and RAG API endpoints.
Handles document upload, processing, indexing, search, and RAG queries.
"""
import logging
from typing import List, Optional
from datetime import datetime
import io

from fastapi import APIRouter, Depends, HTTPException, UploadFile, File, Query, BackgroundTasks
from sqlalchemy.orm import Session
from sqlalchemy import func, or_, desc

from app.core.deps import get_db, get_current_user
from app.models.user import User
from app.models.document import Document, DocumentChunk, SearchResult, EmbeddingModel
from app.models.audit import AuditLog
from app.schemas.document import (
    DocumentCreate,
    DocumentUpdate,
    DocumentResponse,
    DocumentListResponse,
    DocumentContentResponse,
    DocumentChunkListResponse,
    DocumentIndexRequest,
    DocumentIndexResponse,
    DocumentSearchRequest,
    DocumentSearchResponse,
    SearchResultResponse,
    RAGQueryRequest,
    RAGQueryResponse,
    RAGContextChunk,
    DocumentStatistics,
    DocumentTablesResponse,
    DocumentImagesResponse,
    EmbeddingModelListResponse
)
from app.services.document_processor import document_processor
from app.services.chunking_service import chunking_service
from app.services.embedding_service import get_embedding_service

logger = logging.getLogger(__name__)

router = APIRouter(prefix="/documents", tags=["documents"])


# Document Management Endpoints

@router.post("/upload", response_model=DocumentResponse)
async def upload_document(
    background_tasks: BackgroundTasks,
    file: UploadFile = File(...),
    title: Optional[str] = None,
    tags: Optional[str] = Query(None, description="Comma-separated tags"),
    is_public: bool = False,
    required_permission: Optional[str] = None,
    auto_index: bool = Query(True, description="Automatically index document"),
    db: Session = Depends(get_db),
    current_user: User = Depends(get_current_user)
):
    """
    Upload a document file.
    
    Supported formats: PDF, DOCX, PPTX, HTML, MD, TXT, and images.
    """
    try:
        # Validate file type
        file_ext = file.filename.split('.')[-1].lower() if '.' in file.filename else ''
        if not document_processor.validate_file_type(file_ext):
            raise HTTPException(
                status_code=400,
                detail=f"Unsupported file type: {file_ext}"
            )
        
        # Read file
        file_bytes = await file.read()
        file_size = len(file_bytes)
        
        # Create document record
        document = Document(
            title=title or file.filename,
            source=file.filename,
            source_type="file",
            file_type=file_ext,
            file_size=file_size,
            uploaded_by=current_user.id,
            is_public=is_public,
            required_permission=required_permission,
            tags=tags.split(',') if tags else None
        )
        
        db.add(document)
        db.flush()
        
        # Process document in background
        if auto_index:
            background_tasks.add_task(
                process_and_index_document,
                document.id,
                file_bytes,
                file.filename,
                file_ext,
                db
            )
        
        db.commit()
        db.refresh(document)
        
        # Audit log
        audit = AuditLog(
            user_id=current_user.id,
            action="document.upload",
            resource_type="document",
            resource_id=document.id,
            details={"filename": file.filename, "size": file_size}
        )
        db.add(audit)
        db.commit()
        
        logger.info(f"Document uploaded: {document.id} by user {current_user.id}")
        
        return document
        
    except Exception as e:
        logger.error(f"Error uploading document: {str(e)}", exc_info=True)
        raise HTTPException(status_code=500, detail=str(e))


@router.get("/{document_id}", response_model=DocumentResponse)
def get_document(
    document_id: int,
    db: Session = Depends(get_db),
    current_user: User = Depends(get_current_user)
):
    """Get document by ID."""
    document = db.query(Document).filter(Document.id == document_id).first()
    
    if not document:
        raise HTTPException(status_code=404, detail="Document not found")
    
    # Check permissions
    if not document.is_public and document.uploaded_by != current_user.id:
        if document.required_permission:
            require_permissions([document.required_permission])(current_user)
    
    # Update last accessed
    document.last_accessed = datetime.utcnow()
    db.commit()
    
    return document


@router.get("/", response_model=DocumentListResponse)
def list_documents(
    page: int = Query(1, ge=1),
    page_size: int = Query(20, ge=1, le=100),
    search: Optional[str] = None,
    tags: Optional[str] = None,
    source_type: Optional[str] = None,
    is_indexed: Optional[bool] = None,
    db: Session = Depends(get_db),
    current_user: User = Depends(get_current_user)
):
    """List documents with pagination and filters."""
    query = db.query(Document)
    
    # Filter by access (public or owned)
    query = query.filter(
        or_(
            Document.is_public == True,
            Document.uploaded_by == current_user.id
        )
    )
    
    # Apply filters
    if search:
        query = query.filter(
            or_(
                Document.title.ilike(f"%{search}%"),
                Document.source.ilike(f"%{search}%")
            )
        )
    
    if tags:
        tag_list = tags.split(',')
        query = query.filter(Document.tags.contains(tag_list))
    
    if source_type:
        query = query.filter(Document.source_type == source_type)
    
    if is_indexed is not None:
        query = query.filter(Document.is_indexed == is_indexed)
    
    # Get total count
    total = query.count()
    
    # Paginate
    documents = query.order_by(desc(Document.created_at))\
        .offset((page - 1) * page_size)\
        .limit(page_size)\
        .all()
    
    return {
        "documents": documents,
        "total": total,
        "page": page,
        "page_size": page_size
    }


@router.put("/{document_id}", response_model=DocumentResponse)
def update_document(
    document_id: int,
    update_data: DocumentUpdate,
    db: Session = Depends(get_db),
    current_user: User = Depends(get_current_user)
):
    """Update document metadata."""
    document = db.query(Document).filter(Document.id == document_id).first()
    
    if not document:
        raise HTTPException(status_code=404, detail="Document not found")
    
    # Check ownership
    if document.uploaded_by != current_user.id:
        raise HTTPException(status_code=403, detail="Not authorized")
    
    # Update fields
    if update_data.title is not None:
        document.title = update_data.title
    if update_data.tags is not None:
        document.tags = update_data.tags
    if update_data.meta_data is not None:
        document.meta_data = update_data.meta_data
    if update_data.is_public is not None:
        document.is_public = update_data.is_public
    if update_data.required_permission is not None:
        document.required_permission = update_data.required_permission
    
    document.updated_at = datetime.utcnow()
    
    db.commit()
    db.refresh(document)
    
    # Audit log
    audit = AuditLog(
        user_id=current_user.id,
        action="document.update",
        resource_type="document",
        resource_id=document.id,
        details=update_data.dict(exclude_none=True)
    )
    db.add(audit)
    db.commit()
    
    return document


@router.delete("/{document_id}")
def delete_document(
    document_id: int,
    db: Session = Depends(get_db),
    current_user: User = Depends(get_current_user)
):
    """Delete document and all associated chunks."""
    document = db.query(Document).filter(Document.id == document_id).first()
    
    if not document:
        raise HTTPException(status_code=404, detail="Document not found")
    
    # Check ownership
    if document.uploaded_by != current_user.id:
        raise HTTPException(status_code=403, detail="Not authorized")
    
    # Delete chunks
    db.query(DocumentChunk).filter(DocumentChunk.document_id == document_id).delete()
    
    # Delete document
    db.delete(document)
    db.commit()
    
    # Audit log
    audit = AuditLog(
        user_id=current_user.id,
        action="document.delete",
        resource_type="document",
        resource_id=document_id,
        details={"title": document.title}
    )
    db.add(audit)
    db.commit()
    
    return {"message": "Document deleted successfully"}


# Document Processing and Indexing Endpoints

@router.post("/{document_id}/index", response_model=DocumentIndexResponse)
async def index_document(
    document_id: int,
    request: DocumentIndexRequest = DocumentIndexRequest(),
    db: Session = Depends(get_db),
    current_user: User = Depends(get_current_user)
):
    """
    Process and index a document for search.
    
    Creates embeddings for all document chunks.
    """
    document = db.query(Document).filter(Document.id == document_id).first()
    
    if not document:
        raise HTTPException(status_code=404, detail="Document not found")
    
    # Check permissions
    if document.uploaded_by != current_user.id and not document.is_public:
        raise HTTPException(status_code=403, detail="Not authorized")
    
    try:
        start_time = datetime.utcnow()
        
        # Note: This is a simplified version - in production, file content would be stored/retrieved
        # For now, return a mock response
        document.is_indexed = True
        document.updated_at = datetime.utcnow()
        db.commit()
        
        processing_time = (datetime.utcnow() - start_time).total_seconds()
        
        # Audit log
        audit = AuditLog(
            user_id=current_user.id,
            action="document.index",
            resource_type="document",
            resource_id=document.id,
            details=request.dict()
        )
        db.add(audit)
        db.commit()
        
        return {
            "document_id": document_id,
            "chunks_created": document.chunk_count,
            "tables_extracted": 0,
            "images_extracted": 0,
            "total_tokens": 0,
            "processing_time_seconds": processing_time,
            "success": True,
            "error": None
        }
        
    except Exception as e:
        logger.error(f"Error indexing document {document_id}: {str(e)}", exc_info=True)
        document.processing_error = str(e)
        db.commit()
        
        return {
            "document_id": document_id,
            "chunks_created": 0,
            "tables_extracted": 0,
            "images_extracted": 0,
            "total_tokens": 0,
            "processing_time_seconds": 0,
            "success": False,
            "error": str(e)
        }


@router.get("/{document_id}/chunks", response_model=DocumentChunkListResponse)
def get_document_chunks(
    document_id: int,
    db: Session = Depends(get_db),
    current_user: User = Depends(get_current_user)
):
    """Get all chunks for a document."""
    document = db.query(Document).filter(Document.id == document_id).first()
    
    if not document:
        raise HTTPException(status_code=404, detail="Document not found")
    
    # Check permissions
    if not document.is_public and document.uploaded_by != current_user.id:
        if document.required_permission:
            require_permissions([document.required_permission])(current_user)
    
    chunks = db.query(DocumentChunk)\
        .filter(DocumentChunk.document_id == document_id)\
        .order_by(DocumentChunk.chunk_index)\
        .all()
    
    return {
        "chunks": chunks,
        "total": len(chunks),
        "document_id": document_id
    }


# Search and RAG Endpoints

@router.post("/search", response_model=DocumentSearchResponse)
async def search_documents(
    request: DocumentSearchRequest,
    db: Session = Depends(get_db),
    current_user: User = Depends(get_current_user)
):
    """
    Semantic search across indexed documents.
    
    Supports vector search, keyword search, and hybrid search.
    """
    try:
        start_time = datetime.utcnow()
        
        # Generate query embedding for vector search
        if request.search_type in ["vector", "hybrid"]:
            embedding_service = get_embedding_service()
            query_embedding = await embedding_service.generate_embedding(request.query)
        
        # Vector search
        if request.search_type == "vector":
            results = await vector_search(
                query_embedding,
                request.top_k,
                request.filters,
                request.min_score,
                current_user.id,
                db
            )
        
        # Keyword search
        elif request.search_type == "keyword":
            results = await keyword_search(
                request.query,
                request.top_k,
                request.filters,
                current_user.id,
                db
            )
        
        # Hybrid search
        else:
            vector_results = await vector_search(
                query_embedding,
                request.top_k,
                request.filters,
                request.min_score,
                current_user.id,
                db
            )
            keyword_results = await keyword_search(
                request.query,
                request.top_k,
                request.filters,
                current_user.id,
                db
            )
            results = merge_search_results(vector_results, keyword_results, request.top_k)
        
        processing_time = (datetime.utcnow() - start_time).total_seconds() * 1000
        
        # Audit log
        audit = AuditLog(
            user_id=current_user.id,
            action="document.search",
            resource_type="document",
            details={
                "query": request.query,
                "search_type": request.search_type,
                "results_count": len(results)
            }
        )
        db.add(audit)
        db.commit()
        
        return {
            "query": request.query,
            "results": results,
            "total_results": len(results),
            "search_type": request.search_type,
            "processing_time_ms": processing_time
        }
        
    except Exception as e:
        logger.error(f"Error searching documents: {str(e)}", exc_info=True)
        raise HTTPException(status_code=500, detail=str(e))


@router.post("/rag/query", response_model=RAGQueryResponse)
async def rag_query(
    request: RAGQueryRequest,
    db: Session = Depends(get_db),
    current_user: User = Depends(get_current_user)
):
    """
    Retrieve relevant context for RAG (Retrieval-Augmented Generation).
    
    Returns top-k most relevant document chunks for LLM context.
    """
    try:
        start_time = datetime.utcnow()
        
        # Generate query embedding
        embedding_service = get_embedding_service()
        query_embedding = await embedding_service.generate_embedding(request.query)
        
        # Search for relevant chunks
        search_results = await vector_search(
            query_embedding,
            request.top_k,
            request.filters,
            None,
            current_user.id,
            db
        )
        
        # Format as context chunks
        context_chunks = []
        total_tokens = 0
        sources = set()
        
        for result in search_results:
            chunk_tokens = result.get("meta_data", {}).get("token_count", 0)
            total_tokens += chunk_tokens
            sources.add(result["document_title"])
            
            context_chunk = {
                "content": result["content"],
                "document_title": result["document_title"],
                "relevance_score": result["relevance_score"],
                "meta_data": result.get("meta_data") if request.include_metadata else None
            }
            context_chunks.append(context_chunk)
        
        processing_time = (datetime.utcnow() - start_time).total_seconds() * 1000
        
        # Audit log
        audit = AuditLog(
            user_id=current_user.id,
            action="document.rag_query",
            resource_type="document",
            details={
                "query": request.query,
                "chunks_retrieved": len(context_chunks),
                "session_id": request.session_id
            }
        )
        db.add(audit)
        db.commit()
        
        return {
            "query": request.query,
            "context_chunks": context_chunks,
            "total_tokens": total_tokens,
            "sources": list(sources),
            "processing_time_ms": processing_time
        }
        
    except Exception as e:
        logger.error(f"Error in RAG query: {str(e)}", exc_info=True)
        raise HTTPException(status_code=500, detail=str(e))


# Statistics and Analytics

@router.get("/stats/overview", response_model=DocumentStatistics)
def get_document_statistics(
    db: Session = Depends(get_db),
    current_user: User = Depends(get_current_user)
):
    """Get document statistics and analytics."""
    # Get counts
    total_docs = db.query(Document).filter(
        or_(
            Document.is_public == True,
            Document.uploaded_by == current_user.id
        )
    ).count()
    
    processed_docs = db.query(Document).filter(
        Document.is_processed == True,
        or_(
            Document.is_public == True,
            Document.uploaded_by == current_user.id
        )
    ).count()
    
    indexed_docs = db.query(Document).filter(
        Document.is_indexed == True,
        or_(
            Document.is_public == True,
            Document.uploaded_by == current_user.id
        )
    ).count()
    
    total_chunks = db.query(func.sum(Document.chunk_count)).filter(
        or_(
            Document.is_public == True,
            Document.uploaded_by == current_user.id
        )
    ).scalar() or 0
    
    # Get documents by type
    docs_by_type = db.query(
        Document.file_type,
        func.count(Document.id)
    ).filter(
        or_(
            Document.is_public == True,
            Document.uploaded_by == current_user.id
        )
    ).group_by(Document.file_type).all()
    
    # Get storage size
    total_size = db.query(func.sum(Document.file_size)).filter(
        or_(
            Document.is_public == True,
            Document.uploaded_by == current_user.id
        )
    ).scalar() or 0
    
    return {
        "total_documents": total_docs,
        "processed_documents": processed_docs,
        "indexed_documents": indexed_docs,
        "total_chunks": int(total_chunks),
        "total_searches": 0,  # Would track in separate table
        "documents_by_type": {ft: count for ft, count in docs_by_type if ft},
        "avg_chunks_per_document": total_chunks / total_docs if total_docs > 0 else 0,
        "storage_size_mb": total_size / (1024 * 1024),
        "recent_uploads": db.query(Document).filter(
            or_(
                Document.is_public == True,
                Document.uploaded_by == current_user.id
            )
        ).filter(
            Document.created_at >= datetime.utcnow().replace(hour=0, minute=0, second=0)
        ).count()
    }


# Helper Functions

async def process_and_index_document(
    document_id: int,
    file_bytes: bytes,
    filename: str,
    file_type: str,
    db: Session
):
    """Background task to process and index document."""
    try:
        document = db.query(Document).filter(Document.id == document_id).first()
        if not document:
            return
        
        # Process document
        result = document_processor.process_from_bytes(file_bytes, filename, file_type)
        
        if not result["success"]:
            document.processing_error = result["error"]
            document.is_processed = False
            db.commit()
            return
        
        # Update document
        document.is_processed = True
        document.meta_data = result["metadata"]
        db.commit()
        
        # Create chunks
        chunks = chunking_service.chunk_document(
            result["text"],
            result["structure"],
            result["tables"],
            result["images"],
            result["metadata"]
        )
        
        # Generate embeddings
        chunk_texts = [c["content"] for c in chunks]
        embedding_service = get_embedding_service()
        embeddings = await embedding_service.generate_embeddings(chunk_texts)
        
        # Save chunks with embeddings
        for idx, (chunk, embedding) in enumerate(zip(chunks, embeddings)):
            db_chunk = DocumentChunk(
                document_id=document_id,
                chunk_index=idx,
                content=chunk["content"],
                embedding=embedding,
                token_count=chunk["meta_data"].get("token_count"),
                char_count=chunk["meta_data"].get("char_count"),
                meta_data=chunk["meta_data"],
                search_keywords=chunk["search_keywords"]
            )
            db.add(db_chunk)
        
        # Update document
        document.chunk_count = len(chunks)
        document.is_indexed = True
        db.commit()
        
        logger.info(f"Successfully indexed document {document_id} with {len(chunks)} chunks")
        
    except Exception as e:
        logger.error(f"Error processing document {document_id}: {str(e)}", exc_info=True)
        document = db.query(Document).filter(Document.id == document_id).first()
        if document:
            document.processing_error = str(e)
            db.commit()


async def vector_search(
    query_embedding: List[float],
    top_k: int,
    filters: Optional[dict],
    min_score: Optional[float],
    user_id: int,
    db: Session
) -> List[dict]:
    """Perform vector similarity search."""
    # This would use pgvector's <-> operator in production
    # For now, return empty list as placeholder
    return []


async def keyword_search(
    query: str,
    top_k: int,
    filters: Optional[dict],
    user_id: int,
    db: Session
) -> List[dict]:
    """Perform keyword-based search."""
    chunks = db.query(DocumentChunk, Document)\
        .join(Document, DocumentChunk.document_id == Document.id)\
        .filter(
            or_(
                Document.is_public == True,
                Document.uploaded_by == user_id
            )
        )\
        .filter(DocumentChunk.content.ilike(f"%{query}%"))\
        .limit(top_k)\
        .all()
    
    results = []
    for chunk, doc in chunks:
        results.append({
            "chunk_id": chunk.id,
            "document_id": doc.id,
            "document_title": doc.title,
            "content": chunk.content,
            "relevance_score": 0.5,  # Placeholder
            "rank": len(results) + 1,
            "meta_data": chunk.meta_data,
            "search_type": "keyword"
        })
    
    return results


def merge_search_results(vector_results: List[dict], keyword_results: List[dict], top_k: int) -> List[dict]:
    """Merge and rank results from vector and keyword search."""
    # Simple merging - in production would use reciprocal rank fusion
    merged = {}
    
    for result in vector_results:
        chunk_id = result["chunk_id"]
        merged[chunk_id] = result
        merged[chunk_id]["relevance_score"] = result["relevance_score"] * 0.7
    
    for result in keyword_results:
        chunk_id = result["chunk_id"]
        if chunk_id in merged:
            merged[chunk_id]["relevance_score"] += result["relevance_score"] * 0.3
        else:
            merged[chunk_id] = result
            merged[chunk_id]["relevance_score"] = result["relevance_score"] * 0.3
    
    # Sort by score and return top k
    sorted_results = sorted(
        merged.values(),
        key=lambda x: x["relevance_score"],
        reverse=True
    )
    
    return sorted_results[:top_k]
</file>

<file path="app/api/v1/llm.py">
"""
LLM Gateway API endpoints.
Manages LLM models, configurations, and connections.
"""
import logging
from typing import List, Optional
from datetime import datetime

from fastapi import APIRouter, Depends, HTTPException
from sqlalchemy.orm import Session
from pydantic import BaseModel, Field

from app.core.deps import get_current_user, get_db
from app.models.user import User
from app.models.audit import AuditLog
from app.services.llm_service import llm_service

logger = logging.getLogger(__name__)

router = APIRouter(prefix="/llm", tags=["llm"])


# Schemas
class LLMModelResponse(BaseModel):
    """Schema for LLM model information."""
    id: str
    name: str
    provider: str
    context_window: Optional[int] = None
    supports_tools: Optional[bool] = None
    supports_vision: Optional[bool] = None
    available: bool


class LLMModelListResponse(BaseModel):
    """Schema for list of LLM models."""
    models: List[LLMModelResponse]
    total: int


class LLMTestRequest(BaseModel):
    """Schema for testing LLM model."""
    prompt: str = Field(default="Hello, how are you?", max_length=500)


class LLMTestResponse(BaseModel):
    """Schema for LLM test result."""
    model_id: str
    success: bool
    response: Optional[str] = None
    error: Optional[str] = None
    latency_ms: Optional[float] = None


class LLMStatusResponse(BaseModel):
    """Schema for LLM gateway status."""
    total_models: int
    available_models: int
    providers: List[str]
    default_model: Optional[str] = None


# Endpoints

@router.get("/models", response_model=LLMModelListResponse)
async def list_models(
    current_user: User = Depends(get_current_user)
):
    """
    List all available LLM models.
    
    Returns models from all configured providers:
    - OpenAI (GPT-4, GPT-3.5-turbo)
    - Anthropic (Claude 3 family)
    - Local Ollama models
    """
    try:
        models = llm_service.list_models()
        
        # Get detailed info for each model
        detailed_models = []
        for model in models:
            info = await llm_service.get_model_info(model["id"])
            if info:
                detailed_models.append(LLMModelResponse(**info))
        
        return {
            "models": detailed_models,
            "total": len(detailed_models)
        }
        
    except Exception as e:
        logger.error(f"Error listing models: {str(e)}", exc_info=True)
        raise HTTPException(status_code=500, detail=str(e))


@router.get("/models/{model_id}", response_model=LLMModelResponse)
async def get_model(
    model_id: str,
    current_user: User = Depends(get_current_user)
):
    """Get detailed information about a specific model."""
    try:
        info = await llm_service.get_model_info(model_id)
        
        if not info:
            raise HTTPException(
                status_code=404,
                detail=f"Model '{model_id}' not found"
            )
        
        return LLMModelResponse(**info)
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error getting model {model_id}: {str(e)}", exc_info=True)
        raise HTTPException(status_code=500, detail=str(e))


@router.post("/models/{model_id}/test", response_model=LLMTestResponse)
async def test_model(
    model_id: str,
    request: LLMTestRequest = LLMTestRequest(),
    current_user: User = Depends(get_current_user),
    db: Session = Depends(get_db)
):
    """
    Test a model by sending a simple prompt.
    
    Verifies:
    - Model availability
    - API connectivity
    - Response latency
    """
    try:
        start_time = datetime.utcnow()
        
        # Check if model exists
        if not llm_service.get_provider(model_id):
            raise HTTPException(
                status_code=404,
                detail=f"Model '{model_id}' not found"
            )
        
        # Test the model
        messages = [{"role": "user", "content": request.prompt}]
        response_text = ""
        error = None
        
        try:
            async for chunk in llm_service.stream_chat(
                model_id=model_id,
                messages=messages,
                temperature=0.7,
                max_tokens=100
            ):
                if chunk["type"] == "content":
                    response_text += chunk["content"]
                elif chunk["type"] == "error":
                    error = chunk["error"]
                    break
            
            latency = (datetime.utcnow() - start_time).total_seconds() * 1000
            
            # Audit log
            audit = AuditLog(
                user_id=current_user.id,
                action="llm.test",
                resource_type="llm",
                resource_id=model_id,
                details={
                    "model_id": model_id,
                    "success": error is None,
                    "latency_ms": latency
                }
            )
            db.add(audit)
            db.commit()
            
            return LLMTestResponse(
                model_id=model_id,
                success=error is None,
                response=response_text if response_text else None,
                error=error,
                latency_ms=latency
            )
            
        except Exception as e:
            latency = (datetime.utcnow() - start_time).total_seconds() * 1000
            logger.error(f"Error testing model {model_id}: {str(e)}", exc_info=True)
            
            return LLMTestResponse(
                model_id=model_id,
                success=False,
                error=str(e),
                latency_ms=latency
            )
            
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error in test endpoint: {str(e)}", exc_info=True)
        raise HTTPException(status_code=500, detail=str(e))


@router.get("/status", response_model=LLMStatusResponse)
async def get_status(
    current_user: User = Depends(get_current_user)
):
    """
    Get LLM gateway status.
    
    Returns:
    - Total number of registered models
    - Available models
    - Active providers
    - Default model
    """
    try:
        models = llm_service.list_models()
        
        # Get unique providers
        providers = list(set(model["provider"] for model in models))
        
        # Count available models
        available = sum(1 for model in models if model["available"])
        
        return LLMStatusResponse(
            total_models=len(models),
            available_models=available,
            providers=providers,
            default_model="gpt-3.5-turbo"  # Default fallback
        )
        
    except Exception as e:
        logger.error(f"Error getting LLM status: {str(e)}", exc_info=True)
        raise HTTPException(status_code=500, detail=str(e))


@router.get("/providers")
async def list_providers(
    current_user: User = Depends(get_current_user)
):
    """
    List all configured LLM providers.
    
    Returns information about each provider including:
    - Name and status
    - Available models
    - API connectivity
    """
    try:
        models = llm_service.list_models()
        
        # Group models by provider
        providers_dict = {}
        for model in models:
            provider = model["provider"]
            if provider not in providers_dict:
                providers_dict[provider] = {
                    "name": provider,
                    "models": [],
                    "available": True
                }
            providers_dict[provider]["models"].append(model["id"])
        
        providers = list(providers_dict.values())
        
        return {
            "providers": providers,
            "total": len(providers)
        }
        
    except Exception as e:
        logger.error(f"Error listing providers: {str(e)}", exc_info=True)
        raise HTTPException(status_code=500, detail=str(e))
</file>

<file path="app/api/v1/notifications.py">
"""
Notification API endpoints.

Provides:
- Real-time SSE notification streaming
- Notification CRUD operations
- User preference management
- Broadcast notifications (admin only)
- Notification statistics
"""
import json
import asyncio
import logging
from datetime import datetime
from typing import List, Optional

from fastapi import APIRouter, Depends, HTTPException, status, Query
from fastapi.responses import StreamingResponse
from sqlalchemy.ext.asyncio import AsyncSession

from app.core.deps import (
    get_current_user,
    get_db,
    require_permission
)
from app.models.user import User
from app.models.notification import NotificationType, NotificationPriority
from app.schemas.notification import (
    NotificationResponse,
    NotificationListResponse,
    NotificationUpdate,
    NotificationPreferenceResponse,
    NotificationPreferenceListResponse,
    NotificationPreferenceUpdate,
    BroadcastNotificationRequest,
    NotificationStatsResponse
)
from app.services.notification_service import notification_service
from app.core.logging import log_api_call

logger = logging.getLogger(__name__)

router = APIRouter(prefix="/notifications", tags=["notifications"])


@router.get(
    "/stream",
    summary="Real-time notification stream (SSE)",
    description="Server-Sent Events endpoint for receiving notifications in real-time"
)
async def stream_notifications(
    current_user: User = Depends(get_current_user)
):
    """
    Stream real-time notifications via Server-Sent Events (SSE).
    
    Keep this connection open to receive notifications as they arrive.
    The stream will send periodic keepalive messages.
    
    **Event Format:**
    ```
    event: notification
    data: {"id": 1, "type": "APPROVAL_REQUESTED", "title": "...", ...}
    
    event: keepalive
    data: {"timestamp": "2024-01-01T12:00:00Z"}
    ```
    """
    logger.info(f"User {current_user.id} connecting to notification stream")
    
    async def event_generator():
        # Register connection
        queue = await notification_service.connect(current_user.id)
        
        try:
            # Send initial connection confirmation
            yield f"event: connected\ndata: {json.dumps({'user_id': current_user.id, 'timestamp': datetime.utcnow().isoformat()})}\n\n"
            
            # Main event loop
            while True:
                try:
                    # Wait for notification with timeout for keepalive
                    notification = await asyncio.wait_for(
                        queue.get(),
                        timeout=30.0  # 30 second keepalive
                    )
                    
                    # Send notification
                    yield f"event: notification\ndata: {json.dumps(notification)}\n\n"
                    
                except asyncio.TimeoutError:
                    # Send keepalive
                    yield f"event: keepalive\ndata: {json.dumps({'timestamp': datetime.utcnow().isoformat()})}\n\n"
                    
        except asyncio.CancelledError:
            logger.info(f"User {current_user.id} notification stream cancelled")
        except Exception as e:
            logger.error(f"Error in notification stream for user {current_user.id}: {e}")
            yield f"event: error\ndata: {json.dumps({'error': str(e)})}\n\n"
        finally:
            # Unregister connection
            await notification_service.disconnect(current_user.id, queue)
            logger.info(f"User {current_user.id} disconnected from notification stream")
    
    return StreamingResponse(
        event_generator(),
        media_type="text/event-stream",
        headers={
            "Cache-Control": "no-cache",
            "Connection": "keep-alive",
            "X-Accel-Buffering": "no"  # Disable nginx buffering
        }
    )


@router.get(
    "",
    response_model=NotificationListResponse,
    summary="Get notifications",
    description="Retrieve paginated list of notifications for the current user"
)
@log_api_call
async def get_notifications(
    unread_only: bool = Query(False, description="Only return unread notifications"),
    page: int = Query(1, ge=1, description="Page number"),
    page_size: int = Query(50, ge=1, le=100, description="Items per page"),
    db: AsyncSession = Depends(get_db),
    current_user: User = Depends(get_current_user)
) -> NotificationListResponse:
    """
    Get paginated notifications for the current user.
    
    **Query Parameters:**
    - `unread_only`: Filter to only unread notifications
    - `page`: Page number (starts at 1)
    - `page_size`: Number of items per page (max 100)
    
    **Returns:**
    - List of notifications with pagination metadata
    """
    offset = (page - 1) * page_size
    
    notifications, total = await notification_service.get_user_notifications(
        db=db,
        user_id=current_user.id,
        unread_only=unread_only,
        limit=page_size,
        offset=offset
    )
    
    # Count unread
    _, unread_count = await notification_service.get_user_notifications(
        db=db,
        user_id=current_user.id,
        unread_only=True,
        limit=1,
        offset=0
    )
    
    return NotificationListResponse(
        notifications=notifications,
        total=total,
        unread_count=unread_count,
        page=page,
        page_size=page_size
    )


@router.get(
    "/stats",
    response_model=NotificationStatsResponse,
    summary="Get notification statistics",
    description="Get detailed statistics about user's notifications"
)
@log_api_call
async def get_notification_stats(
    db: AsyncSession = Depends(get_db),
    current_user: User = Depends(get_current_user)
) -> NotificationStatsResponse:
    """
    Get notification statistics for the current user.
    
    **Returns:**
    - Total notification count
    - Unread count
    - Breakdown by type
    - Breakdown by priority
    - Recent notifications count (last 24 hours)
    """
    stats = await notification_service.get_notification_stats(db, current_user.id)
    return NotificationStatsResponse(**stats)


@router.patch(
    "/{notification_id}",
    response_model=NotificationResponse,
    summary="Update notification",
    description="Update a notification (e.g., mark as read)"
)
@log_api_call
async def update_notification(
    notification_id: int,
    update: NotificationUpdate,
    db: AsyncSession = Depends(get_db),
    current_user: User = Depends(get_current_user)
) -> NotificationResponse:
    """
    Update a notification.
    
    Currently supports marking as read/unread.
    
    **Path Parameters:**
    - `notification_id`: ID of the notification to update
    
    **Request Body:**
    ```json
    {
        "is_read": true
    }
    ```
    """
    if update.is_read is not None and update.is_read:
        notification = await notification_service.mark_as_read(
            db=db,
            notification_id=notification_id,
            user_id=current_user.id
        )
        
        if not notification:
            raise HTTPException(
                status_code=status.HTTP_404_NOT_FOUND,
                detail="Notification not found"
            )
        
        return notification
    
    raise HTTPException(
        status_code=status.HTTP_400_BAD_REQUEST,
        detail="No valid update fields provided"
    )


@router.post(
    "/mark-all-read",
    summary="Mark all as read",
    description="Mark all user's notifications as read"
)
@log_api_call
async def mark_all_notifications_read(
    db: AsyncSession = Depends(get_db),
    current_user: User = Depends(get_current_user)
) -> dict:
    """
    Mark all notifications as read for the current user.
    
    **Returns:**
    - Count of notifications marked as read
    """
    count = await notification_service.mark_all_as_read(db, current_user.id)
    
    return {
        "success": True,
        "marked_read": count,
        "message": f"Marked {count} notifications as read"
    }


@router.delete(
    "/{notification_id}",
    summary="Delete notification",
    description="Delete a specific notification"
)
@log_api_call
async def delete_notification(
    notification_id: int,
    db: AsyncSession = Depends(get_db),
    current_user: User = Depends(get_current_user)
) -> dict:
    """
    Delete a notification.
    
    **Path Parameters:**
    - `notification_id`: ID of the notification to delete
    """
    deleted = await notification_service.delete_notification(
        db=db,
        notification_id=notification_id,
        user_id=current_user.id
    )
    
    if not deleted:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail="Notification not found"
        )
    
    return {
        "success": True,
        "message": "Notification deleted successfully"
    }


@router.get(
    "/preferences",
    response_model=NotificationPreferenceListResponse,
    summary="Get notification preferences",
    description="Get user's notification preferences"
)
@log_api_call
async def get_notification_preferences(
    db: AsyncSession = Depends(get_db),
    current_user: User = Depends(get_current_user)
) -> NotificationPreferenceListResponse:
    """
    Get all notification preferences for the current user.
    
    **Returns:**
    - List of notification preferences (which types are enabled/disabled)
    """
    preferences = await notification_service.get_user_preferences(db, current_user.id)
    
    return NotificationPreferenceListResponse(
        preferences=preferences,
        total=len(preferences)
    )


@router.put(
    "/preferences/{notification_type}",
    response_model=NotificationPreferenceResponse,
    summary="Update notification preference",
    description="Update preference for a specific notification type"
)
@log_api_call
async def update_notification_preference(
    notification_type: NotificationType,
    update: NotificationPreferenceUpdate,
    db: AsyncSession = Depends(get_db),
    current_user: User = Depends(get_current_user)
) -> NotificationPreferenceResponse:
    """
    Update or create a notification preference.
    
    **Path Parameters:**
    - `notification_type`: Type of notification (e.g., APPROVAL_REQUESTED)
    
    **Request Body:**
    ```json
    {
        "enabled": true,
        "delivery_method": "realtime"
    }
    ```
    """
    preference = await notification_service.update_preference(
        db=db,
        user_id=current_user.id,
        notification_type=notification_type,
        enabled=update.enabled if update.enabled is not None else True,
        delivery_method=update.delivery_method or "realtime"
    )
    
    return preference


@router.post(
    "/broadcast",
    summary="Send broadcast notification (Admin)",
    description="Send a notification to multiple users (requires admin permission)"
)
@log_api_call
async def broadcast_notification(
    request: BroadcastNotificationRequest,
    db: AsyncSession = Depends(get_db),
    current_user: User = Depends(require_permission("notifications:broadcast"))
) -> dict:
    """
    Send a broadcast notification to multiple users.
    
    **Required Permission:** `notifications:broadcast`
    
    **Request Body:**
    ```json
    {
        "type": "SYSTEM_MAINTENANCE",
        "title": "System Maintenance",
        "message": "The system will undergo maintenance tonight",
        "target_roles": ["ANALYST", "MANAGER"],
        "priority": "high",
        "persist": true
    }
    ```
    
    **Parameters:**
    - `type`: Type of notification
    - `title`: Short title
    - `message`: Detailed message
    - `target_roles`: List of role names (null = all users)
    - `data`: Additional structured data
    - `priority`: Priority level (low, normal, high, urgent)
    - `persist`: Whether to save to database
    
    **Returns:**
    - Count of users notified
    """
    count = await notification_service.send_broadcast_notification(
        db=db,
        notification_type=request.type,
        title=request.title,
        message=request.message,
        target_roles=request.target_roles,
        data=request.data,
        priority=request.priority,
        persist=request.persist
    )
    
    return {
        "success": True,
        "users_notified": count,
        "message": f"Notification sent to {count} users"
    }


@router.get(
    "/health",
    summary="Notification service health",
    description="Check health of notification service"
)
async def notification_health() -> dict:
    """
    Check notification service health.
    
    **Returns:**
    - Service status
    - Active connections count
    - Redis status
    """
    connection_count = sum(
        len(queues) for queues in notification_service._connections.values()
    )
    
    return {
        "status": "healthy",
        "service": "notifications",
        "active_connections": connection_count,
        "redis_configured": notification_service._redis is not None,
        "timestamp": datetime.utcnow().isoformat()
    }
</file>

<file path="app/api/v1/tools.py">
"""
Tool execution API endpoints.
Handles tool registration, execution, approvals, and caching.
"""
from datetime import datetime, timedelta
from typing import List, Optional
import hashlib
import json
from fastapi import APIRouter, Depends, HTTPException, status, Query, BackgroundTasks
from sqlalchemy import select, func, and_, or_, desc
from sqlalchemy.ext.asyncio import AsyncSession

from app.core.deps import (
    get_current_user,
    require_permission,
    get_db
)
from app.models.user import User
from app.models.tool import (
    Tool, 
    ToolExecution, 
    ToolApproval, 
    ToolCache,
    ExecutionStatus,
    ApprovalStatus,
    ToolStatus
)
from app.models.chat import ChatSession
from app.schemas.tool import (
    ToolCreate,
    ToolUpdate,
    ToolResponse,
    ToolListResponse,
    ToolExecutionCreate,
    ToolExecutionUpdate,
    ToolExecutionResponse,
    ToolExecutionListResponse,
    ToolApprovalCreate,
    ToolApprovalUpdate,
    ToolApprovalResponse,
    ToolApprovalListResponse,
    ToolCacheResponse,
    ToolStatistics,
    SystemToolStatistics
)

router = APIRouter(prefix="/tools", tags=["tools"])


# ============================================================================
# Tool Management Endpoints
# ============================================================================

@router.post("/", response_model=ToolResponse, status_code=status.HTTP_201_CREATED)
async def create_tool(
    tool_data: ToolCreate,
    db: AsyncSession = Depends(get_db),
    current_user: User = Depends(require_permission("tools.create"))
):
    """
    Create a new tool.
    Requires 'tools.create' permission.
    """
    # Check if tool with same name exists
    result = await db.execute(
        select(Tool).where(Tool.name == tool_data.name)
    )
    existing_tool = result.scalar_one_or_none()
    
    if existing_tool:
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail=f"Tool with name '{tool_data.name}' already exists"
        )
    
    # Create tool
    tool = Tool(
        **tool_data.model_dump(),
        created_by=current_user.id
    )
    
    db.add(tool)
    await db.commit()
    await db.refresh(tool)
    
    return tool


@router.get("/", response_model=ToolListResponse)
async def list_tools(
    page: int = Query(1, ge=1, description="Page number"),
    page_size: int = Query(50, ge=1, le=100, description="Items per page"),
    status_filter: Optional[ToolStatus] = Query(None, description="Filter by status"),
    category: Optional[str] = Query(None, description="Filter by category"),
    search: Optional[str] = Query(None, description="Search in name/description"),
    db: AsyncSession = Depends(get_db),
    current_user: User = Depends(get_current_user)
):
    """
    List all tools with pagination and filtering.
    """
    # Build query
    query = select(Tool)
    
    # Apply filters
    filters = []
    if status_filter:
        filters.append(Tool.status == status_filter)
    if category:
        filters.append(Tool.category == category)
    if search:
        search_term = f"%{search}%"
        filters.append(
            or_(
                Tool.name.ilike(search_term),
                Tool.description.ilike(search_term)
            )
        )
    
    if filters:
        query = query.where(and_(*filters))
    
    # Get total count
    count_query = select(func.count()).select_from(Tool)
    if filters:
        count_query = count_query.where(and_(*filters))
    result = await db.execute(count_query)
    total = result.scalar()
    
    # Apply pagination
    query = query.offset((page - 1) * page_size).limit(page_size)
    query = query.order_by(Tool.name)
    
    result = await db.execute(query)
    tools = result.scalars().all()
    
    return ToolListResponse(
        tools=tools,
        total=total,
        page=page,
        page_size=page_size
    )


@router.get("/{tool_id}", response_model=ToolResponse)
async def get_tool(
    tool_id: int,
    db: AsyncSession = Depends(get_db),
    current_user: User = Depends(get_current_user)
):
    """
    Get a specific tool by ID.
    """
    result = await db.execute(
        select(Tool).where(Tool.id == tool_id)
    )
    tool = result.scalar_one_or_none()
    
    if not tool:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail=f"Tool with ID {tool_id} not found"
        )
    
    return tool


@router.put("/{tool_id}", response_model=ToolResponse)
async def update_tool(
    tool_id: int,
    tool_data: ToolUpdate,
    db: AsyncSession = Depends(get_db),
    current_user: User = Depends(require_permission("tools.update"))
):
    """
    Update a tool.
    Requires 'tools.update' permission.
    """
    result = await db.execute(
        select(Tool).where(Tool.id == tool_id)
    )
    tool = result.scalar_one_or_none()
    
    if not tool:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail=f"Tool with ID {tool_id} not found"
        )
    
    # Update fields
    update_data = tool_data.model_dump(exclude_unset=True)
    for field, value in update_data.items():
        setattr(tool, field, value)
    
    tool.updated_at = datetime.utcnow()
    
    await db.commit()
    await db.refresh(tool)
    
    return tool


@router.delete("/{tool_id}", status_code=status.HTTP_204_NO_CONTENT)
async def delete_tool(
    tool_id: int,
    db: AsyncSession = Depends(get_db),
    current_user: User = Depends(require_permission("tools.delete"))
):
    """
    Delete a tool (soft delete by setting status to inactive).
    Requires 'tools.delete' permission.
    """
    result = await db.execute(
        select(Tool).where(Tool.id == tool_id)
    )
    tool = result.scalar_one_or_none()
    
    if not tool:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail=f"Tool with ID {tool_id} not found"
        )
    
    # Soft delete
    tool.status = ToolStatus.INACTIVE
    tool.updated_at = datetime.utcnow()
    
    await db.commit()


# ============================================================================
# Tool Execution Endpoints
# ============================================================================

def _generate_cache_key(tool_id: int, input_data: dict) -> str:
    """Generate a cache key from tool ID and input data."""
    input_str = json.dumps(input_data, sort_keys=True)
    input_hash = hashlib.sha256(input_str.encode()).hexdigest()
    return f"tool:{tool_id}:{input_hash}"


async def _check_cache(
    db: AsyncSession,
    tool_id: int,
    input_data: dict
) -> Optional[ToolCache]:
    """Check if a cached result exists for this tool execution."""
    cache_key = _generate_cache_key(tool_id, input_data)
    
    result = await db.execute(
        select(ToolCache).where(
            and_(
                ToolCache.cache_key == cache_key,
                or_(
                    ToolCache.expires_at.is_(None),
                    ToolCache.expires_at > datetime.utcnow()
                )
            )
        )
    )
    cache = result.scalar_one_or_none()
    
    if cache:
        # Update hit count
        cache.hit_count += 1
        cache.last_hit_at = datetime.utcnow()
        await db.commit()
    
    return cache


async def _execute_tool(
    execution: ToolExecution,
    tool: Tool,
    db: AsyncSession
):
    """
    Execute a tool (background task).
    This is a placeholder - actual execution would integrate with tool runners.
    """
    try:
        execution.status = ExecutionStatus.RUNNING
        execution.started_at = datetime.utcnow()
        await db.commit()
        
        # TODO: Integrate with actual tool execution framework
        # For now, simulate execution
        import asyncio
        await asyncio.sleep(1)
        
        # Mock successful execution
        execution.status = ExecutionStatus.COMPLETED
        execution.completed_at = datetime.utcnow()
        execution.execution_time = (
            execution.completed_at - execution.started_at
        ).total_seconds()
        execution.output_data = {
            "result": "Tool executed successfully",
            "timestamp": datetime.utcnow().isoformat()
        }
        
        # Update tool statistics
        tool.execution_count += 1
        tool.success_count += 1
        
        # Calculate average execution time
        if tool.avg_execution_time:
            tool.avg_execution_time = (
                (tool.avg_execution_time * (tool.execution_count - 1) + 
                 execution.execution_time) / tool.execution_count
            )
        else:
            tool.avg_execution_time = execution.execution_time
        
        tool.last_executed_at = datetime.utcnow()
        
        # Cache the result
        input_str = json.dumps(execution.input_data, sort_keys=True)
        input_hash = hashlib.sha256(input_str.encode()).hexdigest()
        cache_key = _generate_cache_key(tool.id, execution.input_data)
        
        cache = ToolCache(
            tool_id=tool.id,
            cache_key=cache_key,
            input_hash=input_hash,
            output_data=execution.output_data,
            expires_at=datetime.utcnow() + timedelta(hours=24)
        )
        db.add(cache)
        
        await db.commit()
        
    except Exception as e:
        execution.status = ExecutionStatus.FAILED
        execution.error_message = str(e)
        execution.completed_at = datetime.utcnow()
        
        tool.execution_count += 1
        tool.failure_count += 1
        
        await db.commit()


@router.post("/execute", response_model=ToolExecutionResponse, status_code=status.HTTP_201_CREATED)
async def execute_tool(
    execution_data: ToolExecutionCreate,
    background_tasks: BackgroundTasks,
    db: AsyncSession = Depends(get_db),
    current_user: User = Depends(get_current_user)
):
    """
    Execute a tool.
    If tool requires approval, creates a pending approval request.
    Otherwise, executes immediately.
    """
    # Get tool
    result = await db.execute(
        select(Tool).where(Tool.id == execution_data.tool_id)
    )
    tool = result.scalar_one_or_none()
    
    if not tool:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail=f"Tool with ID {execution_data.tool_id} not found"
        )
    
    if tool.status != ToolStatus.ACTIVE:
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail=f"Tool '{tool.name}' is not active"
        )
    
    # Check cache first
    cache = await _check_cache(db, tool.id, execution_data.input_data)
    if cache:
        # Return cached result
        execution = ToolExecution(
            tool_id=tool.id,
            user_id=current_user.id,
            session_id=execution_data.session_id,
            status=ExecutionStatus.COMPLETED,
            input_data=execution_data.input_data,
            output_data=cache.output_data,
            started_at=datetime.utcnow(),
            completed_at=datetime.utcnow(),
            execution_time=0.0,
            requires_approval=False
        )
        db.add(execution)
        await db.commit()
        await db.refresh(execution)
        
        return ToolExecutionResponse(
            id=execution.id,
            tool_id=execution.tool_id,
            tool_name=tool.name,
            user_id=execution.user_id,
            session_id=execution.session_id,
            status=execution.status,
            input_data=execution.input_data,
            output_data=execution.output_data,
            error_message=execution.error_message,
            started_at=execution.started_at,
            completed_at=execution.completed_at,
            execution_time=execution.execution_time,
            retry_count=execution.retry_count,
            requires_approval=execution.requires_approval,
            approval_id=execution.approval_id,
            created_at=execution.created_at
        )
    
    # Create execution record
    execution = ToolExecution(
        tool_id=tool.id,
        user_id=current_user.id,
        session_id=execution_data.session_id,
        status=ExecutionStatus.PENDING,
        input_data=execution_data.input_data,
        requires_approval=tool.requires_approval
    )
    
    db.add(execution)
    await db.commit()
    await db.refresh(execution)
    
    # If requires approval, create approval request
    if tool.requires_approval:
        approval = ToolApproval(
            execution_id=execution.id,
            requested_by=current_user.id,
            status=ApprovalStatus.PENDING,
            reason=execution_data.approval_reason,
            expires_at=datetime.utcnow() + timedelta(hours=24)
        )
        db.add(approval)
        await db.commit()
        await db.refresh(approval)
        
        execution.approval_id = approval.id
        await db.commit()
    else:
        # Execute immediately in background
        background_tasks.add_task(_execute_tool, execution, tool, db)
    
    await db.refresh(execution)
    
    return ToolExecutionResponse(
        id=execution.id,
        tool_id=execution.tool_id,
        tool_name=tool.name,
        user_id=execution.user_id,
        session_id=execution.session_id,
        status=execution.status,
        input_data=execution.input_data,
        output_data=execution.output_data,
        error_message=execution.error_message,
        started_at=execution.started_at,
        completed_at=execution.completed_at,
        execution_time=execution.execution_time,
        retry_count=execution.retry_count,
        requires_approval=execution.requires_approval,
        approval_id=execution.approval_id,
        created_at=execution.created_at
    )


@router.get("/executions/", response_model=ToolExecutionListResponse)
async def list_executions(
    page: int = Query(1, ge=1),
    page_size: int = Query(50, ge=1, le=100),
    tool_id: Optional[int] = Query(None),
    session_id: Optional[int] = Query(None),
    status_filter: Optional[ExecutionStatus] = Query(None),
    db: AsyncSession = Depends(get_db),
    current_user: User = Depends(get_current_user)
):
    """
    List tool executions with filtering.
    Users can see their own executions, admins can see all.
    """
    # Build query
    query = select(ToolExecution)
    
    # Apply filters
    filters = [ToolExecution.user_id == current_user.id]
    
    # Admins can see all executions
    if await current_user.is_superuser():
        filters = []
    
    if tool_id:
        filters.append(ToolExecution.tool_id == tool_id)
    if session_id:
        filters.append(ToolExecution.session_id == session_id)
    if status_filter:
        filters.append(ToolExecution.status == status_filter)
    
    if filters:
        query = query.where(and_(*filters))
    
    # Get total count
    count_query = select(func.count()).select_from(ToolExecution)
    if filters:
        count_query = count_query.where(and_(*filters))
    result = await db.execute(count_query)
    total = result.scalar()
    
    # Apply pagination
    query = query.offset((page - 1) * page_size).limit(page_size)
    query = query.order_by(desc(ToolExecution.created_at))
    
    result = await db.execute(query)
    executions = result.scalars().all()
    
    # Get tool names
    tool_ids = [e.tool_id for e in executions]
    tools_result = await db.execute(
        select(Tool).where(Tool.id.in_(tool_ids))
    )
    tools = {t.id: t.name for t in tools_result.scalars().all()}
    
    execution_responses = [
        ToolExecutionResponse(
            id=e.id,
            tool_id=e.tool_id,
            tool_name=tools.get(e.tool_id, "Unknown"),
            user_id=e.user_id,
            session_id=e.session_id,
            status=e.status,
            input_data=e.input_data,
            output_data=e.output_data,
            error_message=e.error_message,
            started_at=e.started_at,
            completed_at=e.completed_at,
            execution_time=e.execution_time,
            retry_count=e.retry_count,
            requires_approval=e.requires_approval,
            approval_id=e.approval_id,
            created_at=e.created_at
        )
        for e in executions
    ]
    
    return ToolExecutionListResponse(
        executions=execution_responses,
        total=total,
        page=page,
        page_size=page_size
    )


@router.get("/executions/{execution_id}", response_model=ToolExecutionResponse)
async def get_execution(
    execution_id: int,
    db: AsyncSession = Depends(get_db),
    current_user: User = Depends(get_current_user)
):
    """
    Get a specific tool execution.
    """
    result = await db.execute(
        select(ToolExecution).where(ToolExecution.id == execution_id)
    )
    execution = result.scalar_one_or_none()
    
    if not execution:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail=f"Execution with ID {execution_id} not found"
        )
    
    # Check access
    if execution.user_id != current_user.id and not await current_user.is_superuser():
        raise HTTPException(
            status_code=status.HTTP_403_FORBIDDEN,
            detail="Not authorized to view this execution"
        )
    
    # Get tool name
    tool_result = await db.execute(
        select(Tool).where(Tool.id == execution.tool_id)
    )
    tool = tool_result.scalar_one_or_none()
    
    return ToolExecutionResponse(
        id=execution.id,
        tool_id=execution.tool_id,
        tool_name=tool.name if tool else "Unknown",
        user_id=execution.user_id,
        session_id=execution.session_id,
        status=execution.status,
        input_data=execution.input_data,
        output_data=execution.output_data,
        error_message=execution.error_message,
        started_at=execution.started_at,
        completed_at=execution.completed_at,
        execution_time=execution.execution_time,
        retry_count=execution.retry_count,
        requires_approval=execution.requires_approval,
        approval_id=execution.approval_id,
        created_at=execution.created_at
    )


# ============================================================================
# Tool Approval Endpoints
# ============================================================================

@router.get("/approvals/", response_model=ToolApprovalListResponse)
async def list_approvals(
    page: int = Query(1, ge=1),
    page_size: int = Query(50, ge=1, le=100),
    status_filter: Optional[ApprovalStatus] = Query(None),
    pending_only: bool = Query(False, description="Show only pending approvals"),
    db: AsyncSession = Depends(get_db),
    current_user: User = Depends(require_permission("tools.approve"))
):
    """
    List tool approval requests.
    Requires 'tools.approve' permission.
    """
    # Build query
    query = select(ToolApproval)
    
    filters = []
    if status_filter:
        filters.append(ToolApproval.status == status_filter)
    if pending_only:
        filters.append(ToolApproval.status == ApprovalStatus.PENDING)
    
    if filters:
        query = query.where(and_(*filters))
    
    # Get total count
    count_query = select(func.count()).select_from(ToolApproval)
    if filters:
        count_query = count_query.where(and_(*filters))
    result = await db.execute(count_query)
    total = result.scalar()
    
    # Apply pagination
    query = query.offset((page - 1) * page_size).limit(page_size)
    query = query.order_by(desc(ToolApproval.requested_at))
    
    result = await db.execute(query)
    approvals = result.scalars().all()
    
    # Get related data
    execution_ids = [a.execution_id for a in approvals]
    executions_result = await db.execute(
        select(ToolExecution).where(ToolExecution.id.in_(execution_ids))
    )
    executions = {e.id: e for e in executions_result.scalars().all()}
    
    tool_ids = [e.tool_id for e in executions.values()]
    tools_result = await db.execute(
        select(Tool).where(Tool.id.in_(tool_ids))
    )
    tools = {t.id: t.name for t in tools_result.scalars().all()}
    
    user_ids = [a.requested_by for a in approvals]
    if any(a.approved_by for a in approvals):
        user_ids.extend([a.approved_by for a in approvals if a.approved_by])
    users_result = await db.execute(
        select(User).where(User.id.in_(user_ids))
    )
    users = {u.id: u.username for u in users_result.scalars().all()}
    
    approval_responses = [
        ToolApprovalResponse(
            id=a.id,
            execution_id=a.execution_id,
            tool_name=tools.get(executions[a.execution_id].tool_id, "Unknown"),
            requester_id=a.requested_by,
            requester_name=users.get(a.requested_by, "Unknown"),
            approver_id=a.approved_by,
            approver_name=users.get(a.approved_by) if a.approved_by else None,
            status=a.status,
            reason=a.reason,
            notes=a.notes,
            requested_at=a.requested_at,
            responded_at=a.responded_at,
            expires_at=a.expires_at
        )
        for a in approvals
    ]
    
    return ToolApprovalListResponse(
        approvals=approval_responses,
        total=total,
        page=page,
        page_size=page_size
    )


@router.put("/approvals/{approval_id}", response_model=ToolApprovalResponse)
async def update_approval(
    approval_id: int,
    approval_data: ToolApprovalUpdate,
    background_tasks: BackgroundTasks,
    db: AsyncSession = Depends(get_db),
    current_user: User = Depends(require_permission("tools.approve"))
):
    """
    Approve or reject a tool execution request.
    Requires 'tools.approve' permission.
    """
    result = await db.execute(
        select(ToolApproval).where(ToolApproval.id == approval_id)
    )
    approval = result.scalar_one_or_none()
    
    if not approval:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail=f"Approval with ID {approval_id} not found"
        )
    
    if approval.status != ApprovalStatus.PENDING:
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail=f"Approval is already {approval.status.value}"
        )
    
    # Update approval
    approval.status = approval_data.status
    approval.notes = approval_data.notes
    approval.approved_by = current_user.id
    approval.responded_at = datetime.utcnow()
    
    # Get execution
    exec_result = await db.execute(
        select(ToolExecution).where(ToolExecution.id == approval.execution_id)
    )
    execution = exec_result.scalar_one()
    
    if approval_data.status == ApprovalStatus.APPROVED:
        # Execute tool in background
        execution.status = ExecutionStatus.APPROVED
        
        tool_result = await db.execute(
            select(Tool).where(Tool.id == execution.tool_id)
        )
        tool = tool_result.scalar_one()
        
        background_tasks.add_task(_execute_tool, execution, tool, db)
    else:
        # Reject execution
        execution.status = ExecutionStatus.REJECTED
        execution.completed_at = datetime.utcnow()
        execution.error_message = f"Execution rejected by {current_user.username}"
    
    await db.commit()
    await db.refresh(approval)
    
    # Get related data for response
    user_result = await db.execute(
        select(User).where(User.id == approval.requested_by)
    )
    requester = user_result.scalar_one()
    
    tool_result = await db.execute(
        select(Tool).where(Tool.id == execution.tool_id)
    )
    tool = tool_result.scalar_one()
    
    return ToolApprovalResponse(
        id=approval.id,
        execution_id=approval.execution_id,
        tool_name=tool.name,
        requester_id=approval.requested_by,
        requester_name=requester.username,
        approver_id=approval.approved_by,
        approver_name=current_user.username,
        status=approval.status,
        reason=approval.reason,
        notes=approval.notes,
        requested_at=approval.requested_at,
        responded_at=approval.responded_at,
        expires_at=approval.expires_at
    )


# ============================================================================
# Tool Statistics Endpoints
# ============================================================================

@router.get("/statistics/system", response_model=SystemToolStatistics)
async def get_system_statistics(
    db: AsyncSession = Depends(get_db),
    current_user: User = Depends(require_permission("tools.view_stats"))
):
    """
    Get system-wide tool statistics.
    Requires 'tools.view_stats' permission.
    """
    # Get total tools
    total_tools_result = await db.execute(select(func.count()).select_from(Tool))
    total_tools = total_tools_result.scalar()
    
    # Get active tools
    active_tools_result = await db.execute(
        select(func.count()).select_from(Tool).where(Tool.status == ToolStatus.ACTIVE)
    )
    active_tools = active_tools_result.scalar()
    
    # Get total executions
    total_exec_result = await db.execute(
        select(func.count()).select_from(ToolExecution)
    )
    total_executions = total_exec_result.scalar()
    
    # Get pending approvals
    pending_approvals_result = await db.execute(
        select(func.count()).select_from(ToolApproval)
        .where(ToolApproval.status == ApprovalStatus.PENDING)
    )
    pending_approvals = pending_approvals_result.scalar()
    
    # Get success rate
    completed_result = await db.execute(
        select(func.count()).select_from(ToolExecution)
        .where(ToolExecution.status == ExecutionStatus.COMPLETED)
    )
    completed_count = completed_result.scalar()
    
    success_rate = (completed_count / total_executions * 100) if total_executions > 0 else 0
    
    # Get average execution time
    avg_time_result = await db.execute(
        select(func.avg(ToolExecution.execution_time))
        .where(ToolExecution.execution_time.is_not(None))
    )
    avg_time = avg_time_result.scalar() or 0.0
    
    # Get top tools
    top_tools_result = await db.execute(
        select(Tool).order_by(desc(Tool.execution_count)).limit(10)
    )
    top_tools = top_tools_result.scalars().all()
    
    top_tool_stats = [
        ToolStatistics(
            tool_id=tool.id,
            tool_name=tool.name,
            total_executions=tool.execution_count,
            successful_executions=tool.success_count,
            failed_executions=tool.failure_count,
            avg_execution_time=tool.avg_execution_time or 0.0,
            success_rate=(tool.success_count / tool.execution_count * 100) 
                if tool.execution_count > 0 else 0,
            last_executed=tool.last_executed_at
        )
        for tool in top_tools
    ]
    
    return SystemToolStatistics(
        total_tools=total_tools,
        active_tools=active_tools,
        total_executions=total_executions,
        pending_approvals=pending_approvals,
        execution_success_rate=success_rate,
        avg_execution_time=avg_time,
        top_tools=top_tool_stats
    )


@router.get("/statistics/{tool_id}", response_model=ToolStatistics)
async def get_tool_statistics(
    tool_id: int,
    db: AsyncSession = Depends(get_db),
    current_user: User = Depends(get_current_user)
):
    """
    Get statistics for a specific tool.
    """
    result = await db.execute(
        select(Tool).where(Tool.id == tool_id)
    )
    tool = result.scalar_one_or_none()
    
    if not tool:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail=f"Tool with ID {tool_id} not found"
        )
    
    success_rate = (
        (tool.success_count / tool.execution_count * 100) 
        if tool.execution_count > 0 
        else 0
    )
    
    return ToolStatistics(
        tool_id=tool.id,
        tool_name=tool.name,
        total_executions=tool.execution_count,
        successful_executions=tool.success_count,
        failed_executions=tool.failure_count,
        avg_execution_time=tool.avg_execution_time or 0.0,
        success_rate=success_rate,
        last_executed=tool.last_executed_at
    )
</file>

<file path="app/api/v1/vault.py">
"""
Secrets vault API endpoints.
Handles secure storage, retrieval, and management of secrets.
"""
from datetime import datetime, timedelta
from typing import List, Optional
import hashlib
from fastapi import APIRouter, Depends, HTTPException, status, Query, Request
from sqlalchemy import select, func, and_, or_, desc
from sqlalchemy.ext.asyncio import AsyncSession

from app.core.deps import (
    get_current_user,
    require_permission,
    get_db
)
from app.models.user import User
from app.models.secret import Secret, SecretVersion, SecretAccessLog, SecretType
from app.schemas.secret import (
    SecretCreate,
    SecretUpdate,
    SecretResponse,
    SecretValueResponse,
    SecretListResponse,
    SecretVersionResponse,
    SecretVersionListResponse,
    SecretAccessLogResponse,
    SecretAccessLogListResponse,
    SecretRotateRequest,
    SecretStatistics
)
from app.api.v1.audit import create_audit_log
from app.models.audit import AuditAction

router = APIRouter(prefix="/vault", tags=["vault"])


# ============================================================================
# Helper Functions
# ============================================================================

async def log_secret_access(
    db: AsyncSession,
    secret: Secret,
    user: User,
    access_type: str,
    request: Request
):
    """Log secret access for audit trail"""
    ip_address = request.client.host if request.client else None
    user_agent = request.headers.get("user-agent")
    
    access_log = SecretAccessLog(
        secret_id=secret.id,
        user_id=user.id,
        access_type=access_type,
        ip_address=ip_address,
        user_agent=user_agent
    )
    
    db.add(access_log)
    
    # Update secret's last accessed time
    secret.last_accessed_at = datetime.utcnow()
    secret.access_count += 1
    
    # Create audit log
    await create_audit_log(
        db=db,
        user_id=user.id,
        action=AuditAction.SECRET_ACCESSED if access_type == "read" else AuditAction.SECRET_CREATED,
        resource_type="secret",
        resource_id=secret.id,
        details={
            "secret_name": secret.name,
            "access_type": access_type,
            "secret_type": secret.secret_type.value
        },
        ip_address=ip_address,
        user_agent=user_agent
    )
    
    await db.commit()


def _hash_value(value: str) -> str:
    """Generate SHA256 hash of value for versioning"""
    return hashlib.sha256(value.encode()).hexdigest()


# ============================================================================
# Secret Management Endpoints
# ============================================================================

@router.post("/secrets", response_model=SecretResponse, status_code=status.HTTP_201_CREATED)
async def create_secret(
    secret_data: SecretCreate,
    request: Request,
    db: AsyncSession = Depends(get_db),
    current_user: User = Depends(require_permission("vault.create"))
):
    """
    Create a new secret.
    Requires 'vault.create' permission.
    The value will be encrypted at rest.
    """
    # Check if secret with same name exists
    result = await db.execute(
        select(Secret).where(Secret.name == secret_data.name)
    )
    existing_secret = result.scalar_one_or_none()
    
    if existing_secret:
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail=f"Secret with name '{secret_data.name}' already exists"
        )
    
    # Create secret
    secret = Secret(
        name=secret_data.name,
        description=secret_data.description,
        secret_type=secret_data.secret_type,
        tags=secret_data.tags,
        expires_at=secret_data.expires_at,
        created_by=current_user.id
    )
    
    # Encrypt and set value (uses property setter)
    secret.value = secret_data.value
    
    db.add(secret)
    await db.commit()
    await db.refresh(secret)
    
    # Create initial version
    value_hash = _hash_value(secret_data.value)
    version = SecretVersion(
        secret_id=secret.id,
        version=1,
        value_hash=value_hash,
        created_by=current_user.id
    )
    db.add(version)
    
    # Log access
    await log_secret_access(db, secret, current_user, "create", request)
    
    return SecretResponse(
        id=secret.id,
        name=secret.name,
        description=secret.description,
        secret_type=secret.secret_type,
        tags=secret.tags,
        is_active=secret.is_active,
        is_expired=secret.is_expired,
        version=secret.version,
        expires_at=secret.expires_at,
        last_accessed_at=secret.last_accessed_at,
        last_rotated_at=secret.last_rotated_at,
        created_by=secret.created_by,
        created_at=secret.created_at,
        updated_at=secret.updated_at
    )


@router.get("/secrets", response_model=SecretListResponse)
async def list_secrets(
    page: int = Query(1, ge=1),
    page_size: int = Query(50, ge=1, le=100),
    secret_type: Optional[SecretType] = Query(None),
    is_active: Optional[bool] = Query(None),
    include_expired: bool = Query(False, description="Include expired secrets"),
    search: Optional[str] = Query(None, description="Search in name/description"),
    db: AsyncSession = Depends(get_db),
    current_user: User = Depends(require_permission("vault.list"))
):
    """
    List secrets with filtering and pagination.
    Requires 'vault.list' permission.
    Does not include secret values.
    """
    # Build query
    query = select(Secret)
    
    filters = []
    if secret_type:
        filters.append(Secret.secret_type == secret_type)
    if is_active is not None:
        filters.append(Secret.is_active == is_active)
    if not include_expired:
        filters.append(or_(
            Secret.expires_at.is_(None),
            Secret.expires_at > datetime.utcnow()
        ))
    if search:
        search_term = f"%{search}%"
        filters.append(or_(
            Secret.name.ilike(search_term),
            Secret.description.ilike(search_term)
        ))
    
    if filters:
        query = query.where(and_(*filters))
    
    # Get total count
    count_query = select(func.count()).select_from(Secret)
    if filters:
        count_query = count_query.where(and_(*filters))
    result = await db.execute(count_query)
    total = result.scalar()
    
    # Apply pagination
    query = query.offset((page - 1) * page_size).limit(page_size)
    query = query.order_by(Secret.name)
    
    result = await db.execute(query)
    secrets = result.scalars().all()
    
    secret_responses = [
        SecretResponse(
            id=s.id,
            name=s.name,
            description=s.description,
            secret_type=s.secret_type,
            tags=s.tags,
            is_active=s.is_active,
            is_expired=s.is_expired,
            version=s.version,
            expires_at=s.expires_at,
            last_accessed_at=s.last_accessed_at,
            last_rotated_at=s.last_rotated_at,
            created_by=s.created_by,
            created_at=s.created_at,
            updated_at=s.updated_at
        )
        for s in secrets
    ]
    
    return SecretListResponse(
        secrets=secret_responses,
        total=total,
        page=page,
        page_size=page_size
    )


@router.get("/secrets/{secret_id}", response_model=SecretResponse)
async def get_secret_metadata(
    secret_id: int,
    db: AsyncSession = Depends(get_db),
    current_user: User = Depends(require_permission("vault.read"))
):
    """
    Get secret metadata without the value.
    Requires 'vault.read' permission.
    """
    result = await db.execute(
        select(Secret).where(Secret.id == secret_id)
    )
    secret = result.scalar_one_or_none()
    
    if not secret:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail=f"Secret with ID {secret_id} not found"
        )
    
    return SecretResponse(
        id=secret.id,
        name=secret.name,
        description=secret.description,
        secret_type=secret.secret_type,
        tags=secret.tags,
        is_active=secret.is_active,
        is_expired=secret.is_expired,
        version=secret.version,
        expires_at=secret.expires_at,
        last_accessed_at=secret.last_accessed_at,
        last_rotated_at=secret.last_rotated_at,
        created_by=secret.created_by,
        created_at=secret.created_at,
        updated_at=secret.updated_at
    )


@router.get("/secrets/{secret_id}/value", response_model=SecretValueResponse)
async def get_secret_value(
    secret_id: int,
    request: Request,
    db: AsyncSession = Depends(get_db),
    current_user: User = Depends(require_permission("vault.read"))
):
    """
    Get secret with decrypted value.
    Requires 'vault.read' permission.
    Access is logged for audit.
    """
    result = await db.execute(
        select(Secret).where(Secret.id == secret_id)
    )
    secret = result.scalar_one_or_none()
    
    if not secret:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail=f"Secret with ID {secret_id} not found"
        )
    
    if not secret.is_active:
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail="Secret is not active"
        )
    
    if secret.is_expired:
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail="Secret has expired"
        )
    
    # Log access
    await log_secret_access(db, secret, current_user, "read", request)
    
    # Decrypt value (uses property getter)
    decrypted_value = secret.value
    
    return SecretValueResponse(
        id=secret.id,
        name=secret.name,
        description=secret.description,
        secret_type=secret.secret_type,
        value=decrypted_value,
        tags=secret.tags,
        is_active=secret.is_active,
        version=secret.version,
        expires_at=secret.expires_at,
        created_at=secret.created_at
    )


@router.put("/secrets/{secret_id}", response_model=SecretResponse)
async def update_secret(
    secret_id: int,
    secret_data: SecretUpdate,
    request: Request,
    db: AsyncSession = Depends(get_db),
    current_user: User = Depends(require_permission("vault.update"))
):
    """
    Update secret metadata or value.
    Requires 'vault.update' permission.
    Updating the value creates a new version.
    """
    result = await db.execute(
        select(Secret).where(Secret.id == secret_id)
    )
    secret = result.scalar_one_or_none()
    
    if not secret:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail=f"Secret with ID {secret_id} not found"
        )
    
    # Update fields
    update_data = secret_data.model_dump(exclude_unset=True)
    
    # Handle value update separately for versioning
    new_value = update_data.pop("value", None)
    
    for field, value in update_data.items():
        setattr(secret, field, value)
    
    if new_value:
        # Create new version
        old_value = secret.value
        secret.value = new_value
        secret.version += 1
        secret.last_rotated_at = datetime.utcnow()
        
        value_hash = _hash_value(new_value)
        version = SecretVersion(
            secret_id=secret.id,
            version=secret.version,
            value_hash=value_hash,
            created_by=current_user.id
        )
        db.add(version)
    
    secret.updated_at = datetime.utcnow()
    
    await log_secret_access(db, secret, current_user, "update", request)
    
    return SecretResponse(
        id=secret.id,
        name=secret.name,
        description=secret.description,
        secret_type=secret.secret_type,
        tags=secret.tags,
        is_active=secret.is_active,
        is_expired=secret.is_expired,
        version=secret.version,
        expires_at=secret.expires_at,
        last_accessed_at=secret.last_accessed_at,
        last_rotated_at=secret.last_rotated_at,
        created_by=secret.created_by,
        created_at=secret.created_at,
        updated_at=secret.updated_at
    )


@router.post("/secrets/{secret_id}/rotate", response_model=SecretResponse)
async def rotate_secret(
    secret_id: int,
    rotate_data: SecretRotateRequest,
    request: Request,
    db: AsyncSession = Depends(get_db),
    current_user: User = Depends(require_permission("vault.rotate"))
):
    """
    Rotate a secret by creating a new version.
    Requires 'vault.rotate' permission.
    """
    result = await db.execute(
        select(Secret).where(Secret.id == secret_id)
    )
    secret = result.scalar_one_or_none()
    
    if not secret:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail=f"Secret with ID {secret_id} not found"
        )
    
    # Update value and increment version
    secret.value = rotate_data.new_value
    secret.version += 1
    secret.last_rotated_at = datetime.utcnow()
    secret.updated_at = datetime.utcnow()
    
    # Create version record
    value_hash = _hash_value(rotate_data.new_value)
    version = SecretVersion(
        secret_id=secret.id,
        version=secret.version,
        value_hash=value_hash,
        created_by=current_user.id
    )
    db.add(version)
    
    # Log rotation
    await create_audit_log(
        db=db,
        user_id=current_user.id,
        action=AuditAction.SECRET_ROTATED,
        resource_type="secret",
        resource_id=secret.id,
        details={
            "secret_name": secret.name,
            "new_version": secret.version,
            "reason": rotate_data.reason
        }
    )
    
    await log_secret_access(db, secret, current_user, "rotate", request)
    
    return SecretResponse(
        id=secret.id,
        name=secret.name,
        description=secret.description,
        secret_type=secret.secret_type,
        tags=secret.tags,
        is_active=secret.is_active,
        is_expired=secret.is_expired,
        version=secret.version,
        expires_at=secret.expires_at,
        last_accessed_at=secret.last_accessed_at,
        last_rotated_at=secret.last_rotated_at,
        created_by=secret.created_by,
        created_at=secret.created_at,
        updated_at=secret.updated_at
    )


@router.delete("/secrets/{secret_id}", status_code=status.HTTP_204_NO_CONTENT)
async def delete_secret(
    secret_id: int,
    request: Request,
    db: AsyncSession = Depends(get_db),
    current_user: User = Depends(require_permission("vault.delete"))
):
    """
    Delete a secret (soft delete by deactivating).
    Requires 'vault.delete' permission.
    """
    result = await db.execute(
        select(Secret).where(Secret.id == secret_id)
    )
    secret = result.scalar_one_or_none()
    
    if not secret:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail=f"Secret with ID {secret_id} not found"
        )
    
    # Soft delete
    secret.is_active = False
    secret.updated_at = datetime.utcnow()
    
    # Log deletion
    await create_audit_log(
        db=db,
        user_id=current_user.id,
        action=AuditAction.SECRET_DELETED,
        resource_type="secret",
        resource_id=secret.id,
        details={"secret_name": secret.name}
    )
    
    await db.commit()


# ============================================================================
# Secret Version Endpoints
# ============================================================================

@router.get("/secrets/{secret_id}/versions", response_model=SecretVersionListResponse)
async def list_secret_versions(
    secret_id: int,
    db: AsyncSession = Depends(get_db),
    current_user: User = Depends(require_permission("vault.read"))
):
    """
    List all versions of a secret.
    Requires 'vault.read' permission.
    """
    # Verify secret exists
    result = await db.execute(
        select(Secret).where(Secret.id == secret_id)
    )
    secret = result.scalar_one_or_none()
    
    if not secret:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail=f"Secret with ID {secret_id} not found"
        )
    
    # Get versions
    versions_result = await db.execute(
        select(SecretVersion)
        .where(SecretVersion.secret_id == secret_id)
        .order_by(desc(SecretVersion.version))
    )
    versions = versions_result.scalars().all()
    
    version_responses = [
        SecretVersionResponse(
            id=v.id,
            secret_id=v.secret_id,
            version=v.version,
            value_hash=v.value_hash,
            created_by=v.created_by,
            created_at=v.created_at
        )
        for v in versions
    ]
    
    return SecretVersionListResponse(
        versions=version_responses,
        total=len(versions)
    )


# ============================================================================
# Secret Access Log Endpoints
# ============================================================================

@router.get("/secrets/{secret_id}/access-logs", response_model=SecretAccessLogListResponse)
async def get_secret_access_logs(
    secret_id: int,
    page: int = Query(1, ge=1),
    page_size: int = Query(50, ge=1, le=100),
    db: AsyncSession = Depends(get_db),
    current_user: User = Depends(require_permission("vault.audit"))
):
    """
    Get access logs for a specific secret.
    Requires 'vault.audit' permission.
    """
    # Verify secret exists
    result = await db.execute(
        select(Secret).where(Secret.id == secret_id)
    )
    secret = result.scalar_one_or_none()
    
    if not secret:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail=f"Secret with ID {secret_id} not found"
        )
    
    # Build query
    query = select(SecretAccessLog).where(SecretAccessLog.secret_id == secret_id)
    
    # Get total count
    count_result = await db.execute(
        select(func.count()).select_from(SecretAccessLog)
        .where(SecretAccessLog.secret_id == secret_id)
    )
    total = count_result.scalar()
    
    # Apply pagination
    query = query.offset((page - 1) * page_size).limit(page_size)
    query = query.order_by(desc(SecretAccessLog.accessed_at))
    
    result = await db.execute(query)
    logs = result.scalars().all()
    
    # Get usernames
    user_ids = [log.user_id for log in logs]
    if user_ids:
        users_result = await db.execute(
            select(User).where(User.id.in_(user_ids))
        )
        users = {u.id: u.username for u in users_result.scalars().all()}
    else:
        users = {}
    
    log_responses = [
        SecretAccessLogResponse(
            id=log.id,
            secret_id=log.secret_id,
            secret_name=secret.name,
            user_id=log.user_id,
            username=users.get(log.user_id, "Unknown"),
            access_type=log.access_type,
            ip_address=log.ip_address,
            user_agent=log.user_agent,
            accessed_at=log.accessed_at
        )
        for log in logs
    ]
    
    return SecretAccessLogListResponse(
        logs=log_responses,
        total=total,
        page=page,
        page_size=page_size
    )


# ============================================================================
# Statistics Endpoint
# ============================================================================

@router.get("/statistics", response_model=SecretStatistics)
async def get_vault_statistics(
    db: AsyncSession = Depends(get_db),
    current_user: User = Depends(require_permission("vault.audit"))
):
    """
    Get vault statistics.
    Requires 'vault.audit' permission.
    """
    # Total secrets
    total_result = await db.execute(
        select(func.count()).select_from(Secret)
    )
    total_secrets = total_result.scalar()
    
    # Active secrets
    active_result = await db.execute(
        select(func.count()).select_from(Secret)
        .where(Secret.is_active == True)
    )
    active_secrets = active_result.scalar()
    
    # Expired secrets
    expired_result = await db.execute(
        select(func.count()).select_from(Secret)
        .where(and_(
            Secret.expires_at.is_not(None),
            Secret.expires_at <= datetime.utcnow()
        ))
    )
    expired_secrets = expired_result.scalar()
    
    # Secrets by type
    type_result = await db.execute(
        select(Secret.secret_type, func.count().label('count'))
        .group_by(Secret.secret_type)
    )
    secrets_by_type = {row.secret_type.value: row.count for row in type_result}
    
    # Recent accesses (last 24h)
    recent_result = await db.execute(
        select(func.count()).select_from(SecretAccessLog)
        .where(SecretAccessLog.accessed_at >= datetime.utcnow() - timedelta(hours=24))
    )
    recent_accesses = recent_result.scalar()
    
    # Secrets expiring soon (next 30 days)
    expiring_result = await db.execute(
        select(func.count()).select_from(Secret)
        .where(and_(
            Secret.expires_at.is_not(None),
            Secret.expires_at > datetime.utcnow(),
            Secret.expires_at <= datetime.utcnow() + timedelta(days=30)
        ))
    )
    secrets_expiring_soon = expiring_result.scalar()
    
    # Never accessed
    never_accessed_result = await db.execute(
        select(func.count()).select_from(Secret)
        .where(Secret.last_accessed_at.is_(None))
    )
    never_accessed = never_accessed_result.scalar()
    
    # Most accessed secrets
    most_accessed_result = await db.execute(
        select(Secret.id, Secret.name, Secret.access_count)
        .order_by(desc(Secret.access_count))
        .limit(10)
    )
    most_accessed = [
        {
            "secret_id": row.id,
            "secret_name": row.name,
            "access_count": row.access_count
        }
        for row in most_accessed_result
    ]
    
    return SecretStatistics(
        total_secrets=total_secrets,
        active_secrets=active_secrets,
        expired_secrets=expired_secrets,
        secrets_by_type=secrets_by_type,
        recent_accesses=recent_accesses,
        secrets_expiring_soon=secrets_expiring_soon,
        never_accessed=never_accessed,
        most_accessed=most_accessed
    )
</file>

<file path="app/core/deps.py">
"""FastAPI dependencies for authentication and authorization."""
from typing import Optional

from fastapi import Depends, HTTPException, status
from fastapi.security import HTTPBearer, HTTPAuthorizationCredentials
from sqlalchemy.ext.asyncio import AsyncSession
from sqlalchemy import select

from app.config import settings
from app.core.security import validate_access_token
from app.db.base import get_db
from app.models.user import User

# HTTP Bearer token authentication
security = HTTPBearer()


async def get_current_user(
    credentials: HTTPAuthorizationCredentials = Depends(security),
    db: AsyncSession = Depends(get_db),
) -> User:
    """Get the current authenticated user from JWT token.
    
    Args:
        credentials: HTTP Bearer credentials with JWT token
        db: Async database session
        
    Returns:
        The authenticated user
        
    Raises:
        HTTPException: If token is invalid or user not found
    """
    # Extract token
    token = credentials.credentials
    
    # Validate token and get user ID
    user_id = validate_access_token(token)
    if user_id is None:
        raise HTTPException(
            status_code=status.HTTP_401_UNAUTHORIZED,
            detail="Could not validate credentials",
            headers={"WWW-Authenticate": "Bearer"},
        )
    
    # Get user from database
    result = await db.execute(
        select(User).where(User.id == int(user_id))
    )
    user = result.scalar_one_or_none()
    
    if user is None:
        raise HTTPException(
            status_code=status.HTTP_401_UNAUTHORIZED,
            detail="User not found",
            headers={"WWW-Authenticate": "Bearer"},
        )
    
    # Check if user is active
    if not user.is_active:
        raise HTTPException(
            status_code=status.HTTP_403_FORBIDDEN,
            detail="Inactive user",
        )
    
    return user


async def get_current_active_user(
    current_user: User = Depends(get_current_user),
) -> User:
    """Get current active user (alias for clarity).
    
    Args:
        current_user: The current user from get_current_user
        
    Returns:
        The authenticated active user
    """
    return current_user


async def get_current_superuser(
    current_user: User = Depends(get_current_user),
) -> User:
    """Get current user if they are a superuser.
    
    Args:
        current_user: The current user from get_current_user
        
    Returns:
        The authenticated superuser
        
    Raises:
        HTTPException: If user is not a superuser
    """
    if not current_user.is_superuser:
        raise HTTPException(
            status_code=status.HTTP_403_FORBIDDEN,
            detail="Not enough permissions",
        )
    return current_user


def require_permission(permission: str):
    """Create a dependency that requires a specific permission.
    
    Args:
        permission: The permission name required
        
    Returns:
        A dependency function that checks the permission
    """
    async def permission_checker(
        current_user: User = Depends(get_current_user),
    ) -> User:
        if not current_user.has_permission(permission):
            raise HTTPException(
                status_code=status.HTTP_403_FORBIDDEN,
                detail=f"Permission required: {permission}",
            )
        return current_user
    
    return permission_checker


def require_role(role: str):
    """Create a dependency that requires a specific role.
    
    Args:
        role: The role name required
        
    Returns:
        A dependency function that checks the role
    """
    async def role_checker(
        current_user: User = Depends(get_current_user),
    ) -> User:
        if not current_user.has_role(role):
            raise HTTPException(
                status_code=status.HTTP_403_FORBIDDEN,
                detail=f"Role required: {role}",
            )
        return current_user
    
    return role_checker


async def get_optional_user(
    credentials: Optional[HTTPAuthorizationCredentials] = Depends(HTTPBearer(auto_error=False)),
    db: AsyncSession = Depends(get_db),
) -> Optional[User]:
    """Get the current user if authenticated, None otherwise.
    
    Useful for endpoints that work with or without authentication.
    
    Args:
        credentials: Optional HTTP Bearer credentials
        db: Async database session
        
    Returns:
        The authenticated user or None
    """
    if credentials is None:
        return None
    
    # Extract token
    token = credentials.credentials
    
    # Validate token and get user ID
    user_id = validate_access_token(token)
    if user_id is None:
        return None
    
    # Get user from database
    result = await db.execute(
        select(User).where(User.id == int(user_id))
    )
    user = result.scalar_one_or_none()
    
    if user is None or not user.is_active:
        return None
    
    return user
</file>

<file path="app/core/logging.py">
"""Logging configuration using structlog."""
import logging
import sys
from typing import Any

import structlog
from structlog.types import EventDict, Processor

from app.config import settings


def add_app_context(logger: Any, method_name: str, event_dict: EventDict) -> EventDict:
    """Add application context to log events."""
    event_dict["app"] = settings.APP_NAME
    event_dict["environment"] = settings.ENVIRONMENT
    return event_dict


def setup_logging() -> structlog.BoundLogger:
    """Configure structured logging."""
    
    # Configure standard logging
    logging.basicConfig(
        format="%(message)s",
        stream=sys.stdout,
        level=getattr(logging, settings.LOG_LEVEL.upper()),
    )
    
    # Processors for structlog
    processors: list[Processor] = [
        structlog.contextvars.merge_contextvars,
        add_app_context,
        structlog.processors.add_log_level,
        structlog.processors.TimeStamper(fmt="iso"),
        structlog.processors.StackInfoRenderer(),
    ]
    
    # Add development-friendly formatting
    if settings.is_development:
        processors.extend([
            structlog.dev.set_exc_info,
            structlog.dev.ConsoleRenderer(colors=True),
        ])
    else:
        # Production: JSON formatting
        processors.extend([
            structlog.processors.format_exc_info,
            structlog.processors.JSONRenderer(),
        ])
    
    # Configure structlog
    structlog.configure(
        processors=processors,
        wrapper_class=structlog.make_filtering_bound_logger(
            getattr(logging, settings.LOG_LEVEL.upper())
        ),
        context_class=dict,
        logger_factory=structlog.PrintLoggerFactory(),
        cache_logger_on_first_use=True,
    )
    
    return structlog.get_logger()


def log_api_call(func):
    """Decorator to log API calls with timing information."""
    import functools
    import time
    
    @functools.wraps(func)
    async def wrapper(*args, **kwargs):
        logger = structlog.get_logger()
        start_time = time.time()
        
        # Get function info
        func_name = func.__name__
        
        try:
            result = await func(*args, **kwargs)
            duration = time.time() - start_time
            
            logger.info(
                "api_call_completed",
                function=func_name,
                duration_seconds=round(duration, 3),
                status="success"
            )
            
            return result
            
        except Exception as e:
            duration = time.time() - start_time
            
            logger.error(
                "api_call_failed",
                function=func_name,
                duration_seconds=round(duration, 3),
                error=str(e),
                status="error"
            )
            raise
    
    return wrapper
</file>

<file path="app/core/security.py">
"""Security utilities for authentication and authorization."""
from datetime import datetime, timedelta
from typing import Any, Optional, Union

from jose import JWTError, jwt
from passlib.context import CryptContext

from app.config import settings

# Password hashing
pwd_context = CryptContext(schemes=["bcrypt"], deprecated="auto")


def verify_password(plain_password: str, hashed_password: str) -> bool:
    """Verify a password against a hash.
    
    Args:
        plain_password: The plain text password
        hashed_password: The hashed password to check against
        
    Returns:
        True if password matches, False otherwise
    """
    return pwd_context.verify(plain_password, hashed_password)


def get_password_hash(password: str) -> str:
    """Hash a password for storing.
    
    Args:
        password: The plain text password to hash
        
    Returns:
        The hashed password
    """
    return pwd_context.hash(password)


def create_access_token(
    subject: Union[str, int],
    expires_delta: Optional[timedelta] = None,
    additional_claims: Optional[dict[str, Any]] = None,
) -> str:
    """Create a JWT access token.
    
    Args:
        subject: The subject (usually user ID) for the token
        expires_delta: Optional custom expiration time
        additional_claims: Optional additional claims to include in token
        
    Returns:
        Encoded JWT token string
    """
    if expires_delta:
        expire = datetime.utcnow() + expires_delta
    else:
        expire = datetime.utcnow() + timedelta(
            minutes=settings.ACCESS_TOKEN_EXPIRE_MINUTES
        )
    
    to_encode = {
        "exp": expire,
        "sub": str(subject),
        "type": "access",
    }
    
    if additional_claims:
        to_encode.update(additional_claims)
    
    encoded_jwt = jwt.encode(
        to_encode,
        settings.SECRET_KEY,
        algorithm=settings.ALGORITHM,
    )
    return encoded_jwt


def create_refresh_token(
    subject: Union[str, int],
    expires_delta: Optional[timedelta] = None,
) -> str:
    """Create a JWT refresh token.
    
    Args:
        subject: The subject (usually user ID) for the token
        expires_delta: Optional custom expiration time
        
    Returns:
        Encoded JWT refresh token string
    """
    if expires_delta:
        expire = datetime.utcnow() + expires_delta
    else:
        expire = datetime.utcnow() + timedelta(
            days=settings.REFRESH_TOKEN_EXPIRE_DAYS
        )
    
    to_encode = {
        "exp": expire,
        "sub": str(subject),
        "type": "refresh",
    }
    
    encoded_jwt = jwt.encode(
        to_encode,
        settings.SECRET_KEY,
        algorithm=settings.ALGORITHM,
    )
    return encoded_jwt


def decode_token(token: str) -> Optional[dict[str, Any]]:
    """Decode and validate a JWT token.
    
    Args:
        token: The JWT token to decode
        
    Returns:
        The decoded token payload if valid, None otherwise
    """
    try:
        payload = jwt.decode(
            token,
            settings.SECRET_KEY,
            algorithms=[settings.ALGORITHM],
        )
        return payload
    except JWTError:
        return None


def validate_access_token(token: str) -> Optional[str]:
    """Validate an access token and return the subject.
    
    Args:
        token: The access token to validate
        
    Returns:
        The subject (user ID) if valid, None otherwise
    """
    payload = decode_token(token)
    if payload is None:
        return None
    
    # Check token type
    if payload.get("type") != "access":
        return None
    
    # Get subject
    subject: str = payload.get("sub")
    if subject is None:
        return None
    
    return subject


def validate_refresh_token(token: str) -> Optional[str]:
    """Validate a refresh token and return the subject.
    
    Args:
        token: The refresh token to validate
        
    Returns:
        The subject (user ID) if valid, None otherwise
    """
    payload = decode_token(token)
    if payload is None:
        return None
    
    # Check token type
    if payload.get("type") != "refresh":
        return None
    
    # Get subject
    subject: str = payload.get("sub")
    if subject is None:
        return None
    
    return subject
</file>

<file path="app/db/base.py">
"""Database base configuration and session management."""
from typing import AsyncGenerator
from sqlalchemy.ext.asyncio import AsyncSession, create_async_engine, async_sessionmaker
from sqlalchemy.orm import declarative_base
from sqlalchemy.pool import NullPool

from app.config import settings

# Create base class for models
Base = declarative_base()

# Async database engine (will be initialized in main.py lifespan)
async_engine = None
AsyncSessionLocal = None


def init_db():
    """Initialize database engine and session factory."""
    global async_engine, AsyncSessionLocal
    
    # Convert sync DATABASE_URL to async (postgresql:// -> postgresql+asyncpg://)
    async_url = settings.DATABASE_URL.replace(
        "postgresql://", "postgresql+asyncpg://"
    ).replace(
        "postgres://", "postgresql+asyncpg://"
    )
    
    # Create async engine
    async_engine = create_async_engine(
        async_url,
        echo=settings.is_development,
        pool_pre_ping=True,
        pool_size=20,
        max_overflow=10,
    )
    
    # Create async session factory
    AsyncSessionLocal = async_sessionmaker(
        async_engine,
        class_=AsyncSession,
        expire_on_commit=False,
        autocommit=False,
        autoflush=False,
    )
    
    return async_engine, AsyncSessionLocal


async def get_db() -> AsyncGenerator[AsyncSession, None]:
    """Get async database session dependency."""
    if AsyncSessionLocal is None:
        raise RuntimeError("Database not initialized. Call init_db() first.")
    
    async with AsyncSessionLocal() as session:
        try:
            yield session
        finally:
            await session.close()


async def close_db():
    """Close database connections."""
    global async_engine
    if async_engine:
        await async_engine.dispose()
</file>

<file path="app/models/__init__.py">
"""Database models for CDSA Backend."""
from app.db.base import Base
from app.models.user import User, Role, Permission, Session
from app.models.chat import ChatSession, ChatMessage, ContextWindow, MessageRole
from app.models.tool import (
    Tool,
    ToolExecution,
    ToolApproval,
    ToolCache,
    ToolStatus,
    ExecutionStatus,
    ApprovalStatus,
    ToolCategory,
    ToolExecutionStatus,  # Backward compatibility alias
)
from app.models.audit import AuditLog, AuditAction, SystemMetric
from app.models.secret import Secret, SecretVersion, SecretAccessLog, SecretType
from app.models.document import (
    Document,
    DocumentChunk,
    SearchResult,
    EmbeddingModel,
)
from app.models.notification import (
    Notification,
    NotificationPreference,
    NotificationType,
    NotificationPriority,
)

__all__ = [
    # Base
    "Base",
    # User & Auth
    "User",
    "Role",
    "Permission",
    "Session",
    # Chat
    "ChatSession",
    "ChatMessage",
    "ContextWindow",
    "MessageRole",
    # Tools
    "Tool",
    "ToolExecution",
    "ToolApproval",
    "ToolCache",
    "ToolStatus",
    "ExecutionStatus",
    "ApprovalStatus",
    "ToolCategory",
    "ToolExecutionStatus",  # Backward compatibility
    # Audit
    "AuditLog",
    "AuditAction",
    "SystemMetric",
    # Secrets
    "Secret",
    "SecretVersion",
    "SecretAccessLog",
    "SecretType",
    # Documents & RAG
    "Document",
    "DocumentChunk",
    "SearchResult",
    "EmbeddingModel",
    # Notifications
    "Notification",
    "NotificationPreference",
    "NotificationType",
    "NotificationPriority",
]
</file>

<file path="app/models/audit.py">
"""Audit logging models."""
from datetime import datetime
from typing import Optional

from sqlalchemy import Column, DateTime, ForeignKey, Integer, String, Text, JSON, Enum as SQLEnum
from sqlalchemy.orm import relationship
import enum

from app.db.base import Base


class AuditAction(str, enum.Enum):
    """Audit action types."""
    # Authentication
    LOGIN = "login"
    LOGOUT = "logout"
    LOGIN_FAILED = "login_failed"
    PASSWORD_CHANGE = "password_change"
    PASSWORD_RESET = "password_reset"
    
    # User management
    USER_CREATE = "user_create"
    USER_UPDATE = "user_update"
    USER_DELETE = "user_delete"
    USER_ROLE_ASSIGN = "user_role_assign"
    USER_ROLE_REMOVE = "user_role_remove"
    
    # Tool execution
    TOOL_EXECUTE = "tool_execute"
    TOOL_APPROVE = "tool_approve"
    TOOL_REJECT = "tool_reject"
    TOOL_CANCEL = "tool_cancel"
    
    # Data access
    DATA_READ = "data_read"
    DATA_WRITE = "data_write"
    DATA_DELETE = "data_delete"
    DATA_EXPORT = "data_export"
    
    # Secrets management
    SECRET_CREATE = "secret_create"
    SECRET_READ = "secret_read"
    SECRET_UPDATE = "secret_update"
    SECRET_DELETE = "secret_delete"
    
    # Chat
    CHAT_CREATE = "chat_create"
    CHAT_DELETE = "chat_delete"
    MESSAGE_SEND = "message_send"
    
    # System
    SYSTEM_CONFIG_CHANGE = "system_config_change"
    SYSTEM_ERROR = "system_error"
    API_CALL = "api_call"


class AuditLog(Base):
    """Audit log model for compliance and security tracking."""
    
    __tablename__ = "audit_logs"

    id = Column(Integer, primary_key=True, index=True)
    
    # User context
    user_id = Column(Integer, ForeignKey('users.id', ondelete='SET NULL'), nullable=True, index=True)
    username = Column(String(100), nullable=True, index=True)  # Denormalized for historical record
    
    # Action details
    action = Column(SQLEnum(AuditAction), nullable=False, index=True)
    resource_type = Column(String(50), nullable=True, index=True)  # e.g., 'tool', 'secret', 'user'
    resource_id = Column(String(100), nullable=True, index=True)
    
    # Request context
    ip_address = Column(String(45), nullable=True, index=True)  # IPv6 compatible
    user_agent = Column(String(500), nullable=True)
    session_id = Column(String(255), nullable=True, index=True)
    
    # Action metadata
    details = Column(JSON, nullable=True)  # Additional context
    changes = Column(JSON, nullable=True)  # Before/after for updates
    
    # Result
    success = Column(String(20), nullable=False, default="success", index=True)  # success, failure, error
    error_message = Column(Text, nullable=True)
    
    # Associated tool execution (if applicable)
    tool_execution_id = Column(Integer, ForeignKey('tool_executions.id', ondelete='SET NULL'), nullable=True, index=True)
    
    # Compliance flags
    sensitive_data = Column(String(20), nullable=False, default="false", index=True)  # For GDPR/compliance
    retention_days = Column(Integer, default=2555)  # 7 years default for compliance
    
    # Timestamp
    created_at = Column(DateTime, default=datetime.utcnow, nullable=False, index=True)
    
    # Relationships
    user = relationship("User", back_populates="audit_logs")
    tool_execution = relationship("ToolExecution", back_populates="audit_logs")
    
    def __repr__(self) -> str:
        return f"<AuditLog(id={self.id}, user={self.username}, action={self.action}, success={self.success})>"
    
    @property
    def age_days(self) -> int:
        """Calculate age of log entry in days."""
        return (datetime.utcnow() - self.created_at).days
    
    @property
    def should_be_retained(self) -> bool:
        """Check if log should still be retained."""
        return self.age_days < self.retention_days


class SystemMetric(Base):
    """System metrics for monitoring and performance tracking."""
    
    __tablename__ = "system_metrics"

    id = Column(Integer, primary_key=True, index=True)
    
    # Metric details
    metric_name = Column(String(100), nullable=False, index=True)
    metric_value = Column(String(50), nullable=False)
    metric_unit = Column(String(20), nullable=True)
    
    # Context
    component = Column(String(50), nullable=True, index=True)  # e.g., 'api', 'worker', 'database'
    tags = Column(JSON, nullable=True)  # Additional tags for filtering
    
    # Timestamp
    created_at = Column(DateTime, default=datetime.utcnow, nullable=False, index=True)
    
    def __repr__(self) -> str:
        return f"<SystemMetric(id={self.id}, metric={self.metric_name}, value={self.metric_value})>"
</file>

<file path="app/models/chat.py">
"""Chat and conversation models."""
from datetime import datetime
from typing import Optional

from sqlalchemy import Column, DateTime, ForeignKey, Integer, String, Text, Boolean, JSON, Enum as SQLEnum
from sqlalchemy.orm import relationship
import enum

from app.db.base import Base


class MessageRole(str, enum.Enum):
    """Message role enum."""
    USER = "user"
    ASSISTANT = "assistant"
    SYSTEM = "system"
    TOOL = "tool"


class ChatSession(Base):
    """Chat session model for grouping related messages."""
    
    __tablename__ = "chat_sessions"

    id = Column(Integer, primary_key=True, index=True)
    user_id = Column(Integer, ForeignKey('users.id', ondelete='CASCADE'), nullable=False, index=True)
    title = Column(String(255), nullable=False)
    
    # Session metadata
    context_window_size = Column(Integer, default=4096)
    model = Column(String(100))  # LLM model used
    temperature = Column(String(10), default="0.7")
    
    # Session status
    is_active = Column(Boolean, default=True, nullable=False)
    
    # Timestamps
    created_at = Column(DateTime, default=datetime.utcnow, nullable=False)
    updated_at = Column(DateTime, default=datetime.utcnow, onupdate=datetime.utcnow, nullable=False)
    last_message_at = Column(DateTime, default=datetime.utcnow, nullable=False)
    
    # Relationships
    messages = relationship("ChatMessage", back_populates="session", cascade="all, delete-orphan", order_by="ChatMessage.created_at")
    
    def __repr__(self) -> str:
        return f"<ChatSession(id={self.id}, user_id={self.user_id}, title={self.title})>"


class ChatMessage(Base):
    """Chat message model."""
    
    __tablename__ = "chat_messages"

    id = Column(Integer, primary_key=True, index=True)
    session_id = Column(Integer, ForeignKey('chat_sessions.id', ondelete='CASCADE'), nullable=False, index=True)
    user_id = Column(Integer, ForeignKey('users.id', ondelete='CASCADE'), nullable=False, index=True)
    
    # Message content
    role = Column(SQLEnum(MessageRole), nullable=False, index=True)
    content = Column(Text, nullable=False)
    
    # Message metadata
    tokens = Column(Integer)  # Token count for this message
    model = Column(String(100))  # LLM model that generated this (for assistant messages)
    meta_data = Column(JSON)  # Additional metadata (tool calls, citations, etc.)
    
    # Tool execution reference (if this message triggered a tool)
    tool_execution_id = Column(Integer, ForeignKey('tool_executions.id', ondelete='SET NULL'), nullable=True)
    
    # Timestamps
    created_at = Column(DateTime, default=datetime.utcnow, nullable=False, index=True)
    
    # Relationships
    session = relationship("ChatSession", back_populates="messages")
    user = relationship("User", back_populates="chat_messages")
    tool_execution = relationship("ToolExecution", back_populates="chat_messages", foreign_keys=[tool_execution_id])
    
    def __repr__(self) -> str:
        return f"<ChatMessage(id={self.id}, session_id={self.session_id}, role={self.role})>"


class ContextWindow(Base):
    """Context window tracking for managing token limits."""
    
    __tablename__ = "context_windows"

    id = Column(Integer, primary_key=True, index=True)
    session_id = Column(Integer, ForeignKey('chat_sessions.id', ondelete='CASCADE'), nullable=False, index=True, unique=True)
    
    # Token tracking
    total_tokens = Column(Integer, default=0, nullable=False)
    max_tokens = Column(Integer, default=4096, nullable=False)
    
    # Message tracking
    included_message_ids = Column(JSON)  # List of message IDs currently in context
    
    # Strategy
    strategy = Column(String(50), default="sliding_window")  # sliding_window, summarization, hybrid
    
    # Timestamps
    updated_at = Column(DateTime, default=datetime.utcnow, onupdate=datetime.utcnow, nullable=False)
    
    def __repr__(self) -> str:
        return f"<ContextWindow(id={self.id}, session_id={self.session_id}, tokens={self.total_tokens}/{self.max_tokens})>"
    
    @property
    def usage_percentage(self) -> float:
        """Calculate context window usage percentage."""
        if self.max_tokens == 0:
            return 0.0
        return (self.total_tokens / self.max_tokens) * 100
    
    @property
    def is_near_limit(self) -> bool:
        """Check if context window is near limit (>80%)."""
        return self.usage_percentage > 80
</file>

<file path="app/models/document.py">
"""Document and RAG (Retrieval-Augmented Generation) models."""
from datetime import datetime
from typing import Optional

from sqlalchemy import Column, DateTime, ForeignKey, Integer, String, Text, Boolean, JSON, Float
from sqlalchemy.orm import relationship
from sqlalchemy.dialects.postgresql import ARRAY

from app.db.base import Base

# Try to import pgvector, use Text fallback if not available
try:
    from pgvector.sqlalchemy import Vector
    VECTOR_AVAILABLE = True
except ImportError:
    VECTOR_AVAILABLE = False
    Vector = None


class Document(Base):
    """Document model for RAG system."""
    
    __tablename__ = "documents"

    id = Column(Integer, primary_key=True, index=True)
    
    # Document identification
    title = Column(String(500), nullable=False, index=True)
    source = Column(String(500), nullable=False)  # File path, URL, etc.
    source_type = Column(String(50), nullable=False, index=True)  # file, url, database, api
    
    # Content
    content = Column(Text, nullable=False)
    content_hash = Column(String(64), nullable=False, index=True)  # SHA-256 hash for deduplication
    
    # Document metadata
    meta_data = Column(JSON, nullable=True)  # Custom metadata
    tags = Column(ARRAY(String), nullable=True)  # Document tags
    file_type = Column(String(50), nullable=True)  # pdf, docx, txt, etc.
    file_size = Column(Integer, nullable=True)  # Size in bytes
    
    # Processing status
    is_processed = Column(Boolean, default=False, nullable=False, index=True)
    is_indexed = Column(Boolean, default=False, nullable=False, index=True)
    processing_error = Column(Text, nullable=True)
    
    # Ownership
    uploaded_by = Column(Integer, ForeignKey('users.id', ondelete='SET NULL'), nullable=True, index=True)
    
    # Access control
    is_public = Column(Boolean, default=False, nullable=False)
    required_permission = Column(String(100), nullable=True)
    
    # Timestamps
    created_at = Column(DateTime, default=datetime.utcnow, nullable=False, index=True)
    updated_at = Column(DateTime, default=datetime.utcnow, onupdate=datetime.utcnow, nullable=False)
    last_accessed = Column(DateTime, nullable=True)
    
    # Relationships
    uploader = relationship("User")
    chunks = relationship("DocumentChunk", back_populates="document", cascade="all, delete-orphan")
    
    def __repr__(self) -> str:
        return f"<Document(id={self.id}, title={self.title}, source_type={self.source_type})>"
    
    @property
    def chunk_count(self) -> int:
        """Get number of chunks for this document."""
        return len(self.chunks) if self.chunks else 0


class DocumentChunk(Base):
    """Document chunk model for vector search."""
    
    __tablename__ = "document_chunks"

    id = Column(Integer, primary_key=True, index=True)
    document_id = Column(Integer, ForeignKey('documents.id', ondelete='CASCADE'), nullable=False, index=True)
    
    # Chunk details
    chunk_index = Column(Integer, nullable=False)  # Position in document
    content = Column(Text, nullable=False)
    
    # Vector embedding (for semantic search)
    # Supports various embedding dimensions: 384 (MiniLM), 768 (BERT), 1536 (OpenAI), 3072 (text-embedding-3-large)
    if VECTOR_AVAILABLE:
        embedding = Column(Vector(1536), nullable=True)  # Default to OpenAI embedding size
    else:
        embedding = Column(Text, nullable=True)  # Fallback to TEXT when pgvector not installed
    
    # Chunk metadata
    token_count = Column(Integer, nullable=True)
    char_count = Column(Integer, nullable=True)
    meta_data = Column(JSON, nullable=True)  # Page number, section, etc.
    
    # Search optimization
    search_keywords = Column(ARRAY(String), nullable=True)  # Extracted keywords
    
    # Timestamps
    created_at = Column(DateTime, default=datetime.utcnow, nullable=False)
    
    # Relationships
    document = relationship("Document", back_populates="chunks")
    search_results = relationship("SearchResult", back_populates="chunk", cascade="all, delete-orphan")
    
    def __repr__(self) -> str:
        return f"<DocumentChunk(id={self.id}, document_id={self.document_id}, index={self.chunk_index})>"


class SearchResult(Base):
    """Search result model for tracking RAG queries and results."""
    
    __tablename__ = "search_results"

    id = Column(Integer, primary_key=True, index=True)
    
    # Query details
    query = Column(Text, nullable=False, index=True)
    if VECTOR_AVAILABLE:
        query_embedding = Column(Vector(1536), nullable=True)
    else:
        query_embedding = Column(Text, nullable=True)  # Fallback to TEXT when pgvector not installed
    
    # Result details
    chunk_id = Column(Integer, ForeignKey('document_chunks.id', ondelete='CASCADE'), nullable=False, index=True)
    relevance_score = Column(Float, nullable=False)  # Similarity score
    rank = Column(Integer, nullable=False)  # Rank in search results
    
    # Context
    user_id = Column(Integer, ForeignKey('users.id', ondelete='SET NULL'), nullable=True, index=True)
    session_id = Column(Integer, ForeignKey('chat_sessions.id', ondelete='SET NULL'), nullable=True, index=True)
    
    # Search metadata
    search_type = Column(String(50), default="vector")  # vector, keyword, hybrid
    filters_applied = Column(JSON, nullable=True)
    
    # Feedback
    was_helpful = Column(Boolean, nullable=True)  # User feedback
    
    # Timestamp
    created_at = Column(DateTime, default=datetime.utcnow, nullable=False, index=True)
    
    # Relationships
    chunk = relationship("DocumentChunk", back_populates="search_results")
    user = relationship("User")
    session = relationship("ChatSession")
    
    def __repr__(self) -> str:
        return f"<SearchResult(id={self.id}, chunk_id={self.chunk_id}, score={self.relevance_score})>"


class EmbeddingModel(Base):
    """Embedding model configuration for vector generation."""
    
    __tablename__ = "embedding_models"

    id = Column(Integer, primary_key=True, index=True)
    
    # Model details
    name = Column(String(100), unique=True, nullable=False, index=True)
    display_name = Column(String(255), nullable=False)
    provider = Column(String(50), nullable=False)  # openai, huggingface, local
    model_id = Column(String(255), nullable=False)
    
    # Model configuration
    dimension = Column(Integer, nullable=False)  # Embedding vector size
    max_tokens = Column(Integer, nullable=False)  # Max input tokens
    cost_per_1k_tokens = Column(Float, nullable=True)  # Cost tracking
    
    # Model status
    is_active = Column(Boolean, default=True, nullable=False)
    is_default = Column(Boolean, default=False, nullable=False)
    
    # Performance metadata
    avg_latency_ms = Column(Float, nullable=True)
    
    # Timestamps
    created_at = Column(DateTime, default=datetime.utcnow, nullable=False)
    updated_at = Column(DateTime, default=datetime.utcnow, onupdate=datetime.utcnow, nullable=False)
    
    def __repr__(self) -> str:
        return f"<EmbeddingModel(id={self.id}, name={self.name}, dimension={self.dimension})>"
</file>

<file path="app/models/notification.py">
"""
Notification models for the CDSA application.

Provides models for:
- Notification: Persistent notifications with priority levels
- NotificationPreference: User-specific notification settings
"""
from datetime import datetime
from typing import Optional, Dict, Any
from enum import Enum

from sqlalchemy import (
    Column, Integer, String, Boolean, DateTime, 
    ForeignKey, Text, JSON, Index
)
from sqlalchemy.orm import relationship

from app.db.base import Base


class NotificationType(str, Enum):
    """Enum for notification types."""
    APPROVAL_REQUESTED = "APPROVAL_REQUESTED"
    APPROVAL_DECISION = "APPROVAL_DECISION"
    DOCUMENT_INDEXED = "DOCUMENT_INDEXED"
    DOCUMENT_PROCESSING_FAILED = "DOCUMENT_PROCESSING_FAILED"
    TOOL_ADDED = "TOOL_ADDED"
    TOOL_UPDATED = "TOOL_UPDATED"
    TOOL_REMOVED = "TOOL_REMOVED"
    SECURITY_ALERT = "SECURITY_ALERT"
    VAULT_SECRET_ADDED = "VAULT_SECRET_ADDED"
    VAULT_SECRET_ACCESSED = "VAULT_SECRET_ACCESSED"
    SYSTEM_MAINTENANCE = "SYSTEM_MAINTENANCE"
    LONG_RUNNING_TASK_COMPLETED = "LONG_RUNNING_TASK_COMPLETED"


class NotificationPriority(str, Enum):
    """Enum for notification priority levels."""
    LOW = "low"
    NORMAL = "normal"
    HIGH = "high"
    URGENT = "urgent"


class Notification(Base):
    """
    Model for storing persistent notifications.
    
    Attributes:
        id: Primary key
        user_id: Foreign key to user who receives the notification
        type: Type of notification (from NotificationType enum)
        title: Short notification title
        message: Detailed notification message
        data: Additional structured data (JSON)
        priority: Notification priority level
        is_read: Whether the notification has been read
        read_at: Timestamp when notification was read
        expires_at: Optional expiration timestamp
        created_at: When notification was created
    """
    __tablename__ = "notifications"
    
    id = Column(Integer, primary_key=True, index=True)
    user_id = Column(Integer, ForeignKey("users.id", ondelete="CASCADE"), nullable=False)
    type = Column(String(100), nullable=False)
    title = Column(String(500), nullable=False)
    message = Column(Text, nullable=False)
    data = Column(JSON, default=dict)
    priority = Column(String(20), default="normal", nullable=False)
    is_read = Column(Boolean, default=False, nullable=False)
    read_at = Column(DateTime, nullable=True)
    expires_at = Column(DateTime, nullable=True)
    created_at = Column(DateTime, default=datetime.utcnow, nullable=False)
    
    # Relationships
    user = relationship("User", back_populates="notifications")
    
    # Indexes
    __table_args__ = (
        Index('idx_notifications_user', 'user_id'),
        Index('idx_notifications_unread', 'user_id', 'is_read', 'created_at'),
        Index('idx_notifications_type', 'type'),
        Index('idx_notifications_priority', 'priority', 'created_at'),
    )
    
    def __repr__(self):
        return f"<Notification(id={self.id}, user_id={self.user_id}, type={self.type}, title='{self.title}')>"
    
    def mark_as_read(self) -> None:
        """Mark notification as read with current timestamp."""
        if not self.is_read:
            self.is_read = True
            self.read_at = datetime.utcnow()
    
    def to_dict(self) -> Dict[str, Any]:
        """Convert notification to dictionary for SSE streaming."""
        return {
            "id": self.id,
            "user_id": self.user_id,
            "type": self.type,
            "title": self.title,
            "message": self.message,
            "data": self.data or {},
            "priority": self.priority,
            "is_read": self.is_read,
            "read_at": self.read_at.isoformat() if self.read_at else None,
            "expires_at": self.expires_at.isoformat() if self.expires_at else None,
            "created_at": self.created_at.isoformat()
        }


class NotificationPreference(Base):
    """
    Model for user notification preferences.
    
    Allows users to customize which types of notifications they receive
    and how they are delivered.
    
    Attributes:
        id: Primary key
        user_id: Foreign key to user
        notification_type: Type of notification (from NotificationType enum)
        enabled: Whether this notification type is enabled for the user
        delivery_method: How to deliver (realtime, email, etc.)
        created_at: When preference was created
        updated_at: When preference was last updated
    """
    __tablename__ = "notification_preferences"
    
    id = Column(Integer, primary_key=True, index=True)
    user_id = Column(Integer, ForeignKey("users.id", ondelete="CASCADE"), nullable=False)
    notification_type = Column(String(100), nullable=False)
    enabled = Column(Boolean, default=True, nullable=False)
    delivery_method = Column(String(50), default="realtime", nullable=False)
    created_at = Column(DateTime, default=datetime.utcnow, nullable=False)
    updated_at = Column(DateTime, default=datetime.utcnow, onupdate=datetime.utcnow, nullable=False)
    
    # Relationships
    user = relationship("User", back_populates="notification_preferences")
    
    # Constraints
    __table_args__ = (
        Index('idx_notification_prefs_user_type', 'user_id', 'notification_type', unique=True),
    )
    
    def __repr__(self):
        return f"<NotificationPreference(user_id={self.user_id}, type={self.notification_type}, enabled={self.enabled})>"
</file>

<file path="app/models/secret.py">
"""Secrets vault models for secure credential management."""
from datetime import datetime
from typing import Optional

from sqlalchemy import Column, DateTime, ForeignKey, Integer, String, Text, Boolean, JSON, Enum as SQLEnum
from sqlalchemy.orm import relationship
import enum

from app.db.base import Base


class SecretType(str, enum.Enum):
    """Secret type enum."""
    API_KEY = "api_key"
    PASSWORD = "password"
    TOKEN = "token"
    CERTIFICATE = "certificate"
    SSH_KEY = "ssh_key"
    DATABASE_CREDENTIALS = "database_credentials"
    OTHER = "other"


class Secret(Base):
    """Secrets vault model for storing encrypted credentials."""
    
    __tablename__ = "secrets"

    id = Column(Integer, primary_key=True, index=True)
    
    # Secret identification
    name = Column(String(100), unique=True, nullable=False, index=True)
    display_name = Column(String(255), nullable=False)
    description = Column(Text, nullable=True)
    secret_type = Column(SQLEnum(SecretType), nullable=False, index=True)
    
    # Encrypted data (encrypted at rest)
    encrypted_value = Column(Text, nullable=False)
    encryption_key_id = Column(String(100), nullable=False)  # KMS key ID
    
    # Metadata
    meta_data = Column(JSON, nullable=True)  # Additional secret metadata
    tags = Column(JSON, nullable=True)  # Tags for organization
    
    # Access control
    owner_id = Column(Integer, ForeignKey('users.id', ondelete='CASCADE'), nullable=False, index=True)
    required_permission = Column(String(100))  # Permission required to access
    
    # Secret status
    is_active = Column(Boolean, default=True, nullable=False)
    is_rotatable = Column(Boolean, default=True, nullable=False)
    
    # Rotation settings
    rotation_enabled = Column(Boolean, default=False, nullable=False)
    rotation_days = Column(Integer, nullable=True)  # Auto-rotate every N days
    last_rotated = Column(DateTime, nullable=True)
    next_rotation = Column(DateTime, nullable=True)
    
    # Expiration
    expires_at = Column(DateTime, nullable=True, index=True)
    
    # Timestamps
    created_at = Column(DateTime, default=datetime.utcnow, nullable=False)
    updated_at = Column(DateTime, default=datetime.utcnow, onupdate=datetime.utcnow, nullable=False)
    last_accessed = Column(DateTime, nullable=True)
    
    # Relationships
    owner = relationship("User")
    access_logs = relationship("SecretAccessLog", back_populates="secret", cascade="all, delete-orphan")
    versions = relationship("SecretVersion", back_populates="secret", cascade="all, delete-orphan", order_by="SecretVersion.version_number.desc()")
    
    def __repr__(self) -> str:
        return f"<Secret(id={self.id}, name={self.name}, type={self.secret_type})>"
    
    @property
    def is_expired(self) -> bool:
        """Check if secret has expired."""
        if not self.expires_at:
            return False
        return datetime.utcnow() > self.expires_at
    
    @property
    def needs_rotation(self) -> bool:
        """Check if secret needs rotation."""
        if not self.rotation_enabled or not self.next_rotation:
            return False
        return datetime.utcnow() >= self.next_rotation


class SecretVersion(Base):
    """Secret version history for rotation and rollback."""
    
    __tablename__ = "secret_versions"

    id = Column(Integer, primary_key=True, index=True)
    secret_id = Column(Integer, ForeignKey('secrets.id', ondelete='CASCADE'), nullable=False, index=True)
    
    # Version details
    version_number = Column(Integer, nullable=False)
    encrypted_value = Column(Text, nullable=False)
    encryption_key_id = Column(String(100), nullable=False)
    
    # Version metadata
    created_by = Column(Integer, ForeignKey('users.id', ondelete='SET NULL'), nullable=True)
    rotation_reason = Column(String(255), nullable=True)
    
    # Status
    is_active = Column(Boolean, default=False, nullable=False)
    
    # Timestamp
    created_at = Column(DateTime, default=datetime.utcnow, nullable=False, index=True)
    
    # Relationships
    secret = relationship("Secret", back_populates="versions")
    creator = relationship("User")
    
    def __repr__(self) -> str:
        return f"<SecretVersion(id={self.id}, secret_id={self.secret_id}, version={self.version_number})>"


class SecretAccessLog(Base):
    """Log of secret access for audit compliance."""
    
    __tablename__ = "secret_access_logs"

    id = Column(Integer, primary_key=True, index=True)
    secret_id = Column(Integer, ForeignKey('secrets.id', ondelete='CASCADE'), nullable=False, index=True)
    user_id = Column(Integer, ForeignKey('users.id', ondelete='SET NULL'), nullable=True, index=True)
    
    # Access details
    access_type = Column(String(20), nullable=False, index=True)  # read, write, delete
    success = Column(Boolean, nullable=False, index=True)
    
    # Context
    ip_address = Column(String(45), nullable=True)
    user_agent = Column(String(500), nullable=True)
    tool_id = Column(Integer, ForeignKey('tools.id', ondelete='SET NULL'), nullable=True)  # If accessed by a tool
    
    # Timestamp
    accessed_at = Column(DateTime, default=datetime.utcnow, nullable=False, index=True)
    
    # Relationships
    secret = relationship("Secret", back_populates="access_logs")
    user = relationship("User")
    tool = relationship("Tool")
    
    def __repr__(self) -> str:
        return f"<SecretAccessLog(id={self.id}, secret_id={self.secret_id}, type={self.access_type})>"
</file>

<file path="app/models/tool.py">
"""Tool execution and approval models."""
from datetime import datetime
from typing import Optional

from sqlalchemy import Column, DateTime, ForeignKey, Integer, String, Text, Boolean, JSON, Enum as SQLEnum, Float
from sqlalchemy.orm import relationship
import enum

from app.db.base import Base


class ToolStatus(str, enum.Enum):
    """Tool registration status enum."""
    ACTIVE = "active"
    INACTIVE = "inactive"
    DEPRECATED = "deprecated"


class ExecutionStatus(str, enum.Enum):
    """Tool execution status enum."""
    PENDING = "pending"
    APPROVED = "approved"
    REJECTED = "rejected"
    RUNNING = "running"
    COMPLETED = "completed"
    FAILED = "failed"
    CANCELLED = "cancelled"


class ApprovalStatus(str, enum.Enum):
    """Approval request status enum."""
    PENDING = "pending"
    APPROVED = "approved"
    REJECTED = "rejected"
    EXPIRED = "expired"


class ToolCategory(str, enum.Enum):
    """Tool category enum."""
    DATA_ACCESS = "data_access"
    DATA_ANALYSIS = "data_analysis"
    FILE_OPERATIONS = "file_operations"
    API_CALLS = "api_calls"
    CODE_EXECUTION = "code_execution"
    SYSTEM_COMMANDS = "system_commands"
    DATABASE_OPERATIONS = "database_operations"


# Keep old name for backward compatibility
ToolExecutionStatus = ExecutionStatus


class Tool(Base):
    """Tool definition model."""
    
    __tablename__ = "tools"

    id = Column(Integer, primary_key=True, index=True)
    name = Column(String(200), unique=True, nullable=False, index=True)
    description = Column(Text, nullable=True)
    category = Column(String(50), nullable=True, index=True)
    
    # Tool configuration
    config = Column(JSON, nullable=True)
    input_schema = Column(JSON, nullable=True)
    output_schema = Column(JSON, nullable=True)
    requires_approval = Column(Boolean, default=False, nullable=False)
    is_dangerous = Column(Boolean, default=False, nullable=False)
    
    # Tool status
    status = Column(SQLEnum(ToolStatus), default=ToolStatus.ACTIVE, nullable=False, index=True)
    
    # Execution limits
    timeout_seconds = Column(Integer, default=300)
    max_retries = Column(Integer, default=3)
    
    # Statistics
    execution_count = Column(Integer, default=0, nullable=False)
    success_count = Column(Integer, default=0, nullable=False)
    failure_count = Column(Integer, default=0, nullable=False)
    avg_execution_time = Column(Float, nullable=True)
    last_executed_at = Column(DateTime, nullable=True)
    
    # Ownership
    created_by = Column(Integer, ForeignKey('users.id', ondelete='SET NULL'), nullable=True)
    
    # Timestamps
    created_at = Column(DateTime, default=datetime.utcnow, nullable=False)
    updated_at = Column(DateTime, default=datetime.utcnow, onupdate=datetime.utcnow, nullable=False)
    
    # Relationships
    executions = relationship("ToolExecution", back_populates="tool", cascade="all, delete-orphan")
    
    def __repr__(self) -> str:
        return f"<Tool(id={self.id}, name={self.name}, status={self.status})>"


class ToolExecution(Base):
    """Tool execution model."""
    
    __tablename__ = "tool_executions"

    id = Column(Integer, primary_key=True, index=True)
    tool_id = Column(Integer, ForeignKey('tools.id', ondelete='CASCADE'), nullable=False, index=True)
    user_id = Column(Integer, ForeignKey('users.id', ondelete='CASCADE'), nullable=False, index=True)
    session_id = Column(Integer, ForeignKey('chat_sessions.id', ondelete='CASCADE'), nullable=True, index=True)
    
    # Execution details
    status = Column(SQLEnum(ExecutionStatus), default=ExecutionStatus.PENDING, nullable=False, index=True)
    input_data = Column(JSON, nullable=False)  # Tool input parameters
    output_data = Column(JSON, nullable=True)  # Tool execution result
    error_message = Column(Text, nullable=True)  # Error message if failed
    
    # Execution metadata
    started_at = Column(DateTime, nullable=True)
    completed_at = Column(DateTime, nullable=True)
    execution_time = Column(Float, nullable=True)  # Execution time in seconds
    retry_count = Column(Integer, default=0, nullable=False)
    
    # Approval tracking (if required)
    requires_approval = Column(Boolean, default=False, nullable=False)
    approval_id = Column(Integer, ForeignKey('tool_approvals.id', ondelete='SET NULL'), nullable=True)
    
    # Timestamps
    created_at = Column(DateTime, default=datetime.utcnow, nullable=False, index=True)
    
    # Relationships
    tool = relationship("Tool", back_populates="executions")
    user = relationship("User")
    session = relationship("ChatSession")
    approval = relationship("ToolApproval", back_populates="execution", foreign_keys=[approval_id], uselist=False)
    chat_messages = relationship("ChatMessage", back_populates="tool_execution", foreign_keys="ChatMessage.tool_execution_id")
    audit_logs = relationship("AuditLog", back_populates="tool_execution")
    
    def __repr__(self) -> str:
        return f"<ToolExecution(id={self.id}, tool_id={self.tool_id}, status={self.status})>"
    
    @property
    def is_pending(self) -> bool:
        """Check if execution is pending approval."""
        return self.status == ExecutionStatus.PENDING
    
    @property
    def is_running(self) -> bool:
        """Check if execution is currently running."""
        return self.status == ExecutionStatus.RUNNING
    
    @property
    def is_complete(self) -> bool:
        """Check if execution is complete (success or failure)."""
        return self.status in [ExecutionStatus.COMPLETED, ExecutionStatus.FAILED]


class ToolApproval(Base):
    """Tool execution approval model."""
    
    __tablename__ = "tool_approvals"

    id = Column(Integer, primary_key=True, index=True)
    execution_id = Column(Integer, ForeignKey('tool_executions.id', ondelete='CASCADE'), nullable=False, index=True)
    requested_by = Column(Integer, ForeignKey('users.id', ondelete='CASCADE'), nullable=False, index=True)  # User requesting approval
    
    # Approval details
    status = Column(SQLEnum(ApprovalStatus), default=ApprovalStatus.PENDING, nullable=False, index=True)
    approved_by = Column(Integer, ForeignKey('users.id', ondelete='SET NULL'), nullable=True, index=True)
    reason = Column(Text, nullable=True)  # Reason for approval request
    notes = Column(Text, nullable=True)  # Approver notes
    
    # Timestamps
    requested_at = Column(DateTime, default=datetime.utcnow, nullable=False, index=True)
    responded_at = Column(DateTime, nullable=True)
    expires_at = Column(DateTime, nullable=True)  # Approval request expiration
    
    # Relationships
    execution = relationship("ToolExecution", foreign_keys=[execution_id])
    user = relationship("User", foreign_keys=[requested_by], back_populates="tool_approvals")
    approver = relationship("User", foreign_keys=[approved_by])
    
    def __repr__(self) -> str:
        return f"<ToolApproval(id={self.id}, execution_id={self.execution_id}, status={self.status})>"
    
    @property
    def is_pending(self) -> bool:
        """Check if approval is still pending."""
        return self.status == ApprovalStatus.PENDING
    
    @property
    def is_expired(self) -> bool:
        """Check if approval request has expired."""
        if not self.expires_at:
            return False
        return datetime.utcnow() > self.expires_at and self.status == ApprovalStatus.PENDING


class ToolCache(Base):
    """Tool execution result cache model."""
    
    __tablename__ = "tool_cache"

    id = Column(Integer, primary_key=True, index=True)
    tool_id = Column(Integer, ForeignKey('tools.id', ondelete='CASCADE'), nullable=False, index=True)
    cache_key = Column(String(255), unique=True, nullable=False, index=True)
    input_hash = Column(String(64), nullable=False, index=True)
    
    # Cached data
    output_data = Column(JSON, nullable=False)
    
    # Cache metadata
    hit_count = Column(Integer, default=0, nullable=False)
    last_hit_at = Column(DateTime, nullable=True)
    
    # Cache expiration
    expires_at = Column(DateTime, nullable=True, index=True)
    
    # Timestamps
    created_at = Column(DateTime, default=datetime.utcnow, nullable=False)
    
    def __repr__(self) -> str:
        return f"<ToolCache(id={self.id}, tool_id={self.tool_id}, hits={self.hit_count})>"
    
    @property
    def is_expired(self) -> bool:
        """Check if cache entry has expired."""
        if not self.expires_at:
            return False
        return datetime.utcnow() > self.expires_at
</file>

<file path="app/models/user.py">
"""User authentication models."""
from datetime import datetime
from typing import List, Optional

from sqlalchemy import Boolean, Column, DateTime, Integer, String, Table, ForeignKey
from sqlalchemy.orm import relationship

from app.db.base import Base

# Association tables for many-to-many relationships
user_roles = Table(
    'user_roles',
    Base.metadata,
    Column('user_id', Integer, ForeignKey('users.id', ondelete='CASCADE'), primary_key=True),
    Column('role_id', Integer, ForeignKey('roles.id', ondelete='CASCADE'), primary_key=True),
)

role_permissions = Table(
    'role_permissions',
    Base.metadata,
    Column('role_id', Integer, ForeignKey('roles.id', ondelete='CASCADE'), primary_key=True),
    Column('permission_id', Integer, ForeignKey('permissions.id', ondelete='CASCADE'), primary_key=True),
)


class User(Base):
    """User model for authentication and authorization."""
    
    __tablename__ = "users"

    id = Column(Integer, primary_key=True, index=True)
    email = Column(String(255), unique=True, index=True, nullable=False)
    username = Column(String(100), unique=True, index=True, nullable=False)
    hashed_password = Column(String(255), nullable=False)
    full_name = Column(String(255))
    
    # Account status
    is_active = Column(Boolean, default=True, nullable=False)
    is_superuser = Column(Boolean, default=False, nullable=False)
    is_verified = Column(Boolean, default=False, nullable=False)
    
    # Timestamps
    created_at = Column(DateTime, default=datetime.utcnow, nullable=False)
    updated_at = Column(DateTime, default=datetime.utcnow, onupdate=datetime.utcnow, nullable=False)
    last_login = Column(DateTime, nullable=True)
    
    # Relationships
    roles = relationship("Role", secondary=user_roles, back_populates="users", lazy="select")
    sessions = relationship("Session", back_populates="user", cascade="all, delete-orphan")
    chat_messages = relationship("ChatMessage", back_populates="user", cascade="all, delete-orphan")
    tool_approvals = relationship("ToolApproval", back_populates="user", foreign_keys="ToolApproval.requested_by", cascade="all, delete-orphan")
    audit_logs = relationship("AuditLog", back_populates="user", cascade="all, delete-orphan")
    notifications = relationship("Notification", back_populates="user", cascade="all, delete-orphan")
    notification_preferences = relationship("NotificationPreference", back_populates="user", cascade="all, delete-orphan")
    
    def __repr__(self) -> str:
        return f"<User(id={self.id}, email={self.email}, username={self.username})>"
    
    @property
    def permissions(self) -> List[str]:
        """Get all permissions for this user from their roles."""
        perms = set()
        for role in self.roles:
            for perm in role.permissions:
                perms.add(perm.name)
        return list(perms)
    
    def has_permission(self, permission_name: str) -> bool:
        """Check if user has a specific permission."""
        if self.is_superuser:
            return True
        return permission_name in self.permissions
    
    def has_role(self, role_name: str) -> bool:
        """Check if user has a specific role."""
        if self.is_superuser:
            return True
        return any(role.name == role_name for role in self.roles)


class Role(Base):
    """Role model for RBAC."""
    
    __tablename__ = "roles"

    id = Column(Integer, primary_key=True, index=True)
    name = Column(String(50), unique=True, nullable=False, index=True)
    description = Column(String(255))
    
    # Timestamps
    created_at = Column(DateTime, default=datetime.utcnow, nullable=False)
    updated_at = Column(DateTime, default=datetime.utcnow, onupdate=datetime.utcnow, nullable=False)
    
    # Relationships
    users = relationship("User", secondary=user_roles, back_populates="roles")
    permissions = relationship("Permission", secondary=role_permissions, back_populates="roles", lazy="select")
    
    def __repr__(self) -> str:
        return f"<Role(id={self.id}, name={self.name})>"


class Permission(Base):
    """Permission model for fine-grained access control."""
    
    __tablename__ = "permissions"

    id = Column(Integer, primary_key=True, index=True)
    name = Column(String(100), unique=True, nullable=False, index=True)
    description = Column(String(255))
    resource = Column(String(50), nullable=False, index=True)  # e.g., 'chat', 'tools', 'secrets'
    action = Column(String(50), nullable=False, index=True)     # e.g., 'read', 'write', 'execute', 'approve'
    
    # Timestamps
    created_at = Column(DateTime, default=datetime.utcnow, nullable=False)
    
    # Relationships
    roles = relationship("Role", secondary=role_permissions, back_populates="permissions")
    
    def __repr__(self) -> str:
        return f"<Permission(id={self.id}, name={self.name}, resource={self.resource}, action={self.action})>"


class Session(Base):
    """User session model for tracking active sessions."""
    
    __tablename__ = "sessions"

    id = Column(Integer, primary_key=True, index=True)
    user_id = Column(Integer, ForeignKey('users.id', ondelete='CASCADE'), nullable=False, index=True)
    token = Column(String(1024), unique=True, nullable=False, index=True)
    refresh_token = Column(String(1024), unique=True, nullable=True, index=True)
    
    # Session metadata
    ip_address = Column(String(45))  # IPv6 compatible
    user_agent = Column(String(500))
    
    # Session status
    is_active = Column(Boolean, default=True, nullable=False)
    expires_at = Column(DateTime, nullable=False)
    
    # Timestamps
    created_at = Column(DateTime, default=datetime.utcnow, nullable=False)
    last_activity = Column(DateTime, default=datetime.utcnow, onupdate=datetime.utcnow, nullable=False)
    
    # Relationships
    user = relationship("User", back_populates="sessions")
    
    def __repr__(self) -> str:
        return f"<Session(id={self.id}, user_id={self.user_id}, is_active={self.is_active})>"
    
    @property
    def is_expired(self) -> bool:
        """Check if session is expired."""
        return datetime.utcnow() > self.expires_at
</file>

<file path="app/schemas/audit.py">
"""
Audit logging Pydantic schemas for request/response validation.
"""
from datetime import datetime
from typing import Optional, Dict, Any, List
from pydantic import BaseModel, Field, ConfigDict

from app.models.audit import AuditAction


# Audit Log Schemas
class AuditLogCreate(BaseModel):
    """Schema for creating an audit log entry"""
    action: AuditAction
    resource_type: str = Field(..., max_length=100)
    resource_id: Optional[int] = None
    details: Optional[Dict[str, Any]] = None
    ip_address: Optional[str] = Field(None, max_length=45)
    user_agent: Optional[str] = None


class AuditLogResponse(BaseModel):
    """Schema for audit log response"""
    id: int
    user_id: Optional[int]
    username: Optional[str]
    action: AuditAction
    resource_type: str
    resource_id: Optional[int]
    details: Optional[Dict[str, Any]]
    ip_address: Optional[str]
    user_agent: Optional[str]
    session_id: Optional[int]
    tool_execution_id: Optional[int]
    created_at: datetime
    
    model_config = ConfigDict(from_attributes=True)


class AuditLogListResponse(BaseModel):
    """Schema for paginated audit log list"""
    logs: List[AuditLogResponse]
    total: int
    page: int
    page_size: int
    
    model_config = ConfigDict(from_attributes=True)


class AuditLogFilterParams(BaseModel):
    """Schema for audit log filtering"""
    user_id: Optional[int] = None
    action: Optional[AuditAction] = None
    resource_type: Optional[str] = None
    resource_id: Optional[int] = None
    start_date: Optional[datetime] = None
    end_date: Optional[datetime] = None
    ip_address: Optional[str] = None


# System Metrics Schemas
class SystemMetricCreate(BaseModel):
    """Schema for creating a system metric"""
    metric_name: str = Field(..., max_length=100)
    metric_value: float
    metric_type: str = Field(default="gauge", max_length=50)
    tags: Optional[Dict[str, Any]] = None


class SystemMetricResponse(BaseModel):
    """Schema for system metric response"""
    id: int
    metric_name: str
    metric_value: float
    metric_type: str
    tags: Optional[Dict[str, Any]]
    recorded_at: datetime
    
    model_config = ConfigDict(from_attributes=True)


class SystemMetricListResponse(BaseModel):
    """Schema for paginated system metrics list"""
    metrics: List[SystemMetricResponse]
    total: int
    page: int
    page_size: int
    
    model_config = ConfigDict(from_attributes=True)


# Statistics Schemas
class AuditStatistics(BaseModel):
    """Schema for audit statistics"""
    total_events: int
    events_by_action: Dict[str, int]
    events_by_resource: Dict[str, int]
    unique_users: int
    unique_ips: int
    events_last_24h: int
    events_last_7d: int
    events_last_30d: int
    top_users: List[Dict[str, Any]]
    recent_failures: int


class SystemHealthMetrics(BaseModel):
    """Schema for system health metrics"""
    cpu_usage: Optional[float] = None
    memory_usage: Optional[float] = None
    disk_usage: Optional[float] = None
    active_sessions: int
    active_executions: int
    cache_hit_rate: Optional[float] = None
    avg_response_time: Optional[float] = None
    error_rate: Optional[float] = None
    uptime_seconds: int
</file>

<file path="app/schemas/auth.py">
"""Authentication Pydantic schemas."""
from datetime import datetime
from typing import Optional

from pydantic import BaseModel, EmailStr, Field


class UserBase(BaseModel):
    """Base user schema."""
    email: EmailStr
    username: str = Field(..., min_length=3, max_length=100)
    full_name: Optional[str] = None


class UserCreate(UserBase):
    """Schema for creating a new user."""
    password: str = Field(..., min_length=8, max_length=100)


class UserUpdate(BaseModel):
    """Schema for updating user information."""
    email: Optional[EmailStr] = None
    username: Optional[str] = Field(None, min_length=3, max_length=100)
    full_name: Optional[str] = None
    password: Optional[str] = Field(None, min_length=8, max_length=100)


class UserResponse(UserBase):
    """Schema for user response."""
    id: int
    is_active: bool
    is_superuser: bool
    is_verified: bool
    created_at: datetime
    last_login: Optional[datetime] = None
    
    class Config:
        from_attributes = True


class UserWithRoles(UserResponse):
    """Schema for user with roles."""
    roles: list[str] = []
    permissions: list[str] = []


class LoginRequest(BaseModel):
    """Schema for login request."""
    username: str = Field(..., description="Username or email")
    password: str


class TokenResponse(BaseModel):
    """Schema for token response."""
    access_token: str
    refresh_token: str
    token_type: str = "bearer"
    expires_in: int = Field(..., description="Token expiration time in seconds")


class RefreshTokenRequest(BaseModel):
    """Schema for refresh token request."""
    refresh_token: str


class PasswordResetRequest(BaseModel):
    """Schema for password reset request."""
    email: EmailStr


class PasswordResetConfirm(BaseModel):
    """Schema for password reset confirmation."""
    token: str
    new_password: str = Field(..., min_length=8, max_length=100)


class PasswordChange(BaseModel):
    """Schema for password change."""
    current_password: str
    new_password: str = Field(..., min_length=8, max_length=100)


class RoleBase(BaseModel):
    """Base role schema."""
    name: str = Field(..., min_length=1, max_length=50)
    description: Optional[str] = None


class RoleCreate(RoleBase):
    """Schema for creating a role."""
    pass


class RoleResponse(RoleBase):
    """Schema for role response."""
    id: int
    created_at: datetime
    
    class Config:
        from_attributes = True


class PermissionBase(BaseModel):
    """Base permission schema."""
    name: str = Field(..., min_length=1, max_length=100)
    description: Optional[str] = None
    resource: str = Field(..., min_length=1, max_length=50)
    action: str = Field(..., min_length=1, max_length=50)


class PermissionCreate(PermissionBase):
    """Schema for creating a permission."""
    pass


class PermissionResponse(PermissionBase):
    """Schema for permission response."""
    id: int
    created_at: datetime
    
    class Config:
        from_attributes = True


class AssignRoleRequest(BaseModel):
    """Schema for assigning role to user."""
    user_id: int
    role_id: int


class AssignPermissionRequest(BaseModel):
    """Schema for assigning permission to role."""
    role_id: int
    permission_id: int
</file>

<file path="app/schemas/chat.py">
"""Chat Pydantic schemas."""
from datetime import datetime
from typing import Optional, List

from pydantic import BaseModel, Field

from app.models.chat import MessageRole


class ChatSessionCreate(BaseModel):
    """Schema for creating a chat session."""
    title: str = Field(..., min_length=1, max_length=500)
    model: Optional[str] = Field(None, max_length=100)
    temperature: Optional[str] = "0.7"
    context_window_size: Optional[int] = 4096


class ChatSessionUpdate(BaseModel):
    """Schema for updating a chat session."""
    title: Optional[str] = Field(None, min_length=1, max_length=500)
    model: Optional[str] = Field(None, max_length=100)
    temperature: Optional[str] = None
    is_active: Optional[bool] = None


class ChatSessionResponse(BaseModel):
    """Schema for chat session response."""
    id: int
    user_id: int
    title: str
    model: Optional[str]
    temperature: str
    context_window_size: int
    is_active: bool
    created_at: datetime
    updated_at: datetime
    last_message_at: datetime
    message_count: int = 0
    
    class Config:
        from_attributes = True


class ChatMessageCreate(BaseModel):
    """Schema for creating a chat message."""
    content: str = Field(..., min_length=1)
    role: MessageRole = MessageRole.USER


class ChatMessageResponse(BaseModel):
    """Schema for chat message response."""
    id: int
    session_id: int
    user_id: int
    role: MessageRole
    content: str
    tokens: Optional[int]
    model: Optional[str]
    meta_data: Optional[dict]
    tool_execution_id: Optional[int]
    created_at: datetime
    
    class Config:
        from_attributes = True


class ChatStreamRequest(BaseModel):
    """Schema for chat streaming request."""
    session_id: int
    message: str = Field(..., min_length=1)
    model: Optional[str] = None
    temperature: Optional[float] = Field(None, ge=0.0, le=2.0)
    max_tokens: Optional[int] = Field(None, gt=0, le=8192)
    tools_enabled: bool = True


class ChatStreamChunk(BaseModel):
    """Schema for chat stream chunk (SSE event)."""
    type: str  # "message", "tool_call", "tool_result", "error", "done"
    content: Optional[str] = None
    tool_name: Optional[str] = None
    tool_args: Optional[dict] = None
    tool_result: Optional[dict] = None
    error: Optional[str] = None
    message_id: Optional[int] = None
    tokens: Optional[int] = None


class ContextWindowResponse(BaseModel):
    """Schema for context window response."""
    id: int
    session_id: int
    total_tokens: int
    max_tokens: int
    usage_percentage: float
    is_near_limit: bool
    included_message_ids: List[int]
    strategy: str
    updated_at: datetime
    
    class Config:
        from_attributes = True


class ChatHistoryResponse(BaseModel):
    """Schema for chat history response."""
    session: ChatSessionResponse
    messages: List[ChatMessageResponse]
    context_window: Optional[ContextWindowResponse]
</file>

<file path="app/schemas/document.py">
"""
Document and RAG Pydantic schemas for request/response validation.
"""
from datetime import datetime
from typing import Optional, Dict, Any, List
from pydantic import BaseModel, Field, ConfigDict


# Document Schemas
class DocumentCreate(BaseModel):
    """Schema for creating a document"""
    title: str = Field(..., min_length=1, max_length=500)
    source_type: str = Field(..., description="file, url, database, api")
    content: Optional[str] = Field(None, description="Text content if provided directly")
    tags: Optional[List[str]] = None
    meta_data: Optional[Dict[str, Any]] = None
    is_public: bool = False
    required_permission: Optional[str] = None


class DocumentUpdate(BaseModel):
    """Schema for updating a document"""
    title: Optional[str] = Field(None, min_length=1, max_length=500)
    tags: Optional[List[str]] = None
    meta_data: Optional[Dict[str, Any]] = None
    is_public: Optional[bool] = None
    required_permission: Optional[str] = None


class DocumentResponse(BaseModel):
    """Schema for document response"""
    id: int
    title: str
    source: str
    source_type: str
    file_type: Optional[str]
    file_size: Optional[int]
    tags: Optional[List[str]]
    meta_data: Optional[Dict[str, Any]]
    is_processed: bool
    is_indexed: bool
    processing_error: Optional[str]
    uploaded_by: Optional[int]
    is_public: bool
    required_permission: Optional[str]
    chunk_count: int
    created_at: datetime
    updated_at: datetime
    last_accessed: Optional[datetime]
    
    model_config = ConfigDict(from_attributes=True)


class DocumentListResponse(BaseModel):
    """Schema for paginated document list"""
    documents: List[DocumentResponse]
    total: int
    page: int
    page_size: int
    
    model_config = ConfigDict(from_attributes=True)


class DocumentContentResponse(BaseModel):
    """Schema for document with content"""
    id: int
    title: str
    content: str
    source_type: str
    file_type: Optional[str]
    meta_data: Optional[Dict[str, Any]]
    created_at: datetime
    
    model_config = ConfigDict(from_attributes=True)


# Document Chunk Schemas
class DocumentChunkResponse(BaseModel):
    """Schema for document chunk response"""
    id: int
    document_id: int
    chunk_index: int
    content: str
    token_count: Optional[int]
    char_count: Optional[int]
    meta_data: Optional[Dict[str, Any]]
    search_keywords: Optional[List[str]]
    created_at: datetime
    
    model_config = ConfigDict(from_attributes=True)


class DocumentChunkListResponse(BaseModel):
    """Schema for document chunks list"""
    chunks: List[DocumentChunkResponse]
    total: int
    document_id: int
    
    model_config = ConfigDict(from_attributes=True)


# Indexing Schemas
class DocumentIndexRequest(BaseModel):
    """Schema for document indexing request"""
    use_docling: bool = Field(default=True, description="Use Docling for enhanced processing")
    extract_tables: bool = Field(default=True, description="Extract table structures")
    extract_images: bool = Field(default=True, description="Extract and caption images")
    chunk_size: Optional[int] = Field(None, ge=100, le=5000, description="Custom chunk size")
    chunk_overlap: Optional[int] = Field(None, ge=0, le=1000, description="Custom chunk overlap")
    embedding_model: Optional[str] = Field(None, description="Specific embedding model to use")


class DocumentIndexResponse(BaseModel):
    """Schema for indexing result"""
    document_id: int
    chunks_created: int
    tables_extracted: int
    images_extracted: int
    total_tokens: int
    processing_time_seconds: float
    success: bool
    error: Optional[str] = None


# Search Schemas
class DocumentSearchRequest(BaseModel):
    """Schema for document search request"""
    query: str = Field(..., min_length=1, max_length=1000)
    search_type: str = Field(default="vector", description="vector, keyword, or hybrid")
    top_k: int = Field(default=5, ge=1, le=50, description="Number of results")
    filters: Optional[Dict[str, Any]] = Field(None, description="Metadata filters")
    min_score: Optional[float] = Field(None, ge=0.0, le=1.0, description="Minimum relevance score")


class SearchResultResponse(BaseModel):
    """Schema for search result"""
    chunk_id: int
    document_id: int
    document_title: str
    content: str
    relevance_score: float
    rank: int
    meta_data: Optional[Dict[str, Any]]
    search_type: str
    
    model_config = ConfigDict(from_attributes=True)


class DocumentSearchResponse(BaseModel):
    """Schema for search results"""
    query: str
    results: List[SearchResultResponse]
    total_results: int
    search_type: str
    processing_time_ms: float


# RAG Query Schemas
class RAGQueryRequest(BaseModel):
    """Schema for RAG query request"""
    query: str = Field(..., min_length=1, max_length=2000)
    top_k: int = Field(default=5, ge=1, le=20, description="Number of context chunks")
    session_id: Optional[int] = Field(None, description="Chat session for context")
    filters: Optional[Dict[str, Any]] = None
    include_metadata: bool = Field(default=True, description="Include chunk metadata")


class RAGContextChunk(BaseModel):
    """Schema for RAG context chunk"""
    content: str
    document_title: str
    relevance_score: float
    meta_data: Optional[Dict[str, Any]]


class RAGQueryResponse(BaseModel):
    """Schema for RAG query response"""
    query: str
    context_chunks: List[RAGContextChunk]
    total_tokens: int
    sources: List[str]
    processing_time_ms: float


# Embedding Model Schemas
class EmbeddingModelResponse(BaseModel):
    """Schema for embedding model"""
    id: int
    name: str
    display_name: str
    provider: str
    model_id: str
    dimension: int
    max_tokens: int
    cost_per_1k_tokens: Optional[float]
    is_active: bool
    is_default: bool
    avg_latency_ms: Optional[float]
    created_at: datetime
    
    model_config = ConfigDict(from_attributes=True)


class EmbeddingModelListResponse(BaseModel):
    """Schema for embedding models list"""
    models: List[EmbeddingModelResponse]
    total: int


# Document Statistics
class DocumentStatistics(BaseModel):
    """Schema for document statistics"""
    total_documents: int
    processed_documents: int
    indexed_documents: int
    total_chunks: int
    total_searches: int
    documents_by_type: Dict[str, int]
    avg_chunks_per_document: float
    storage_size_mb: float
    recent_uploads: int


# Table Extraction Schemas
class TableExtractionResponse(BaseModel):
    """Schema for extracted table"""
    page_number: int
    table_index: int
    headers: List[str]
    rows: List[List[str]]
    markdown: str
    meta_data: Optional[Dict[str, Any]]


class DocumentTablesResponse(BaseModel):
    """Schema for all tables in document"""
    document_id: int
    document_title: str
    tables: List[TableExtractionResponse]
    total_tables: int


# Image Extraction Schemas
class ImageExtractionResponse(BaseModel):
    """Schema for extracted image"""
    page_number: int
    image_index: int
    caption: Optional[str]
    alt_text: Optional[str]
    image_type: str  # figure, chart, diagram, photo
    meta_data: Optional[Dict[str, Any]]


class DocumentImagesResponse(BaseModel):
    """Schema for all images in document"""
    document_id: int
    document_title: str
    images: List[ImageExtractionResponse]
    total_images: int
</file>

<file path="app/schemas/notification.py">
"""
Notification schemas for request/response validation.
"""
from datetime import datetime
from typing import Optional, Dict, Any, List
from pydantic import BaseModel, Field, ConfigDict

from app.models.notification import NotificationType, NotificationPriority


class NotificationBase(BaseModel):
    """Base notification schema."""
    type: NotificationType
    title: str = Field(..., min_length=1, max_length=500)
    message: str = Field(..., min_length=1)
    data: Optional[Dict[str, Any]] = Field(default_factory=dict)
    priority: NotificationPriority = NotificationPriority.NORMAL


class NotificationCreate(NotificationBase):
    """Schema for creating a notification."""
    user_id: int
    expires_at: Optional[datetime] = None


class NotificationResponse(NotificationBase):
    """Schema for notification response."""
    id: int
    user_id: int
    is_read: bool
    read_at: Optional[datetime]
    expires_at: Optional[datetime]
    created_at: datetime
    
    model_config = ConfigDict(from_attributes=True)


class NotificationUpdate(BaseModel):
    """Schema for updating a notification."""
    is_read: Optional[bool] = None


class NotificationListResponse(BaseModel):
    """Schema for paginated notification list."""
    notifications: List[NotificationResponse]
    total: int
    unread_count: int
    page: int
    page_size: int
    
    model_config = ConfigDict(from_attributes=True)


class NotificationPreferenceBase(BaseModel):
    """Base notification preference schema."""
    notification_type: NotificationType
    enabled: bool = True
    delivery_method: str = Field(default="realtime", max_length=50)


class NotificationPreferenceCreate(NotificationPreferenceBase):
    """Schema for creating a notification preference."""
    pass


class NotificationPreferenceUpdate(BaseModel):
    """Schema for updating a notification preference."""
    enabled: Optional[bool] = None
    delivery_method: Optional[str] = Field(None, max_length=50)


class NotificationPreferenceResponse(NotificationPreferenceBase):
    """Schema for notification preference response."""
    id: int
    user_id: int
    created_at: datetime
    updated_at: datetime
    
    model_config = ConfigDict(from_attributes=True)


class NotificationPreferenceListResponse(BaseModel):
    """Schema for notification preferences list."""
    preferences: List[NotificationPreferenceResponse]
    total: int
    
    model_config = ConfigDict(from_attributes=True)


class BroadcastNotificationRequest(BaseModel):
    """Schema for broadcast notification request."""
    type: NotificationType
    title: str = Field(..., min_length=1, max_length=500)
    message: str = Field(..., min_length=1)
    target_roles: Optional[List[str]] = None
    data: Optional[Dict[str, Any]] = Field(default_factory=dict)
    priority: NotificationPriority = NotificationPriority.NORMAL
    persist: bool = True


class NotificationStatsResponse(BaseModel):
    """Schema for notification statistics."""
    total_notifications: int
    unread_count: int
    by_type: Dict[str, int]
    by_priority: Dict[str, int]
    recent_count: int  # Last 24 hours
</file>

<file path="app/schemas/secret.py">
"""
Secrets vault Pydantic schemas for request/response validation.
"""
from datetime import datetime
from typing import Optional, Dict, Any, List
from pydantic import BaseModel, Field, ConfigDict

from app.models.secret import SecretType


# Secret Schemas
class SecretCreate(BaseModel):
    """Schema for creating a secret"""
    name: str = Field(..., min_length=1, max_length=200)
    description: Optional[str] = None
    secret_type: SecretType
    value: str = Field(..., description="Secret value (will be encrypted)")
    tags: Optional[Dict[str, Any]] = None
    expires_at: Optional[datetime] = None


class SecretUpdate(BaseModel):
    """Schema for updating a secret"""
    name: Optional[str] = Field(None, min_length=1, max_length=200)
    description: Optional[str] = None
    value: Optional[str] = Field(None, description="New secret value")
    tags: Optional[Dict[str, Any]] = None
    expires_at: Optional[datetime] = None
    is_active: Optional[bool] = None


class SecretResponse(BaseModel):
    """Schema for secret response (without value)"""
    id: int
    name: str
    description: Optional[str]
    secret_type: SecretType
    tags: Optional[Dict[str, Any]]
    is_active: bool
    is_expired: bool
    version: int
    expires_at: Optional[datetime]
    last_accessed_at: Optional[datetime]
    last_rotated_at: Optional[datetime]
    created_by: int
    created_at: datetime
    updated_at: datetime
    
    model_config = ConfigDict(from_attributes=True)


class SecretValueResponse(BaseModel):
    """Schema for secret response with decrypted value"""
    id: int
    name: str
    description: Optional[str]
    secret_type: SecretType
    value: str  # Decrypted value
    tags: Optional[Dict[str, Any]]
    is_active: bool
    version: int
    expires_at: Optional[datetime]
    created_at: datetime
    
    model_config = ConfigDict(from_attributes=True)


class SecretListResponse(BaseModel):
    """Schema for paginated secret list"""
    secrets: List[SecretResponse]
    total: int
    page: int
    page_size: int
    
    model_config = ConfigDict(from_attributes=True)


# Secret Version Schemas
class SecretVersionResponse(BaseModel):
    """Schema for secret version response"""
    id: int
    secret_id: int
    version: int
    value_hash: str
    created_by: int
    created_at: datetime
    
    model_config = ConfigDict(from_attributes=True)


class SecretVersionListResponse(BaseModel):
    """Schema for secret version list"""
    versions: List[SecretVersionResponse]
    total: int
    
    model_config = ConfigDict(from_attributes=True)


# Secret Access Log Schemas
class SecretAccessLogResponse(BaseModel):
    """Schema for secret access log response"""
    id: int
    secret_id: int
    secret_name: str
    user_id: int
    username: str
    access_type: str
    ip_address: Optional[str]
    user_agent: Optional[str]
    accessed_at: datetime
    
    model_config = ConfigDict(from_attributes=True)


class SecretAccessLogListResponse(BaseModel):
    """Schema for secret access log list"""
    logs: List[SecretAccessLogResponse]
    total: int
    page: int
    page_size: int
    
    model_config = ConfigDict(from_attributes=True)


# Secret Rotation Schema
class SecretRotateRequest(BaseModel):
    """Schema for rotating a secret"""
    new_value: str = Field(..., description="New secret value")
    reason: Optional[str] = Field(None, description="Reason for rotation")


# Secret Statistics
class SecretStatistics(BaseModel):
    """Schema for secret statistics"""
    total_secrets: int
    active_secrets: int
    expired_secrets: int
    secrets_by_type: Dict[str, int]
    recent_accesses: int
    secrets_expiring_soon: int
    never_accessed: int
    most_accessed: List[Dict[str, Any]]
</file>

<file path="app/schemas/tool.py">
"""
Tool-related Pydantic schemas for request/response validation.
"""
from datetime import datetime
from typing import Optional, Dict, Any, List
from pydantic import BaseModel, Field, ConfigDict
from enum import Enum


# Enums matching the database models
class ToolStatus(str, Enum):
    """Tool registration status"""
    ACTIVE = "active"
    INACTIVE = "inactive"
    DEPRECATED = "deprecated"


class ExecutionStatus(str, Enum):
    """Tool execution status"""
    PENDING = "pending"
    APPROVED = "approved"
    REJECTED = "rejected"
    RUNNING = "running"
    COMPLETED = "completed"
    FAILED = "failed"
    CANCELLED = "cancelled"


class ApprovalStatus(str, Enum):
    """Approval request status"""
    PENDING = "pending"
    APPROVED = "approved"
    REJECTED = "rejected"
    EXPIRED = "expired"


# Tool Schemas
class ToolBase(BaseModel):
    """Base tool schema"""
    name: str = Field(..., min_length=1, max_length=200)
    description: Optional[str] = None
    category: Optional[str] = None
    requires_approval: bool = False
    is_dangerous: bool = False
    timeout_seconds: Optional[int] = Field(default=300, ge=1, le=3600)
    max_retries: int = Field(default=3, ge=0, le=10)
    config: Optional[Dict[str, Any]] = None
    input_schema: Optional[Dict[str, Any]] = None
    output_schema: Optional[Dict[str, Any]] = None


class ToolCreate(ToolBase):
    """Schema for creating a tool"""
    pass


class ToolUpdate(BaseModel):
    """Schema for updating a tool"""
    name: Optional[str] = Field(None, min_length=1, max_length=200)
    description: Optional[str] = None
    category: Optional[str] = None
    requires_approval: Optional[bool] = None
    is_dangerous: Optional[bool] = None
    timeout_seconds: Optional[int] = Field(None, ge=1, le=3600)
    max_retries: Optional[int] = Field(None, ge=0, le=10)
    status: Optional[ToolStatus] = None
    config: Optional[Dict[str, Any]] = None
    input_schema: Optional[Dict[str, Any]] = None
    output_schema: Optional[Dict[str, Any]] = None


class ToolResponse(ToolBase):
    """Schema for tool response"""
    id: int
    status: ToolStatus
    execution_count: int
    success_count: int
    failure_count: int
    avg_execution_time: Optional[float]
    last_executed_at: Optional[datetime]
    created_at: datetime
    updated_at: datetime
    
    model_config = ConfigDict(from_attributes=True)


class ToolListResponse(BaseModel):
    """Schema for paginated tool list"""
    tools: List[ToolResponse]
    total: int
    page: int
    page_size: int
    
    model_config = ConfigDict(from_attributes=True)


# Tool Execution Schemas
class ToolExecutionCreate(BaseModel):
    """Schema for creating a tool execution"""
    tool_id: int
    session_id: Optional[int] = None
    input_data: Dict[str, Any]
    approval_reason: Optional[str] = None


class ToolExecutionUpdate(BaseModel):
    """Schema for updating a tool execution"""
    status: Optional[ExecutionStatus] = None
    output_data: Optional[Dict[str, Any]] = None
    error_message: Optional[str] = None


class ToolExecutionResponse(BaseModel):
    """Schema for tool execution response"""
    id: int
    tool_id: int
    tool_name: str
    user_id: int
    session_id: Optional[int]
    status: ExecutionStatus
    input_data: Dict[str, Any]
    output_data: Optional[Dict[str, Any]]
    error_message: Optional[str]
    started_at: Optional[datetime]
    completed_at: Optional[datetime]
    execution_time: Optional[float]
    retry_count: int
    requires_approval: bool
    approval_id: Optional[int]
    created_at: datetime
    
    model_config = ConfigDict(from_attributes=True)


class ToolExecutionListResponse(BaseModel):
    """Schema for paginated tool execution list"""
    executions: List[ToolExecutionResponse]
    total: int
    page: int
    page_size: int
    
    model_config = ConfigDict(from_attributes=True)


# Tool Approval Schemas
class ToolApprovalCreate(BaseModel):
    """Schema for creating a tool approval request"""
    execution_id: int
    reason: Optional[str] = None


class ToolApprovalUpdate(BaseModel):
    """Schema for updating a tool approval"""
    status: ApprovalStatus
    notes: Optional[str] = None


class ToolApprovalResponse(BaseModel):
    """Schema for tool approval response"""
    id: int
    execution_id: int
    tool_name: str
    requester_id: int
    requester_name: str
    approver_id: Optional[int]
    approver_name: Optional[str]
    status: ApprovalStatus
    reason: Optional[str]
    notes: Optional[str]
    requested_at: datetime
    responded_at: Optional[datetime]
    expires_at: Optional[datetime]
    
    model_config = ConfigDict(from_attributes=True)


class ToolApprovalListResponse(BaseModel):
    """Schema for paginated tool approval list"""
    approvals: List[ToolApprovalResponse]
    total: int
    page: int
    page_size: int
    
    model_config = ConfigDict(from_attributes=True)


# Tool Cache Schemas
class ToolCacheResponse(BaseModel):
    """Schema for tool cache entry response"""
    id: int
    tool_id: int
    tool_name: str
    cache_key: str
    input_hash: str
    output_data: Dict[str, Any]
    hit_count: int
    last_hit_at: Optional[datetime]
    expires_at: Optional[datetime]
    created_at: datetime
    
    model_config = ConfigDict(from_attributes=True)


# Tool Statistics Schemas
class ToolStatistics(BaseModel):
    """Schema for tool usage statistics"""
    tool_id: int
    tool_name: str
    total_executions: int
    successful_executions: int
    failed_executions: int
    avg_execution_time: float
    success_rate: float
    last_executed: Optional[datetime]


class SystemToolStatistics(BaseModel):
    """Schema for system-wide tool statistics"""
    total_tools: int
    active_tools: int
    total_executions: int
    pending_approvals: int
    execution_success_rate: float
    avg_execution_time: float
    top_tools: List[ToolStatistics]
</file>

<file path="app/services/chunking_service.py">
"""
Document chunking service for token-optimized splitting.
Preserves document structure and hierarchy for better context in RAG.
"""
import logging
from typing import List, Dict, Any, Optional
from datetime import datetime

logger = logging.getLogger(__name__)


class ChunkingService:
    """
    Advanced chunking service for document splitting.
    
    Features:
    - Token-based chunking optimized for LLM context windows
    - Preserves document hierarchy (headings, sections)
    - Configurable chunk size and overlap
    - Rich metadata preservation (page numbers, bounding boxes)
    - Smart boundary detection (avoids splitting mid-sentence)
    - Support for tables and figures as separate chunks
    """
    
    def __init__(
        self,
        chunk_size: int = 1000,
        chunk_overlap: int = 200,
        max_chunk_size: int = 2000
    ):
        """
        Initialize chunking service.
        
        Args:
            chunk_size: Target chunk size in characters
            chunk_overlap: Overlap between chunks in characters
            max_chunk_size: Maximum chunk size in characters
        """
        self.chunk_size = chunk_size
        self.chunk_overlap = chunk_overlap
        self.max_chunk_size = max_chunk_size
        
        logger.info(
            f"ChunkingService initialized - Size: {chunk_size}, "
            f"Overlap: {chunk_overlap}, Max: {max_chunk_size}"
        )
    
    def chunk_document(
        self,
        text: str,
        doc_structure: Optional[Dict[str, Any]] = None,
        tables: Optional[List[Dict[str, Any]]] = None,
        images: Optional[List[Dict[str, Any]]] = None,
        metadata: Optional[Dict[str, Any]] = None
    ) -> List[Dict[str, Any]]:
        """
        Chunk a document into optimized segments for embedding.
        
        Args:
            text: Full document text (markdown format)
            doc_structure: Document structure from processor
            tables: Extracted tables
            images: Extracted images
            metadata: Document metadata
            
        Returns:
            List of chunks with metadata
        """
        chunks = []
        
        try:
            # Create basic text chunks
            text_chunks = self._chunk_text(text)
            
            # Add text chunks
            for idx, chunk_text in enumerate(text_chunks):
                chunk_meta = {
                    "chunk_type": "text",
                    "chunk_index": len(chunks),
                    "position_in_document": idx / len(text_chunks),
                    "char_count": len(chunk_text),
                    "token_count": self._estimate_tokens(chunk_text)
                }
                
                # Add structural metadata if available
                if doc_structure:
                    chunk_meta.update(self._get_structural_metadata(idx, doc_structure))
                
                # Add document metadata
                if metadata:
                    chunk_meta["document_metadata"] = metadata
                
                chunks.append({
                    "content": chunk_text,
                    "meta_data": chunk_meta,
                    "search_keywords": self._extract_keywords(chunk_text)
                })
            
            # Add table chunks
            if tables:
                for table in tables:
                    chunk_meta = {
                        "chunk_type": "table",
                        "chunk_index": len(chunks),
                        "page_number": table.get("page_number"),
                        "table_index": table.get("table_index"),
                        "row_count": table.get("row_count"),
                        "col_count": table.get("col_count"),
                        "char_count": len(table.get("markdown", "")),
                        "token_count": self._estimate_tokens(table.get("markdown", ""))
                    }
                    
                    # Create table content
                    table_content = self._format_table_content(table)
                    
                    chunks.append({
                        "content": table_content,
                        "meta_data": chunk_meta,
                        "search_keywords": self._extract_keywords(table_content)
                    })
            
            # Add image chunks
            if images:
                for image in images:
                    if image.get("caption") or image.get("alt_text"):
                        chunk_meta = {
                            "chunk_type": "image",
                            "chunk_index": len(chunks),
                            "page_number": image.get("page_number"),
                            "image_index": image.get("image_index"),
                            "image_type": image.get("image_type"),
                            "char_count": len(image.get("caption", "") + image.get("alt_text", "")),
                            "token_count": self._estimate_tokens(
                                image.get("caption", "") + image.get("alt_text", "")
                            )
                        }
                        
                        # Create image content
                        image_content = self._format_image_content(image)
                        
                        chunks.append({
                            "content": image_content,
                            "meta_data": chunk_meta,
                            "search_keywords": self._extract_keywords(image_content)
                        })
            
            logger.info(
                f"Created {len(chunks)} chunks - "
                f"Text: {len(text_chunks)}, Tables: {len(tables) if tables else 0}, "
                f"Images: {len([c for c in chunks if c['meta_data']['chunk_type'] == 'image'])}"
            )
            
            return chunks
            
        except Exception as e:
            logger.error(f"Error chunking document: {str(e)}", exc_info=True)
            # Return a single chunk with the full text as fallback
            return [{
                "content": text[:self.max_chunk_size],
                "meta_data": {
                    "chunk_type": "text",
                    "chunk_index": 0,
                    "char_count": len(text[:self.max_chunk_size]),
                    "token_count": self._estimate_tokens(text[:self.max_chunk_size]),
                    "error": "Chunking failed, using fallback"
                },
                "search_keywords": []
            }]
    
    def _chunk_text(self, text: str) -> List[str]:
        """Split text into chunks using smart boundary detection."""
        if not text:
            return []
        
        chunks = []
        current_chunk = []
        current_size = 0
        
        # Split by paragraphs first
        paragraphs = text.split('\n\n')
        
        for para in paragraphs:
            para = para.strip()
            if not para:
                continue
            
            para_size = len(para)
            
            # If paragraph fits in current chunk
            if current_size + para_size <= self.chunk_size:
                current_chunk.append(para)
                current_size += para_size + 2  # +2 for \n\n
            else:
                # Save current chunk if not empty
                if current_chunk:
                    chunk_text = '\n\n'.join(current_chunk)
                    chunks.append(chunk_text)
                    
                    # Add overlap from end of previous chunk
                    if self.chunk_overlap > 0:
                        overlap_text = chunk_text[-self.chunk_overlap:]
                        current_chunk = [overlap_text, para]
                        current_size = len(overlap_text) + para_size + 2
                    else:
                        current_chunk = [para]
                        current_size = para_size
                else:
                    # Paragraph is larger than chunk size, split it
                    if para_size > self.max_chunk_size:
                        sub_chunks = self._split_large_paragraph(para)
                        chunks.extend(sub_chunks[:-1])
                        current_chunk = [sub_chunks[-1]]
                        current_size = len(sub_chunks[-1])
                    else:
                        current_chunk = [para]
                        current_size = para_size
        
        # Add final chunk
        if current_chunk:
            chunks.append('\n\n'.join(current_chunk))
        
        return chunks
    
    def _split_large_paragraph(self, text: str) -> List[str]:
        """Split a large paragraph into smaller chunks by sentences."""
        chunks = []
        sentences = text.replace('? ', '?|').replace('! ', '!|').replace('. ', '.|').split('|')
        
        current_chunk = []
        current_size = 0
        
        for sentence in sentences:
            sentence = sentence.strip()
            if not sentence:
                continue
            
            sentence_size = len(sentence)
            
            if current_size + sentence_size <= self.chunk_size:
                current_chunk.append(sentence)
                current_size += sentence_size + 1
            else:
                if current_chunk:
                    chunks.append(' '.join(current_chunk))
                current_chunk = [sentence]
                current_size = sentence_size
        
        if current_chunk:
            chunks.append(' '.join(current_chunk))
        
        return chunks
    
    def _format_table_content(self, table: Dict[str, Any]) -> str:
        """Format table as searchable text content."""
        content_parts = [
            f"[TABLE on page {table.get('page_number', '?')}]",
            ""
        ]
        
        # Add headers
        if table.get("headers"):
            content_parts.append("Headers: " + " | ".join(table["headers"]))
            content_parts.append("")
        
        # Add markdown representation
        if table.get("markdown"):
            content_parts.append(table["markdown"])
        
        # Add row count summary
        if table.get("row_count"):
            content_parts.append("")
            content_parts.append(
                f"Table contains {table['row_count']} rows and "
                f"{table.get('col_count', 0)} columns"
            )
        
        return "\n".join(content_parts)
    
    def _format_image_content(self, image: Dict[str, Any]) -> str:
        """Format image metadata as searchable text content."""
        content_parts = [
            f"[{image.get('image_type', 'IMAGE').upper()} on page {image.get('page_number', '?')}]",
            ""
        ]
        
        if image.get("caption"):
            content_parts.append(f"Caption: {image['caption']}")
        
        if image.get("alt_text") and image.get("alt_text") != image.get("caption"):
            content_parts.append(f"Description: {image['alt_text']}")
        
        return "\n".join(content_parts)
    
    def _get_structural_metadata(
        self,
        chunk_index: int,
        structure: Dict[str, Any]
    ) -> Dict[str, Any]:
        """Extract structural metadata for a chunk."""
        metadata = {}
        
        # Estimate page number based on chunk position
        if structure.get("pages"):
            total_pages = len(structure["pages"])
            estimated_page = int((chunk_index / total_pages) * total_pages) + 1
            metadata["estimated_page"] = min(estimated_page, total_pages)
        
        # Add section information if available
        if structure.get("sections"):
            metadata["has_sections"] = True
            metadata["section_count"] = len(structure["sections"])
        
        return metadata
    
    def _extract_keywords(self, text: str, max_keywords: int = 10) -> List[str]:
        """Extract simple keywords from text for search optimization."""
        # Remove special characters and split
        words = text.lower().replace('\n', ' ').split()
        
        # Filter out common words and short words
        stop_words = {
            'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for',
            'of', 'with', 'by', 'from', 'is', 'are', 'was', 'were', 'be', 'been',
            'this', 'that', 'these', 'those', 'it', 'its', 'can', 'will', 'would'
        }
        
        keywords = [
            word.strip('.,;:!?"\'()[]{}')
            for word in words
            if len(word) > 3 and word not in stop_words
        ]
        
        # Count frequency
        keyword_freq = {}
        for word in keywords:
            keyword_freq[word] = keyword_freq.get(word, 0) + 1
        
        # Sort by frequency and return top keywords
        sorted_keywords = sorted(
            keyword_freq.items(),
            key=lambda x: x[1],
            reverse=True
        )
        
        return [kw[0] for kw in sorted_keywords[:max_keywords]]
    
    def _estimate_tokens(self, text: str) -> int:
        """Estimate token count (rough approximation: 1 token  4 chars)."""
        return len(text) // 4
    
    def _chars_to_tokens(self, chars: int) -> int:
        """Convert character count to approximate token count."""
        return chars // 4
    
    def update_config(
        self,
        chunk_size: Optional[int] = None,
        chunk_overlap: Optional[int] = None,
        max_chunk_size: Optional[int] = None
    ):
        """Update chunking configuration."""
        if chunk_size is not None:
            self.chunk_size = chunk_size
        if chunk_overlap is not None:
            self.chunk_overlap = chunk_overlap
        if max_chunk_size is not None:
            self.max_chunk_size = max_chunk_size
        
        logger.info(
            f"ChunkingService config updated - Size: {self.chunk_size}, "
            f"Overlap: {self.chunk_overlap}, Max: {self.max_chunk_size}"
        )


# Create singleton instance
chunking_service = ChunkingService(
    chunk_size=1000,
    chunk_overlap=200,
    max_chunk_size=2000
)
</file>

<file path="app/services/context_manager.py">
"""
Context Window Manager for managing conversation history within token limits.
Handles intelligent truncation, summarization, and context preservation.
"""
import logging
from typing import List, Dict, Any, Optional, Tuple
from datetime import datetime

from sqlalchemy.orm import Session

from app.models.chat import ChatMessage, MessageRole

logger = logging.getLogger(__name__)


class ContextWindowManager:
    """
    Manages conversation context to fit within model token limits.
    
    Features:
    - Token counting and estimation
    - Intelligent message truncation
    - System message preservation
    - Important context retention
    - Conversation summarization
    - Sliding window strategy
    """
    
    # Model context limits (in tokens) - Updated 2024
    MODEL_LIMITS = {
        # OpenAI
        "gpt-4": 128000,
        "gpt-4-turbo": 128000,
        "gpt-3.5-turbo": 16385,
        # Anthropic Claude 3
        "claude-3-opus": 200000,
        "claude-3-sonnet": 200000,
        "claude-3-haiku": 200000,
        # Meta Llama 3.1+ (128K context)
        "llama3": 128000,
        "llama3-70b": 128000,
        "llama3.1": 128000,
        "llama3.3": 128000,
        # Other models
        "mistral": 32000,
        "codellama": 16000,
    }
    
    # Reserve tokens for response
    RESPONSE_RESERVE = 2000
    
    def __init__(self, model_id: str = "gpt-3.5-turbo"):
        """
        Initialize context window manager.
        
        Args:
            model_id: Model identifier for token limit lookup
        """
        self.model_id = model_id
        self.max_tokens = self.MODEL_LIMITS.get(model_id, 4096)
        self.available_tokens = self.max_tokens - self.RESPONSE_RESERVE
        
        logger.info(
            f"ContextWindowManager initialized - Model: {model_id}, "
            f"Max: {self.max_tokens}, Available: {self.available_tokens}"
        )
    
    def estimate_tokens(self, text: str) -> int:
        """
        Estimate token count for text.
        
        Simple estimation: ~4 characters per token.
        For production, use tiktoken for accurate counts.
        """
        return len(text) // 4
    
    def count_message_tokens(self, message: Dict[str, str]) -> int:
        """Count tokens in a single message."""
        # Account for message structure overhead (~4 tokens)
        role_tokens = 4
        content_tokens = self.estimate_tokens(message.get("content", ""))
        return role_tokens + content_tokens
    
    def count_messages_tokens(self, messages: List[Dict[str, str]]) -> int:
        """Count total tokens in message list."""
        return sum(self.count_message_tokens(msg) for msg in messages)
    
    def fits_in_window(
        self,
        messages: List[Dict[str, str]],
        new_message: Optional[str] = None
    ) -> bool:
        """Check if messages fit within context window."""
        total_tokens = self.count_messages_tokens(messages)
        
        if new_message:
            total_tokens += self.estimate_tokens(new_message)
        
        return total_tokens <= self.available_tokens
    
    def truncate_messages(
        self,
        messages: List[Dict[str, str]],
        strategy: str = "sliding"
    ) -> Tuple[List[Dict[str, str]], int]:
        """
        Truncate messages to fit within context window.
        
        Args:
            messages: List of message dicts
            strategy: Truncation strategy ('sliding', 'smart', 'summary')
            
        Returns:
            Tuple of (truncated_messages, tokens_removed)
        """
        if self.fits_in_window(messages):
            return messages, 0
        
        if strategy == "sliding":
            return self._sliding_window_truncate(messages)
        elif strategy == "smart":
            return self._smart_truncate(messages)
        elif strategy == "summary":
            return self._summary_truncate(messages)
        else:
            return self._sliding_window_truncate(messages)
    
    def _sliding_window_truncate(
        self,
        messages: List[Dict[str, str]]
    ) -> Tuple[List[Dict[str, str]], int]:
        """
        Use sliding window to keep most recent messages.
        
        Strategy:
        1. Always keep system messages
        2. Remove oldest user/assistant pairs
        3. Keep most recent N messages that fit
        """
        # Separate system messages from conversation
        system_msgs = [msg for msg in messages if msg["role"] == "system"]
        conv_msgs = [msg for msg in messages if msg["role"] != "system"]
        
        # Start with system messages
        kept_messages = system_msgs.copy()
        current_tokens = self.count_messages_tokens(kept_messages)
        
        # Add conversation messages from most recent
        for msg in reversed(conv_msgs):
            msg_tokens = self.count_message_tokens(msg)
            
            if current_tokens + msg_tokens <= self.available_tokens:
                kept_messages.append(msg)
                current_tokens += msg_tokens
            else:
                break
        
        # Maintain chronological order (except system messages stay first)
        conv_kept = [msg for msg in kept_messages if msg["role"] != "system"]
        conv_kept.reverse()
        kept_messages = system_msgs + conv_kept
        
        original_tokens = self.count_messages_tokens(messages)
        removed_tokens = original_tokens - current_tokens
        
        logger.info(
            f"Sliding window truncation: Kept {len(kept_messages)}/{len(messages)} messages, "
            f"Removed {removed_tokens} tokens"
        )
        
        return kept_messages, removed_tokens
    
    def _smart_truncate(
        self,
        messages: List[Dict[str, str]]
    ) -> Tuple[List[Dict[str, str]], int]:
        """
        Smart truncation preserving important context.
        
        Strategy:
        1. Keep system messages
        2. Keep first user message (context setter)
        3. Keep tool-related messages
        4. Keep most recent messages
        5. Remove middle conversation
        """
        if len(messages) <= 3:
            return self._sliding_window_truncate(messages)
        
        # Identify important messages
        system_msgs = [msg for msg in messages if msg["role"] == "system"]
        first_user = next((msg for msg in messages if msg["role"] == "user"), None)
        tool_msgs = [msg for msg in messages if msg.get("tool_name") or msg["role"] == "tool"]
        
        # Start with critical messages
        kept_messages = system_msgs.copy()
        if first_user:
            kept_messages.append(first_user)
        kept_messages.extend(tool_msgs)
        
        current_tokens = self.count_messages_tokens(kept_messages)
        
        # Add most recent messages
        recent_msgs = []
        for msg in reversed(messages):
            if msg not in kept_messages:
                msg_tokens = self.count_message_tokens(msg)
                if current_tokens + msg_tokens <= self.available_tokens:
                    recent_msgs.insert(0, msg)
                    current_tokens += msg_tokens
                else:
                    break
        
        kept_messages.extend(recent_msgs)
        
        original_tokens = self.count_messages_tokens(messages)
        removed_tokens = original_tokens - current_tokens
        
        logger.info(
            f"Smart truncation: Kept {len(kept_messages)}/{len(messages)} messages, "
            f"Removed {removed_tokens} tokens"
        )
        
        return kept_messages, removed_tokens
    
    def _summary_truncate(
        self,
        messages: List[Dict[str, str]]
    ) -> Tuple[List[Dict[str, str]], int]:
        """
        Truncate with summarization of removed context.
        
        Strategy:
        1. Keep system messages
        2. Summarize oldest messages
        3. Keep recent messages
        4. Insert summary as system message
        
        Note: This is a placeholder. In production, you'd use an LLM
        to generate actual summaries of the removed context.
        """
        # For now, fall back to smart truncation
        # In production, implement actual summarization
        logger.info("Summary truncation requested (using smart truncation as fallback)")
        return self._smart_truncate(messages)
    
    def prepare_messages_for_llm(
        self,
        messages: List[ChatMessage],
        include_system: bool = True,
        max_history: Optional[int] = None
    ) -> List[Dict[str, str]]:
        """
        Prepare database messages for LLM consumption.
        
        Args:
            messages: List of ChatMessage objects from database
            include_system: Whether to include system messages
            max_history: Maximum number of messages to include
            
        Returns:
            List of message dicts ready for LLM
        """
        llm_messages = []
        
        # Add system message if requested
        if include_system:
            llm_messages.append({
                "role": "system",
                "content": "You are a helpful AI assistant with access to various tools and data sources."
            })
        
        # Convert database messages
        for msg in messages[-max_history:] if max_history else messages:
            llm_messages.append({
                "role": msg.role.value,
                "content": msg.content,
                "name": msg.meta_data.get("name") if msg.meta_data else None
            })
        
        # Truncate if needed
        if not self.fits_in_window(llm_messages):
            llm_messages, _ = self.truncate_messages(llm_messages, strategy="smart")
        
        return llm_messages
    
    def get_context_stats(
        self,
        messages: List[Dict[str, str]]
    ) -> Dict[str, Any]:
        """
        Get statistics about context usage.
        
        Returns:
            Dict with token counts, utilization, etc.
        """
        total_tokens = self.count_messages_tokens(messages)
        utilization = (total_tokens / self.available_tokens) * 100
        
        return {
            "total_tokens": total_tokens,
            "max_tokens": self.max_tokens,
            "available_tokens": self.available_tokens,
            "utilization_percent": round(utilization, 2),
            "fits_in_window": total_tokens <= self.available_tokens,
            "message_count": len(messages),
            "model_id": self.model_id
        }
    
    def should_truncate(
        self,
        messages: List[Dict[str, str]],
        threshold: float = 0.8
    ) -> bool:
        """
        Check if truncation is recommended.
        
        Args:
            messages: Message list to check
            threshold: Utilization threshold (0.0-1.0)
            
        Returns:
            True if utilization exceeds threshold
        """
        total_tokens = self.count_messages_tokens(messages)
        utilization = total_tokens / self.available_tokens
        return utilization >= threshold


def create_context_manager(model_id: str = "gpt-3.5-turbo") -> ContextWindowManager:
    """Factory function to create context manager for a model."""
    return ContextWindowManager(model_id=model_id)
</file>

<file path="app/services/document_processor.py">
"""
Document processing service using IBM's Docling for enhanced document parsing.
Handles PDF, DOCX, PPTX, and other document formats with AI-powered extraction.
"""
import io
import logging
from pathlib import Path
from typing import Dict, Any, List, Optional, Tuple
from datetime import datetime

from docling.document_converter import DocumentConverter, PdfFormatOption
from docling.datamodel.base_models import InputFormat
from docling.datamodel.pipeline_options import PdfPipelineOptions, TableFormerMode
from docling_core.types.doc import ImageRefMode, PictureItem, TableItem

logger = logging.getLogger(__name__)


class DocumentProcessor:
    """
    Advanced document processor using Docling for AI-powered extraction.
    
    Features:
    - Multi-format support (PDF, DOCX, PPTX, HTML, images)
    - AI-powered table extraction with TableFormer
    - Image captioning with SmolDocling VLM
    - OCR support for scanned documents
    - Hierarchical structure preservation
    - Rich metadata extraction
    """
    
    def __init__(
        self,
        extract_tables: bool = True,
        extract_images: bool = True,
        ocr_enabled: bool = True,
        table_mode: str = "accurate"
    ):
        """
        Initialize document processor with Docling.
        
        Args:
            extract_tables: Enable AI table extraction
            extract_images: Enable image extraction and captioning
            ocr_enabled: Enable OCR for scanned documents
            table_mode: "fast" or "accurate" for TableFormer
        """
        self.extract_tables = extract_tables
        self.extract_images = extract_images
        self.ocr_enabled = ocr_enabled
        
        # Configure Docling pipeline
        pipeline_options = PdfPipelineOptions()
        pipeline_options.do_table_structure = extract_tables
        pipeline_options.do_ocr = ocr_enabled
        
        # Set TableFormer mode
        if table_mode == "accurate":
            pipeline_options.table_structure_options.mode = TableFormerMode.ACCURATE
        else:
            pipeline_options.table_structure_options.mode = TableFormerMode.FAST
        
        # Initialize converter
        self.converter = DocumentConverter(
            format_options={
                InputFormat.PDF: PdfFormatOption(pipeline_options=pipeline_options)
            }
        )
        
        logger.info(
            f"DocumentProcessor initialized - Tables: {extract_tables}, "
            f"Images: {extract_images}, OCR: {ocr_enabled}, Mode: {table_mode}"
        )
    
    def process_document(
        self,
        file_path: str,
        file_type: Optional[str] = None
    ) -> Dict[str, Any]:
        """
        Process a document and extract all content with metadata.
        
        Args:
            file_path: Path to the document file
            file_type: File extension (pdf, docx, etc.)
            
        Returns:
            Dict containing:
                - text: Full text content
                - tables: List of extracted tables
                - images: List of extracted images with captions
                - metadata: Document metadata
                - structure: Hierarchical document structure
        """
        try:
            start_time = datetime.utcnow()
            
            # Convert document
            result = self.converter.convert(file_path)
            
            # Extract document
            doc = result.document
            
            # Get full text
            text = doc.export_to_markdown()
            
            # Extract tables
            tables = []
            if self.extract_tables:
                tables = self._extract_tables(doc)
            
            # Extract images
            images = []
            if self.extract_images:
                images = self._extract_images(doc)
            
            # Extract metadata
            metadata = self._extract_metadata(doc, file_path)
            
            # Extract structure
            structure = self._extract_structure(doc)
            
            # Calculate processing time
            processing_time = (datetime.utcnow() - start_time).total_seconds()
            
            return {
                "text": text,
                "tables": tables,
                "images": images,
                "metadata": metadata,
                "structure": structure,
                "success": True,
                "processing_time": processing_time,
                "page_count": len(doc.pages),
                "char_count": len(text),
                "error": None
            }
            
        except Exception as e:
            logger.error(f"Error processing document {file_path}: {str(e)}", exc_info=True)
            return {
                "text": "",
                "tables": [],
                "images": [],
                "metadata": {},
                "structure": {},
                "success": False,
                "processing_time": 0,
                "page_count": 0,
                "char_count": 0,
                "error": str(e)
            }
    
    def process_from_bytes(
        self,
        file_bytes: bytes,
        filename: str,
        file_type: str
    ) -> Dict[str, Any]:
        """
        Process document from bytes (for uploaded files).
        
        Args:
            file_bytes: Document bytes
            filename: Original filename
            file_type: File extension
            
        Returns:
            Processing result dict
        """
        try:
            # Write to temp file
            temp_path = Path(f"/tmp/{filename}")
            temp_path.write_bytes(file_bytes)
            
            # Process
            result = self.process_document(str(temp_path), file_type)
            
            # Cleanup
            temp_path.unlink(missing_ok=True)
            
            return result
            
        except Exception as e:
            logger.error(f"Error processing bytes for {filename}: {str(e)}", exc_info=True)
            return {
                "text": "",
                "tables": [],
                "images": [],
                "metadata": {},
                "structure": {},
                "success": False,
                "processing_time": 0,
                "page_count": 0,
                "char_count": 0,
                "error": str(e)
            }
    
    def _extract_tables(self, doc) -> List[Dict[str, Any]]:
        """Extract tables with structure preservation."""
        tables = []
        
        try:
            for page_idx, page in enumerate(doc.pages):
                for item in page.items:
                    if isinstance(item, TableItem):
                        # Get table as markdown
                        table_markdown = item.export_to_markdown()
                        
                        # Extract headers and rows
                        headers = []
                        rows = []
                        
                        if hasattr(item, 'data') and item.data:
                            # Parse table data if available
                            if item.data.grid:
                                for row_idx, row in enumerate(item.data.grid):
                                    row_cells = [cell.text if cell else "" for cell in row]
                                    if row_idx == 0:
                                        headers = row_cells
                                    else:
                                        rows.append(row_cells)
                        
                        tables.append({
                            "page_number": page_idx + 1,
                            "table_index": len([t for t in tables if t["page_number"] == page_idx + 1]),
                            "markdown": table_markdown,
                            "headers": headers,
                            "rows": rows,
                            "row_count": len(rows),
                            "col_count": len(headers) if headers else 0,
                            "meta_data": {
                                "bbox": item.prov[0].bbox.as_tuple() if item.prov else None,
                                "confidence": getattr(item, 'confidence', None)
                            }
                        })
            
            logger.info(f"Extracted {len(tables)} tables from document")
            
        except Exception as e:
            logger.error(f"Error extracting tables: {str(e)}", exc_info=True)
        
        return tables
    
    def _extract_images(self, doc) -> List[Dict[str, Any]]:
        """Extract images with AI-generated captions."""
        images = []
        
        try:
            for page_idx, page in enumerate(doc.pages):
                for item in page.items:
                    if isinstance(item, PictureItem):
                        # Get image caption/alt text
                        caption = item.caption if hasattr(item, 'caption') else None
                        
                        # Determine image type
                        image_type = "figure"
                        if hasattr(item, 'label'):
                            label = item.label.lower()
                            if 'chart' in label or 'graph' in label:
                                image_type = "chart"
                            elif 'diagram' in label:
                                image_type = "diagram"
                            elif 'photo' in label:
                                image_type = "photo"
                        
                        images.append({
                            "page_number": page_idx + 1,
                            "image_index": len([i for i in images if i["page_number"] == page_idx + 1]),
                            "caption": caption,
                            "alt_text": item.text if hasattr(item, 'text') else caption,
                            "image_type": image_type,
                            "meta_data": {
                                "bbox": item.prov[0].bbox.as_tuple() if item.prov else None,
                                "image_format": getattr(item, 'image_format', None)
                            }
                        })
            
            logger.info(f"Extracted {len(images)} images from document")
            
        except Exception as e:
            logger.error(f"Error extracting images: {str(e)}", exc_info=True)
        
        return images
    
    def _extract_metadata(self, doc, file_path: str) -> Dict[str, Any]:
        """Extract document metadata."""
        metadata = {
            "file_path": file_path,
            "page_count": len(doc.pages),
            "processed_at": datetime.utcnow().isoformat()
        }
        
        # Add document properties if available
        if hasattr(doc, 'properties'):
            props = doc.properties
            metadata.update({
                "title": getattr(props, 'title', None),
                "author": getattr(props, 'author', None),
                "subject": getattr(props, 'subject', None),
                "keywords": getattr(props, 'keywords', None),
                "creator": getattr(props, 'creator', None),
                "producer": getattr(props, 'producer', None),
                "creation_date": getattr(props, 'creation_date', None),
                "modification_date": getattr(props, 'modification_date', None)
            })
        
        return metadata
    
    def _extract_structure(self, doc) -> Dict[str, Any]:
        """Extract hierarchical document structure."""
        structure = {
            "pages": [],
            "has_toc": False,
            "sections": []
        }
        
        try:
            for page_idx, page in enumerate(doc.pages):
                page_structure = {
                    "page_number": page_idx + 1,
                    "item_count": len(page.items),
                    "has_tables": any(isinstance(item, TableItem) for item in page.items),
                    "has_images": any(isinstance(item, PictureItem) for item in page.items)
                }
                structure["pages"].append(page_structure)
            
            # Extract table of contents if available
            if hasattr(doc, 'outline') and doc.outline:
                structure["has_toc"] = True
                structure["sections"] = [
                    {
                        "title": item.title,
                        "level": item.level,
                        "page": item.page_number
                    }
                    for item in doc.outline
                ]
            
        except Exception as e:
            logger.error(f"Error extracting structure: {str(e)}", exc_info=True)
        
        return structure
    
    def get_supported_formats(self) -> List[str]:
        """Get list of supported document formats."""
        return [
            "pdf",
            "docx",
            "pptx",
            "html",
            "md",
            "asciidoc",
            "png",
            "jpg",
            "jpeg",
            "tiff",
            "bmp"
        ]
    
    def validate_file_type(self, file_type: str) -> bool:
        """Check if file type is supported."""
        return file_type.lower().lstrip('.') in self.get_supported_formats()


# Create singleton instance
document_processor = DocumentProcessor(
    extract_tables=True,
    extract_images=True,
    ocr_enabled=True,
    table_mode="accurate"
)
</file>

<file path="app/services/embedding_service.py">
"""
Embedding service for generating vector embeddings using OpenAI API.
Supports multiple embedding models with fallback and caching capabilities.
"""
import logging
import time
from typing import List, Dict, Any, Optional, Tuple
import asyncio
from datetime import datetime

import openai
from sqlalchemy.orm import Session
from app.config import settings
from app.models.document import EmbeddingModel

logger = logging.getLogger(__name__)


class EmbeddingService:
    """
    Service for generating vector embeddings for document chunks.
    
    Features:
    - OpenAI embedding models (text-embedding-3-small, text-embedding-3-large)
    - Batch processing for efficiency
    - Automatic retry with exponential backoff
    - Model fallback on errors
    - Performance metrics tracking
    - Cost estimation
    """
    
    def __init__(
        self,
        api_key: Optional[str] = None,
        model: str = "text-embedding-3-small",
        dimension: int = 1536,
        batch_size: int = 100
    ):
        """
        Initialize embedding service.
        
        Args:
            api_key: OpenAI API key (default: from settings)
            model: Embedding model to use
            dimension: Vector dimension (1536 for text-embedding-3-small)
            batch_size: Number of texts to process in one batch
        """
        self.api_key = api_key or settings.OPENAI_API_KEY
        self.model = model
        self.dimension = dimension
        self.batch_size = batch_size
        
        # Initialize OpenAI client
        openai.api_key = self.api_key
        
        # Performance tracking
        self.total_tokens = 0
        self.total_requests = 0
        self.total_errors = 0
        self.total_latency_ms = 0
        
        logger.info(
            f"EmbeddingService initialized - Model: {model}, "
            f"Dimension: {dimension}, Batch: {batch_size}"
        )
    
    async def generate_embedding(self, text: str) -> List[float]:
        """
        Generate embedding for a single text.
        
        Args:
            text: Text to embed
            
        Returns:
            Embedding vector
        """
        embeddings = await self.generate_embeddings([text])
        return embeddings[0] if embeddings else []
    
    async def generate_embeddings(
        self,
        texts: List[str],
        max_retries: int = 3
    ) -> List[List[float]]:
        """
        Generate embeddings for multiple texts with batching and retry.
        
        Args:
            texts: List of texts to embed
            max_retries: Maximum retry attempts on failure
            
        Returns:
            List of embedding vectors
        """
        if not texts:
            return []
        
        # Filter out empty texts
        texts = [t.strip() for t in texts if t and t.strip()]
        if not texts:
            return []
        
        embeddings = []
        
        # Process in batches
        for i in range(0, len(texts), self.batch_size):
            batch = texts[i:i + self.batch_size]
            
            for attempt in range(max_retries):
                try:
                    batch_embeddings = await self._generate_batch(batch)
                    embeddings.extend(batch_embeddings)
                    break
                    
                except Exception as e:
                    logger.error(
                        f"Error generating embeddings (attempt {attempt + 1}/{max_retries}): {str(e)}"
                    )
                    
                    if attempt == max_retries - 1:
                        self.total_errors += 1
                        # Return zero vectors for failed batch
                        embeddings.extend([[0.0] * self.dimension] * len(batch))
                    else:
                        # Exponential backoff
                        await asyncio.sleep(2 ** attempt)
        
        return embeddings
    
    async def _generate_batch(self, texts: List[str]) -> List[List[float]]:
        """Generate embeddings for a batch of texts."""
        start_time = time.time()
        
        try:
            # Call OpenAI API
            response = await asyncio.to_thread(
                openai.embeddings.create,
                model=self.model,
                input=texts,
                encoding_format="float"
            )
            
            # Extract embeddings
            embeddings = [item.embedding for item in response.data]
            
            # Update metrics
            latency_ms = (time.time() - start_time) * 1000
            self.total_latency_ms += latency_ms
            self.total_tokens += response.usage.total_tokens
            self.total_requests += 1
            
            logger.debug(
                f"Generated {len(embeddings)} embeddings in {latency_ms:.2f}ms "
                f"({response.usage.total_tokens} tokens)"
            )
            
            return embeddings
            
        except openai.RateLimitError as e:
            logger.warning(f"Rate limit hit: {str(e)}")
            raise
            
        except openai.APIError as e:
            logger.error(f"OpenAI API error: {str(e)}")
            raise
            
        except Exception as e:
            logger.error(f"Unexpected error generating embeddings: {str(e)}", exc_info=True)
            raise
    
    def estimate_tokens(self, text: str) -> int:
        """Estimate token count for text (rough: 1 token  4 chars)."""
        return len(text) // 4
    
    def estimate_cost(self, token_count: int) -> float:
        """
        Estimate cost for embedding generation.
        
        text-embedding-3-small: $0.02 per 1M tokens
        text-embedding-3-large: $0.13 per 1M tokens
        """
        if "large" in self.model:
            cost_per_1k = 0.00013
        else:  # small model
            cost_per_1k = 0.00002
        
        return (token_count / 1000) * cost_per_1k
    
    def get_performance_metrics(self) -> Dict[str, Any]:
        """Get performance metrics for this service."""
        avg_latency = (
            self.total_latency_ms / self.total_requests
            if self.total_requests > 0
            else 0
        )
        
        return {
            "total_requests": self.total_requests,
            "total_tokens": self.total_tokens,
            "total_errors": self.total_errors,
            "avg_latency_ms": avg_latency,
            "estimated_cost_usd": self.estimate_cost(self.total_tokens),
            "model": self.model,
            "dimension": self.dimension
        }
    
    def reset_metrics(self):
        """Reset performance metrics."""
        self.total_tokens = 0
        self.total_requests = 0
        self.total_errors = 0
        self.total_latency_ms = 0
        logger.info("Performance metrics reset")


class EmbeddingModelManager:
    """
    Manager for multiple embedding models with fallback and load balancing.
    """
    
    def __init__(self, db: Session):
        """
        Initialize embedding model manager.
        
        Args:
            db: Database session
        """
        self.db = db
        self.services: Dict[str, EmbeddingService] = {}
        self._initialize_services()
    
    def _initialize_services(self):
        """Initialize embedding services for all active models."""
        models = self.db.query(EmbeddingModel).filter(
            EmbeddingModel.is_active == True
        ).all()
        
        for model in models:
            try:
                service = EmbeddingService(
                    model=model.model_id,
                    dimension=model.dimension,
                    batch_size=100
                )
                self.services[model.name] = service
                logger.info(f"Initialized embedding service: {model.name}")
            except Exception as e:
                logger.error(f"Failed to initialize {model.name}: {str(e)}")
    
    async def generate_embeddings(
        self,
        texts: List[str],
        model_name: Optional[str] = None
    ) -> Tuple[List[List[float]], str]:
        """
        Generate embeddings using specified model or default.
        
        Args:
            texts: Texts to embed
            model_name: Model to use (default: use default model)
            
        Returns:
            Tuple of (embeddings, model_name_used)
        """
        # Get model
        if model_name and model_name in self.services:
            service = self.services[model_name]
        else:
            # Use default model
            default_model = self.db.query(EmbeddingModel).filter(
                EmbeddingModel.is_default == True,
                EmbeddingModel.is_active == True
            ).first()
            
            if not default_model:
                raise ValueError("No default embedding model configured")
            
            service = self.services.get(default_model.name)
            model_name = default_model.name
            
            if not service:
                raise ValueError(f"Service for {model_name} not initialized")
        
        # Generate embeddings
        embeddings = await service.generate_embeddings(texts)
        
        return embeddings, model_name
    
    def get_model_info(self, model_name: str) -> Optional[Dict[str, Any]]:
        """Get information about an embedding model."""
        model = self.db.query(EmbeddingModel).filter(
            EmbeddingModel.name == model_name
        ).first()
        
        if not model:
            return None
        
        service = self.services.get(model_name)
        metrics = service.get_performance_metrics() if service else {}
        
        return {
            "name": model.name,
            "display_name": model.display_name,
            "provider": model.provider,
            "model_id": model.model_id,
            "dimension": model.dimension,
            "max_tokens": model.max_tokens,
            "is_active": model.is_active,
            "is_default": model.is_default,
            "metrics": metrics
        }
    
    def update_model_metrics(self, model_name: str):
        """Update database with model performance metrics."""
        service = self.services.get(model_name)
        if not service:
            return
        
        model = self.db.query(EmbeddingModel).filter(
            EmbeddingModel.name == model_name
        ).first()
        
        if model:
            metrics = service.get_performance_metrics()
            model.avg_latency_ms = metrics["avg_latency_ms"]
            model.updated_at = datetime.utcnow()
            self.db.commit()
            logger.info(f"Updated metrics for {model_name}")


# Create singleton instance (will be properly initialized with DB session in API)
_embedding_service: Optional[EmbeddingService] = None


def get_embedding_service() -> EmbeddingService:
    """Get or create embedding service singleton."""
    global _embedding_service
    
    if _embedding_service is None:
        _embedding_service = EmbeddingService(
            model=settings.EMBEDDING_MODEL,
            dimension=settings.EMBEDDING_DIMENSIONS,
            batch_size=100
        )
    
    return _embedding_service


async def generate_embeddings_for_chunks(
    chunks: List[str]
) -> List[List[float]]:
    """
    Convenience function to generate embeddings for document chunks.
    
    Args:
        chunks: List of text chunks
        
    Returns:
        List of embedding vectors
    """
    service = get_embedding_service()
    return await service.generate_embeddings(chunks)
</file>

<file path="app/services/llm_service.py">
"""
LLM Service for unified interface to multiple AI providers.
Supports OpenAI, Anthropic, Google Gemini, and local Ollama models.
"""
import os
import logging
from typing import List, Dict, Any, Optional, AsyncIterator
from datetime import datetime
import asyncio

import openai

try:
    from anthropic import AsyncAnthropic
    ANTHROPIC_AVAILABLE = True
except ImportError:
    ANTHROPIC_AVAILABLE = False

try:
    import httpx
    HTTPX_AVAILABLE = True
except ImportError:
    HTTPX_AVAILABLE = False

from app.config import settings

logger = logging.getLogger(__name__)


class LLMProvider:
    """Base class for LLM providers."""
    
    def __init__(self, model_id: str, **kwargs):
        self.model_id = model_id
        self.kwargs = kwargs
    
    async def stream_completion(
        self,
        messages: List[Dict[str, str]],
        temperature: float = 0.7,
        max_tokens: Optional[int] = None,
        **kwargs
    ) -> AsyncIterator[str]:
        """Stream completion from LLM."""
        raise NotImplementedError


class OpenAIProvider(LLMProvider):
    """OpenAI API provider (GPT-4, GPT-3.5, etc.)."""
    
    def __init__(self, model_id: str, api_key: Optional[str] = None):
        super().__init__(model_id)
        self.client = openai.AsyncOpenAI(api_key=api_key or settings.OPENAI_API_KEY)
        logger.info(f"Initialized OpenAI provider with model: {model_id}")
    
    async def stream_completion(
        self,
        messages: List[Dict[str, str]],
        temperature: float = 0.7,
        max_tokens: Optional[int] = None,
        tools: Optional[List[Dict]] = None,
        **kwargs
    ) -> AsyncIterator[Dict[str, Any]]:
        """Stream completion from OpenAI."""
        try:
            params = {
                "model": self.model_id,
                "messages": messages,
                "temperature": temperature,
                "stream": True,
            }
            
            if max_tokens:
                params["max_tokens"] = max_tokens
            
            if tools:
                params["tools"] = tools
                params["tool_choice"] = "auto"
            
            stream = await self.client.chat.completions.create(**params)
            
            async for chunk in stream:
                if not chunk.choices:
                    continue
                
                choice = chunk.choices[0]
                delta = choice.delta
                
                # Handle content
                if delta.content:
                    yield {
                        "type": "content",
                        "content": delta.content,
                        "finish_reason": choice.finish_reason
                    }
                
                # Handle tool calls
                if delta.tool_calls:
                    for tool_call in delta.tool_calls:
                        yield {
                            "type": "tool_call",
                            "tool_call": {
                                "id": tool_call.id,
                                "name": tool_call.function.name if tool_call.function else None,
                                "arguments": tool_call.function.arguments if tool_call.function else None
                            }
                        }
                
                # Handle finish
                if choice.finish_reason:
                    yield {
                        "type": "done",
                        "finish_reason": choice.finish_reason
                    }
                    
        except openai.APIError as e:
            logger.error(f"OpenAI API error: {str(e)}")
            yield {"type": "error", "error": str(e)}
        except Exception as e:
            logger.error(f"Error streaming from OpenAI: {str(e)}", exc_info=True)
            yield {"type": "error", "error": str(e)}


class AnthropicProvider(LLMProvider):
    """Anthropic Claude API provider."""
    
    def __init__(self, model_id: str, api_key: Optional[str] = None):
        super().__init__(model_id)
        self.client = AsyncAnthropic(api_key=api_key or settings.ANTHROPIC_API_KEY)
        logger.info(f"Initialized Anthropic provider with model: {model_id}")
    
    async def stream_completion(
        self,
        messages: List[Dict[str, str]],
        temperature: float = 0.7,
        max_tokens: Optional[int] = None,
        tools: Optional[List[Dict]] = None,
        **kwargs
    ) -> AsyncIterator[Dict[str, Any]]:
        """Stream completion from Anthropic."""
        try:
            # Convert OpenAI-style messages to Anthropic format
            system_message = None
            anthropic_messages = []
            
            for msg in messages:
                if msg["role"] == "system":
                    system_message = msg["content"]
                else:
                    anthropic_messages.append({
                        "role": msg["role"],
                        "content": msg["content"]
                    })
            
            params = {
                "model": self.model_id,
                "messages": anthropic_messages,
                "temperature": temperature,
                "max_tokens": max_tokens or 4096,
                "stream": True
            }
            
            if system_message:
                params["system"] = system_message
            
            if tools:
                # Convert tools to Anthropic format
                params["tools"] = self._convert_tools(tools)
            
            async with self.client.messages.stream(**params) as stream:
                async for event in stream:
                    if hasattr(event, 'type'):
                        if event.type == "content_block_delta":
                            if hasattr(event.delta, 'text'):
                                yield {
                                    "type": "content",
                                    "content": event.delta.text,
                                    "finish_reason": None
                                }
                        elif event.type == "message_stop":
                            yield {
                                "type": "done",
                                "finish_reason": "stop"
                            }
                            
        except Exception as e:
            logger.error(f"Error streaming from Anthropic: {str(e)}", exc_info=True)
            yield {"type": "error", "error": str(e)}
    
    def _convert_tools(self, tools: List[Dict]) -> List[Dict]:
        """Convert OpenAI tool format to Anthropic format."""
        anthropic_tools = []
        for tool in tools:
            if tool.get("type") == "function":
                func = tool["function"]
                anthropic_tools.append({
                    "name": func["name"],
                    "description": func.get("description", ""),
                    "input_schema": func.get("parameters", {})
                })
        return anthropic_tools


class OllamaProvider(LLMProvider):
    """Ollama local model provider."""
    
    def __init__(self, model_id: str, base_url: Optional[str] = None):
        super().__init__(model_id)
        self.base_url = base_url or settings.OLLAMA_BASE_URL or "http://localhost:11434"
        self.client = httpx.AsyncClient(timeout=60.0)
        logger.info(f"Initialized Ollama provider with model: {model_id} at {self.base_url}")
    
    async def stream_completion(
        self,
        messages: List[Dict[str, str]],
        temperature: float = 0.7,
        max_tokens: Optional[int] = None,
        **kwargs
    ) -> AsyncIterator[Dict[str, Any]]:
        """Stream completion from Ollama."""
        try:
            url = f"{self.base_url}/api/chat"
            
            payload = {
                "model": self.model_id,
                "messages": messages,
                "stream": True,
                "options": {
                    "temperature": temperature,
                }
            }
            
            if max_tokens:
                payload["options"]["num_predict"] = max_tokens
            
            async with self.client.stream("POST", url, json=payload) as response:
                if response.status_code != 200:
                    error_msg = await response.aread()
                    logger.error(f"Ollama error: {error_msg}")
                    yield {"type": "error", "error": f"HTTP {response.status_code}"}
                    return
                
                async for line in response.aiter_lines():
                    if not line.strip():
                        continue
                    
                    try:
                        import json
                        data = json.loads(line)
                        
                        if "message" in data and "content" in data["message"]:
                            yield {
                                "type": "content",
                                "content": data["message"]["content"],
                                "finish_reason": None
                            }
                        
                        if data.get("done", False):
                            yield {
                                "type": "done",
                                "finish_reason": "stop"
                            }
                            
                    except json.JSONDecodeError as e:
                        logger.error(f"Error parsing Ollama response: {e}")
                        continue
                        
        except Exception as e:
            logger.error(f"Error streaming from Ollama: {str(e)}", exc_info=True)
            yield {"type": "error", "error": str(e)}
    
    async def __aenter__(self):
        return self
    
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        await self.client.aclose()


class LLMService:
    """
    Unified LLM service for managing multiple AI providers.
    
    Supports:
    - OpenAI (GPT-4, GPT-3.5-turbo, etc.)
    - Anthropic (Claude 3 Opus, Sonnet, Haiku)
    - Google Gemini (via OpenAI-compatible API)
    - Ollama (Local models: Llama 3, Mistral, etc.)
    """
    
    def __init__(self):
        self.providers: Dict[str, LLMProvider] = {}
        self._initialized = False
        # Only initialize if not explicitly skipped (for migrations, CLI tools, etc.)
        if not os.getenv('SKIP_LLM_INIT'):
            self._initialize_providers()
    
    def _initialize_providers(self):
        """Initialize available LLM providers."""
        if self._initialized:
            return
            
        self._initialized = True
        
        # OpenAI models
        if settings.OPENAI_API_KEY:
            self.providers["gpt-4"] = OpenAIProvider("gpt-4-turbo-preview")
            self.providers["gpt-4-turbo"] = OpenAIProvider("gpt-4-turbo-preview")
            self.providers["gpt-3.5-turbo"] = OpenAIProvider("gpt-3.5-turbo")
            logger.info("OpenAI models registered")
        
        # Anthropic models
        if settings.ANTHROPIC_API_KEY:
            self.providers["claude-3-opus"] = AnthropicProvider("claude-3-opus-20240229")
            self.providers["claude-3-sonnet"] = AnthropicProvider("claude-3-sonnet-20240229")
            self.providers["claude-3-haiku"] = AnthropicProvider("claude-3-haiku-20240307")
            logger.info("Anthropic models registered")
        
        # Ollama local models
        try:
            # Try to connect to Ollama
            self.providers["llama3"] = OllamaProvider("llama3:8b")
            self.providers["llama3-70b"] = OllamaProvider("llama3:70b")
            self.providers["mistral"] = OllamaProvider("mistral:latest")
            self.providers["codellama"] = OllamaProvider("codellama:latest")
            logger.info("Ollama models registered")
        except Exception as e:
            logger.warning(f"Ollama not available: {e}")
    
    def get_provider(self, model_id: str) -> Optional[LLMProvider]:
        """Get provider for a specific model."""
        return self.providers.get(model_id)
    
    def list_models(self) -> List[Dict[str, Any]]:
        """List all available models."""
        models = []
        for model_id, provider in self.providers.items():
            provider_name = provider.__class__.__name__.replace("Provider", "")
            models.append({
                "id": model_id,
                "name": model_id,
                "provider": provider_name,
                "available": True
            })
        return models
    
    async def stream_chat(
        self,
        model_id: str,
        messages: List[Dict[str, str]],
        temperature: float = 0.7,
        max_tokens: Optional[int] = None,
        tools: Optional[List[Dict]] = None,
        **kwargs
    ) -> AsyncIterator[Dict[str, Any]]:
        """
        Stream chat completion from specified model.
        
        Args:
            model_id: Model identifier (e.g., 'gpt-4', 'claude-3-opus', 'llama3')
            messages: Chat messages in OpenAI format
            temperature: Sampling temperature (0-2)
            max_tokens: Maximum tokens to generate
            tools: Optional tool definitions for function calling
            
        Yields:
            Stream chunks with type and content
        """
        provider = self.get_provider(model_id)
        
        if not provider:
            logger.error(f"Model not found: {model_id}")
            yield {
                "type": "error",
                "error": f"Model '{model_id}' not available. Use /api/v1/llm/models to see available models."
            }
            return
        
        try:
            async for chunk in provider.stream_completion(
                messages=messages,
                temperature=temperature,
                max_tokens=max_tokens,
                tools=tools,
                **kwargs
            ):
                yield chunk
                
        except Exception as e:
            logger.error(f"Error in LLM stream: {str(e)}", exc_info=True)
            yield {"type": "error", "error": str(e)}
    
    async def get_model_info(self, model_id: str) -> Optional[Dict[str, Any]]:
        """Get information about a specific model."""
        provider = self.get_provider(model_id)
        if not provider:
            return None
        
        provider_name = provider.__class__.__name__.replace("Provider", "")
        
        # Model capabilities and limits
        capabilities = {
            "gpt-4": {"context": 128000, "supports_tools": True, "supports_vision": True},
            "gpt-4-turbo": {"context": 128000, "supports_tools": True, "supports_vision": True},
            "gpt-3.5-turbo": {"context": 16385, "supports_tools": True, "supports_vision": False},
            "claude-3-opus": {"context": 200000, "supports_tools": True, "supports_vision": True},
            "claude-3-sonnet": {"context": 200000, "supports_tools": True, "supports_vision": True},
            "claude-3-haiku": {"context": 200000, "supports_tools": True, "supports_vision": True},
            "llama3": {"context": 8192, "supports_tools": False, "supports_vision": False},
            "llama3-70b": {"context": 8192, "supports_tools": False, "supports_vision": False},
            "mistral": {"context": 32000, "supports_tools": False, "supports_vision": False},
            "codellama": {"context": 16000, "supports_tools": False, "supports_vision": False},
        }
        
        info = capabilities.get(model_id, {"context": 4096, "supports_tools": False, "supports_vision": False})
        
        return {
            "id": model_id,
            "name": model_id,
            "provider": provider_name,
            "context_window": info["context"],
            "supports_tools": info["supports_tools"],
            "supports_vision": info["supports_vision"],
            "available": True
        }


# Create singleton instance
llm_service = LLMService()
</file>

<file path="app/services/notification_service.py">
"""
Notification service for real-time notifications with SSE streaming.

Provides:
- Real-time Server-Sent Events (SSE) streaming
- Multi-instance support via Redis pub/sub
- Persistent notification storage
- User preference management
- Targeted and broadcast notifications
- Integration mixins for other services
"""
import json
import asyncio
import logging
from datetime import datetime, timedelta
from typing import Dict, Set, List, Optional, AsyncIterator, Any
from collections import defaultdict

from sqlalchemy import select, func, and_, or_
from sqlalchemy.ext.asyncio import AsyncSession
from sqlalchemy.orm import joinedload

from app.models.notification import (
    Notification, NotificationPreference, 
    NotificationType, NotificationPriority
)
from app.models.user import User, Role

logger = logging.getLogger(__name__)


class NotificationService:
    """
    Service for managing and delivering notifications.
    
    Features:
    - Real-time SSE delivery to connected clients
    - Redis pub/sub for horizontal scaling
    - Persistent database storage
    - User preference management
    - Priority-based delivery
    - Role-based broadcasting
    """
    
    def __init__(self):
        """Initialize notification service."""
        # Active SSE connections: {user_id: [queue1, queue2, ...]}
        self._connections: Dict[int, List[asyncio.Queue]] = defaultdict(list)
        # Redis client for pub/sub (to be injected)
        self._redis = None
        # Background task for Redis subscription
        self._redis_task: Optional[asyncio.Task] = None
        logger.info("NotificationService initialized")
    
    def set_redis(self, redis_client):
        """
        Set Redis client for pub/sub.
        
        Args:
            redis_client: Redis async client instance
        """
        self._redis = redis_client
        logger.info("Redis client configured for notifications")
    
    async def start_redis_listener(self):
        """Start background task to listen for Redis pub/sub notifications."""
        if not self._redis:
            logger.warning("Redis not configured, skipping pub/sub listener")
            return
        
        self._redis_task = asyncio.create_task(self._redis_subscriber())
        logger.info("Redis pub/sub listener started")
    
    async def stop_redis_listener(self):
        """Stop the Redis pub/sub listener."""
        if self._redis_task:
            self._redis_task.cancel()
            try:
                await self._redis_task
            except asyncio.CancelledError:
                pass
            logger.info("Redis pub/sub listener stopped")
    
    async def _redis_subscriber(self):
        """Background task to subscribe to Redis notifications channel."""
        try:
            pubsub = self._redis.pubsub()
            await pubsub.subscribe("notifications")
            
            async for message in pubsub.listen():
                if message["type"] == "message":
                    try:
                        notification_data = json.loads(message["data"])
                        await self._deliver_to_local_connections(notification_data)
                    except Exception as e:
                        logger.error(f"Error processing Redis notification: {e}")
        except asyncio.CancelledError:
            logger.info("Redis subscriber cancelled")
        except Exception as e:
            logger.error(f"Redis subscriber error: {e}")
    
    async def connect(self, user_id: int) -> asyncio.Queue:
        """
        Register a new SSE connection for a user.
        
        Args:
            user_id: ID of the user connecting
            
        Returns:
            asyncio.Queue for receiving notifications
        """
        queue = asyncio.Queue()
        self._connections[user_id].append(queue)
        logger.info(f"User {user_id} connected (total connections: {len(self._connections[user_id])})")
        return queue
    
    async def disconnect(self, user_id: int, queue: asyncio.Queue):
        """
        Remove an SSE connection for a user.
        
        Args:
            user_id: ID of the user disconnecting
            queue: The queue to remove
        """
        if user_id in self._connections:
            try:
                self._connections[user_id].remove(queue)
                if not self._connections[user_id]:
                    del self._connections[user_id]
                logger.info(f"User {user_id} disconnected")
            except ValueError:
                pass
    
    async def send_notification(
        self,
        db: AsyncSession,
        user_id: int,
        notification_type: NotificationType,
        title: str,
        message: str,
        data: Optional[Dict[str, Any]] = None,
        priority: NotificationPriority = NotificationPriority.NORMAL,
        persist: bool = True,
        expires_at: Optional[datetime] = None
    ) -> Optional[Notification]:
        """
        Send a notification to a specific user.
        
        Args:
            db: Database session
            user_id: Target user ID
            notification_type: Type of notification
            title: Short notification title
            message: Detailed message
            data: Additional structured data
            priority: Notification priority level
            persist: Whether to save to database
            expires_at: Optional expiration timestamp
            
        Returns:
            Created notification if persisted, None otherwise
        """
        notification_dict = {
            "user_id": user_id,
            "type": notification_type.value,
            "title": title,
            "message": message,
            "data": data or {},
            "priority": priority.value,
            "created_at": datetime.utcnow().isoformat(),
            "is_read": False,
            "read_at": None,
            "expires_at": expires_at.isoformat() if expires_at else None
        }
        
        notification_obj = None
        
        # Check user preferences
        if await self._check_user_preferences(db, user_id, notification_type):
            # Persist to database if requested
            if persist:
                notification_obj = await self._save_to_database(db, notification_dict)
                notification_dict["id"] = notification_obj.id
            
            # Publish to Redis for multi-instance delivery
            if self._redis:
                try:
                    await self._redis.publish(
                        "notifications",
                        json.dumps(notification_dict)
                    )
                except Exception as e:
                    logger.error(f"Failed to publish to Redis: {e}")
            
            # Deliver to local connections
            await self._deliver_to_local_connections(notification_dict)
            
            logger.info(
                f"Notification sent to user {user_id}: {notification_type.value} - {title}"
            )
        else:
            logger.debug(
                f"Notification {notification_type.value} disabled for user {user_id}"
            )
        
        return notification_obj
    
    async def send_broadcast_notification(
        self,
        db: AsyncSession,
        notification_type: NotificationType,
        title: str,
        message: str,
        target_roles: Optional[List[str]] = None,
        data: Optional[Dict[str, Any]] = None,
        priority: NotificationPriority = NotificationPriority.NORMAL,
        persist: bool = True
    ) -> int:
        """
        Send a notification to multiple users based on roles.
        
        Args:
            db: Database session
            notification_type: Type of notification
            title: Short notification title
            message: Detailed message
            target_roles: List of role names to target (None = all users)
            data: Additional structured data
            priority: Notification priority level
            persist: Whether to save to database
            
        Returns:
            Number of notifications sent
        """
        # Get target users
        users = await self._get_users_by_roles(db, target_roles)
        
        count = 0
        for user in users:
            await self.send_notification(
                db=db,
                user_id=user.id,
                notification_type=notification_type,
                title=title,
                message=message,
                data=data,
                priority=priority,
                persist=persist
            )
            count += 1
        
        logger.info(
            f"Broadcast notification sent to {count} users: {notification_type.value}"
        )
        return count
    
    async def _deliver_to_local_connections(self, notification: Dict[str, Any]):
        """
        Deliver notification to all active SSE connections for the user.
        
        Args:
            notification: Notification data dictionary
        """
        user_id = notification["user_id"]
        if user_id in self._connections:
            for queue in self._connections[user_id]:
                try:
                    await queue.put(notification)
                except Exception as e:
                    logger.error(f"Failed to deliver notification to queue: {e}")
    
    async def _save_to_database(
        self,
        db: AsyncSession,
        notification_dict: Dict[str, Any]
    ) -> Notification:
        """
        Save notification to database.
        
        Args:
            db: Database session
            notification_dict: Notification data
            
        Returns:
            Created notification object
        """
        # Convert ISO strings back to datetime if needed
        created_at = notification_dict.get("created_at")
        if isinstance(created_at, str):
            created_at = datetime.fromisoformat(created_at.replace('Z', '+00:00'))
        
        expires_at = notification_dict.get("expires_at")
        if isinstance(expires_at, str):
            expires_at = datetime.fromisoformat(expires_at.replace('Z', '+00:00'))
        
        notification = Notification(
            user_id=notification_dict["user_id"],
            type=notification_dict["type"],
            title=notification_dict["title"],
            message=notification_dict["message"],
            data=notification_dict.get("data", {}),
            priority=notification_dict["priority"],
            is_read=notification_dict.get("is_read", False),
            created_at=created_at or datetime.utcnow(),
            expires_at=expires_at
        )
        
        db.add(notification)
        await db.commit()
        await db.refresh(notification)
        return notification
    
    async def _check_user_preferences(
        self,
        db: AsyncSession,
        user_id: int,
        notification_type: NotificationType
    ) -> bool:
        """
        Check if user has enabled this notification type.
        
        Args:
            db: Database session
            user_id: User ID
            notification_type: Type of notification
            
        Returns:
            True if notification should be sent, False otherwise
        """
        result = await db.execute(
            select(NotificationPreference).where(
                and_(
                    NotificationPreference.user_id == user_id,
                    NotificationPreference.notification_type == notification_type.value
                )
            )
        )
        preference = result.scalar_one_or_none()
        
        # If no preference exists, default to enabled
        return preference.enabled if preference else True
    
    async def _get_users_by_roles(
        self,
        db: AsyncSession,
        target_roles: Optional[List[str]] = None
    ) -> List[User]:
        """
        Get users based on role filters.
        
        Args:
            db: Database session
            target_roles: List of role names (None = all active users)
            
        Returns:
            List of matching users
        """
        query = select(User).where(User.is_active == True)
        
        if target_roles:
            query = query.join(User.roles).where(Role.name.in_(target_roles))
        
        result = await db.execute(query)
        return result.scalars().unique().all()
    
    async def get_user_notifications(
        self,
        db: AsyncSession,
        user_id: int,
        unread_only: bool = False,
        limit: int = 50,
        offset: int = 0
    ) -> tuple[List[Notification], int]:
        """
        Retrieve user's notifications from database.
        
        Args:
            db: Database session
            user_id: User ID
            unread_only: Only return unread notifications
            limit: Maximum number of notifications
            offset: Pagination offset
            
        Returns:
            Tuple of (notifications list, total count)
        """
        # Build query
        query = select(Notification).where(Notification.user_id == user_id)
        
        if unread_only:
            query = query.where(Notification.is_read == False)
        
        # Remove expired notifications
        query = query.where(
            or_(
                Notification.expires_at == None,
                Notification.expires_at > datetime.utcnow()
            )
        )
        
        # Get total count
        count_query = select(func.count()).select_from(query.subquery())
        total_result = await db.execute(count_query)
        total = total_result.scalar()
        
        # Get paginated results
        query = query.order_by(Notification.created_at.desc())
        query = query.limit(limit).offset(offset)
        
        result = await db.execute(query)
        notifications = result.scalars().all()
        
        return notifications, total
    
    async def mark_as_read(
        self,
        db: AsyncSession,
        notification_id: int,
        user_id: int
    ) -> Optional[Notification]:
        """
        Mark a notification as read.
        
        Args:
            db: Database session
            notification_id: Notification ID
            user_id: User ID (for ownership verification)
            
        Returns:
            Updated notification or None if not found
        """
        result = await db.execute(
            select(Notification).where(
                and_(
                    Notification.id == notification_id,
                    Notification.user_id == user_id
                )
            )
        )
        notification = result.scalar_one_or_none()
        
        if notification and not notification.is_read:
            notification.mark_as_read()
            await db.commit()
            await db.refresh(notification)
            logger.info(f"Notification {notification_id} marked as read")
        
        return notification
    
    async def mark_all_as_read(
        self,
        db: AsyncSession,
        user_id: int
    ) -> int:
        """
        Mark all user's notifications as read.
        
        Args:
            db: Database session
            user_id: User ID
            
        Returns:
            Number of notifications updated
        """
        result = await db.execute(
            select(Notification).where(
                and_(
                    Notification.user_id == user_id,
                    Notification.is_read == False
                )
            )
        )
        notifications = result.scalars().all()
        
        count = 0
        for notification in notifications:
            notification.mark_as_read()
            count += 1
        
        await db.commit()
        logger.info(f"Marked {count} notifications as read for user {user_id}")
        return count
    
    async def delete_notification(
        self,
        db: AsyncSession,
        notification_id: int,
        user_id: int
    ) -> bool:
        """
        Delete a notification.
        
        Args:
            db: Database session
            notification_id: Notification ID
            user_id: User ID (for ownership verification)
            
        Returns:
            True if deleted, False if not found
        """
        result = await db.execute(
            select(Notification).where(
                and_(
                    Notification.id == notification_id,
                    Notification.user_id == user_id
                )
            )
        )
        notification = result.scalar_one_or_none()
        
        if notification:
            await db.delete(notification)
            await db.commit()
            logger.info(f"Notification {notification_id} deleted")
            return True
        
        return False
    
    async def get_notification_stats(
        self,
        db: AsyncSession,
        user_id: int
    ) -> Dict[str, Any]:
        """
        Get notification statistics for a user.
        
        Args:
            db: Database session
            user_id: User ID
            
        Returns:
            Dictionary with notification statistics
        """
        # Total notifications
        total_result = await db.execute(
            select(func.count()).select_from(Notification).where(
                Notification.user_id == user_id
            )
        )
        total = total_result.scalar()
        
        # Unread count
        unread_result = await db.execute(
            select(func.count()).select_from(Notification).where(
                and_(
                    Notification.user_id == user_id,
                    Notification.is_read == False
                )
            )
        )
        unread = unread_result.scalar()
        
        # By type
        type_result = await db.execute(
            select(
                Notification.type,
                func.count(Notification.id)
            ).where(
                Notification.user_id == user_id
            ).group_by(Notification.type)
        )
        by_type = {row[0]: row[1] for row in type_result}
        
        # By priority
        priority_result = await db.execute(
            select(
                Notification.priority,
                func.count(Notification.id)
            ).where(
                Notification.user_id == user_id
            ).group_by(Notification.priority)
        )
        by_priority = {row[0]: row[1] for row in priority_result}
        
        # Recent (last 24 hours)
        recent_result = await db.execute(
            select(func.count()).select_from(Notification).where(
                and_(
                    Notification.user_id == user_id,
                    Notification.created_at >= datetime.utcnow() - timedelta(hours=24)
                )
            )
        )
        recent = recent_result.scalar()
        
        return {
            "total_notifications": total,
            "unread_count": unread,
            "by_type": by_type,
            "by_priority": by_priority,
            "recent_count": recent
        }
    
    async def get_user_preferences(
        self,
        db: AsyncSession,
        user_id: int
    ) -> List[NotificationPreference]:
        """
        Get all notification preferences for a user.
        
        Args:
            db: Database session
            user_id: User ID
            
        Returns:
            List of notification preferences
        """
        result = await db.execute(
            select(NotificationPreference).where(
                NotificationPreference.user_id == user_id
            ).order_by(NotificationPreference.notification_type)
        )
        return result.scalars().all()
    
    async def update_preference(
        self,
        db: AsyncSession,
        user_id: int,
        notification_type: NotificationType,
        enabled: bool,
        delivery_method: str = "realtime"
    ) -> NotificationPreference:
        """
        Update or create a notification preference.
        
        Args:
            db: Database session
            user_id: User ID
            notification_type: Type of notification
            enabled: Whether notification is enabled
            delivery_method: Delivery method (realtime, email, etc.)
            
        Returns:
            Updated or created preference
        """
        result = await db.execute(
            select(NotificationPreference).where(
                and_(
                    NotificationPreference.user_id == user_id,
                    NotificationPreference.notification_type == notification_type.value
                )
            )
        )
        preference = result.scalar_one_or_none()
        
        if preference:
            preference.enabled = enabled
            preference.delivery_method = delivery_method
            preference.updated_at = datetime.utcnow()
        else:
            preference = NotificationPreference(
                user_id=user_id,
                notification_type=notification_type.value,
                enabled=enabled,
                delivery_method=delivery_method
            )
            db.add(preference)
        
        await db.commit()
        await db.refresh(preference)
        
        logger.info(
            f"Preference updated for user {user_id}: {notification_type.value} = {enabled}"
        )
        return preference


# Global singleton instance
notification_service = NotificationService()


# Integration Mixins for other services
class ApprovalNotificationMixin:
    """Mixin for approval workflow notifications."""
    
    async def notify_approval_requested(
        self,
        db: AsyncSession,
        manager_id: int,
        requester_name: str,
        tool_name: str,
        approval_id: int
    ):
        """Notify manager of new approval request."""
        await notification_service.send_notification(
            db=db,
            user_id=manager_id,
            notification_type=NotificationType.APPROVAL_REQUESTED,
            title="Approval Required",
            message=f"{requester_name} requests approval to run '{tool_name}'",
            data={
                "requester_name": requester_name,
                "tool_name": tool_name,
                "approval_id": approval_id,
                "action_url": f"/approvals/{approval_id}"
            },
            priority=NotificationPriority.HIGH
        )
    
    async def notify_approval_decision(
        self,
        db: AsyncSession,
        requester_id: int,
        approver_name: str,
        tool_name: str,
        decision: str,
        approval_id: int
    ):
        """Notify requester of approval decision."""
        priority = NotificationPriority.HIGH if decision == "APPROVED" else NotificationPriority.NORMAL
        await notification_service.send_notification(
            db=db,
            user_id=requester_id,
            notification_type=NotificationType.APPROVAL_DECISION,
            title=f"Approval {decision}",
            message=f"{approver_name} has {decision.lower()} your request to run '{tool_name}'",
            data={
                "approver_name": approver_name,
                "tool_name": tool_name,
                "decision": decision,
                "approval_id": approval_id
            },
            priority=priority
        )


class DocumentNotificationMixin:
    """Mixin for document-related notifications."""
    
    async def notify_document_indexed(
        self,
        db: AsyncSession,
        user_id: int,
        document_id: int,
        document_title: str
    ):
        """Notify user that document has been indexed."""
        await notification_service.send_notification(
            db=db,
            user_id=user_id,
            notification_type=NotificationType.DOCUMENT_INDEXED,
            title="Document Indexed",
            message=f"'{document_title}' has been successfully indexed and is now searchable",
            data={
                "document_id": document_id,
                "document_title": document_title,
                "action_url": f"/documents/{document_id}"
            },
            priority=NotificationPriority.NORMAL
        )
    
    async def notify_document_failed(
        self,
        db: AsyncSession,
        user_id: int,
        document_id: int,
        document_title: str,
        error_message: str
    ):
        """Notify user that document processing failed."""
        await notification_service.send_notification(
            db=db,
            user_id=user_id,
            notification_type=NotificationType.DOCUMENT_PROCESSING_FAILED,
            title="Document Processing Failed",
            message=f"Failed to process '{document_title}': {error_message}",
            data={
                "document_id": document_id,
                "document_title": document_title,
                "error_message": error_message
            },
            priority=NotificationPriority.HIGH
        )


class ToolNotificationMixin:
    """Mixin for tool registry notifications."""
    
    async def notify_tool_added(
        self,
        db: AsyncSession,
        tool_name: str,
        tool_description: str,
        target_roles: Optional[List[str]] = None
    ):
        """Notify users of new tool availability."""
        await notification_service.send_broadcast_notification(
            db=db,
            notification_type=NotificationType.TOOL_ADDED,
            title="New Tool Available",
            message=f"A new tool '{tool_name}' has been added: {tool_description}",
            target_roles=target_roles,
            data={
                "tool_name": tool_name,
                "tool_description": tool_description,
                "action_url": f"/tools/{tool_name}"
            },
            priority=NotificationPriority.NORMAL
        )
</file>

<file path="app/__init__.py">
"""CDSA Backend Application."""
__version__ = "1.0.0"
</file>

<file path="app/config.py">
"""Application configuration using Pydantic Settings."""
from typing import List
from pydantic import Field, field_validator
from pydantic_settings import BaseSettings, SettingsConfigDict


class Settings(BaseSettings):
    """Application settings."""
    
    model_config = SettingsConfigDict(
        env_file=".env",
        env_file_encoding="utf-8",
        case_sensitive=True,
        extra="ignore"
    )
    
    # Application
    APP_NAME: str = "CDSA Backend"
    APP_VERSION: str = "1.0.0"
    ENVIRONMENT: str = "development"
    DEBUG: bool = False
    LOG_LEVEL: str = "INFO"
    
    # API
    API_HOST: str = "0.0.0.0"
    API_PORT: int = 8000
    API_RELOAD: bool = False
    CORS_ORIGINS: str = "http://localhost:3000"
    
    @field_validator("CORS_ORIGINS")
    @classmethod
    def parse_cors_origins(cls, v: str) -> List[str]:
        """Parse comma-separated CORS origins."""
        return [origin.strip() for origin in v.split(",")]
    
    # Database
    DATABASE_URL: str = Field(..., description="PostgreSQL database URL")
    DB_POOL_SIZE: int = 20
    DB_MAX_OVERFLOW: int = 0
    DB_ECHO: bool = False
    
    # Redis
    REDIS_URL: str = Field(..., description="Redis connection URL")
    REDIS_CACHE_DB: int = 1
    REDIS_SESSION_DB: int = 2
    REDIS_NOTIFICATION_DB: int = 4
    
    # Security
    SECRET_KEY: str = Field(..., description="Secret key for signing")
    JWT_SECRET_KEY: str = Field(..., description="JWT secret key")
    JWT_ALGORITHM: str = "HS256"
    JWT_EXPIRATION_MINUTES: int = 60
    ENCRYPTION_KEY: str = Field(..., description="Encryption key for vault")
    
    # Rate Limiting
    RATE_LIMIT_REQUESTS: int = 100
    RATE_LIMIT_PERIOD: int = 60
    
    # LLM Configuration
    OPENAI_API_KEY: str = ""
    OPENAI_MODEL: str = "gpt-4-turbo-preview"
    OPENAI_TEMPERATURE: float = 0.7
    
    ANTHROPIC_API_KEY: str = ""
    ANTHROPIC_MODEL: str = "claude-3-opus-20240229"
    
    GEMINI_API_KEY: str = ""
    
    OLLAMA_BASE_URL: str = "http://localhost:11434"
    OLLAMA_DEFAULT_MODEL: str = "llama3:8b"
    
    # Embeddings
    EMBEDDING_MODEL: str = "text-embedding-3-small"
    EMBEDDING_DIMENSIONS: int = 1536
    
    # RAG Configuration
    RAG_CHUNK_SIZE: int = 1000
    RAG_CHUNK_OVERLAP: int = 200
    RAG_TOP_K: int = 5
    
    # Celery
    CELERY_BROKER_URL: str = Field(default="redis://localhost:6379/3")
    CELERY_RESULT_BACKEND: str = Field(default="redis://localhost:6379/3")
    
    # Context Window
    MAX_CONTEXT_WINDOW: int = 4096
    CONTEXT_PRUNING_THRESHOLD: int = 3500
    
    # Notifications
    NOTIFICATION_RETENTION_DAYS: int = 30
    NOTIFICATION_BATCH_SIZE: int = 100
    
    # File Storage
    UPLOAD_DIR: str = "./uploads"
    MAX_UPLOAD_SIZE: int = 10485760  # 10MB
    
    # Monitoring
    SENTRY_DSN: str = ""
    ENABLE_METRICS: bool = True
    
    @property
    def is_development(self) -> bool:
        """Check if running in development mode."""
        return self.ENVIRONMENT.lower() == "development"
    
    @property
    def is_production(self) -> bool:
        """Check if running in production mode."""
        return self.ENVIRONMENT.lower() == "production"


# Global settings instance
settings = Settings()
</file>

<file path="app/main.py">
"""FastAPI application entry point."""
from contextlib import asynccontextmanager
from fastapi import FastAPI
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse

from app.config import settings
from app.core.logging import setup_logging
from app.db.base import init_db, close_db, Base
from app.services.notification_service import notification_service

# Import all models to ensure they're registered with Base.metadata
import app.models  # This imports all models from __init__.py

# Setup logging
logger = setup_logging()


@asynccontextmanager
async def lifespan(app: FastAPI):
    """Application lifespan manager."""
    logger.info(f"Starting {settings.APP_NAME} v{settings.APP_VERSION}")
    logger.info(f"Environment: {settings.ENVIRONMENT}")
    
    # Startup
    try:
        # Initialize database connection
        logger.info("Initializing database connection...")
        engine, session_factory = init_db()
        logger.info(" Database connection initialized")
        
        # Create database tables
        logger.info("Creating database tables...")
        try:
            async with engine.begin() as conn:
                await conn.run_sync(Base.metadata.create_all)
            logger.info(" Database tables created successfully")
        except Exception as e:
            logger.error(f"Failed to create database tables: {e}")
            # Don't raise - allow app to continue if tables already exist
            logger.info("Tables may already exist, continuing...")
        
        # Initialize Redis connection (optional, for notifications pub/sub)
        try:
            import redis.asyncio as redis
            redis_client = await redis.from_url(
                settings.REDIS_URL,
                encoding="utf-8",
                decode_responses=True
            )
            notification_service.set_redis(redis_client)
            await notification_service.start_redis_listener()
            logger.info(" Redis connection initialized")
        except Exception as e:
            logger.warning(f"Redis not available (notifications will work locally only): {e}")
        
        logger.info(f"{settings.APP_NAME} startup complete")
        
    except Exception as e:
        logger.error(f"Failed to initialize application: {e}")
        raise
    
    yield
    
    # Shutdown
    logger.info("Shutting down application")
    
    try:
        # Stop notification service
        await notification_service.stop_redis_listener()
        logger.info(" Notification service stopped")
        
        # Close database connections
        await close_db()
        logger.info(" Database connections closed")
        
    except Exception as e:
        logger.error(f"Error during shutdown: {e}")
    
    logger.info("Shutdown complete")


# Create FastAPI application
app = FastAPI(
    title=settings.APP_NAME,
    version=settings.APP_VERSION,
    description="Confidential Data Steward Agent - Backend API",
    docs_url="/docs" if settings.is_development else None,
    redoc_url="/redoc" if settings.is_development else None,
    lifespan=lifespan,
)

# CORS middleware
app.add_middleware(
    CORSMiddleware,
    allow_origins=settings.CORS_ORIGINS,
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)


@app.get("/")
async def root():
    """Root endpoint."""
    return {
        "name": settings.APP_NAME,
        "version": settings.APP_VERSION,
        "environment": settings.ENVIRONMENT,
        "status": "running",
    }


@app.get("/health")
async def health_check():
    """Health check endpoint."""
    # TODO: Add actual health checks for database, redis, etc.
    return {
        "status": "healthy",
        "version": settings.APP_VERSION,
        "services": {
            "database": "pending",
            "redis": "pending",
            "llm": "pending",
        }
    }


# Exception handlers
@app.exception_handler(Exception)
async def general_exception_handler(request, exc):
    """Handle general exceptions."""
    logger.error(f"Unhandled exception: {exc}", exc_info=True)
    return JSONResponse(
        status_code=500,
        content={"detail": "Internal server error"}
    )


# Register API routers
from app.api.v1 import auth, chat, tools, audit, vault, documents, llm, notifications

app.include_router(auth.router, prefix="/api/v1/auth", tags=["Authentication"])
app.include_router(chat.router, prefix="/api/v1/chat", tags=["Chat"])
app.include_router(tools.router, prefix="/api/v1", tags=["Tools"])
app.include_router(audit.router, prefix="/api/v1", tags=["Audit"])
app.include_router(vault.router, prefix="/api/v1", tags=["Vault"])
app.include_router(documents.router, prefix="/api/v1", tags=["Documents"])
app.include_router(llm.router, prefix="/api/v1", tags=["LLM"])
app.include_router(notifications.router, prefix="/api/v1", tags=["Notifications"])


if __name__ == "__main__":
    import uvicorn
    uvicorn.run(
        "app.main:app",
        host=settings.API_HOST,
        port=settings.API_PORT,
        reload=settings.API_RELOAD,
    )
</file>

<file path="scripts/create_tables_direct.py">
#!/usr/bin/env python3
"""
Create database tables directly using SQLAlchemy.
Bypasses Alembic to avoid import issues during migration.
"""
import os
import sys
import asyncio
from pathlib import Path

# Add the project root directory to the Python path
sys.path.insert(0, str(Path(__file__).resolve().parent.parent))

# Set minimal environment to avoid service initialization
os.environ['SKIP_SERVICE_INIT'] = '1'

from app.db.base import Base, init_db
from app.config import settings

# Import all models to register them with Base.metadata
from app.models import (
    User, Role, Permission, Session,
    ChatSession, ChatMessage, ContextWindow,
    Tool, ToolExecution, ToolApproval, ToolCache,
    AuditLog, SystemMetric,
    Secret, SecretVersion, SecretAccessLog,
    Document, DocumentChunk, SearchResult, EmbeddingModel,
    Notification, NotificationPreference
)

async def create_tables():
    """Create all database tables."""
    print("=" * 60)
    print("CDSA Database Table Creation")
    print("=" * 60)
    print(f"\nDatabase URL: {settings.DATABASE_URL}")
    print(f"Environment: {settings.ENVIRONMENT}")
    print("\nImporting models...")
    
    # Get all registered models
    tables = Base.metadata.tables
    print(f" Found {len(tables)} models to create")
    
    print("\nCreating tables...")
    
    try:
        # Initialize database connection
        engine, session_factory = init_db()
        
        # Create all tables
        async with engine.begin() as conn:
            await conn.run_sync(Base.metadata.create_all)
        
        print("\n All tables created successfully!")
        
        # Verify tables were created
        async with engine.connect() as conn:
            result = await conn.execute(text("""
                SELECT tablename FROM pg_tables 
                WHERE schemaname = 'public' 
                ORDER BY tablename
            """))
            created_tables = [row[0] for row in result]
            
        print(f"\n Verified {len(created_tables)} tables in database:")
        for table in sorted(created_tables):
            print(f"  - {table}")
        
        await engine.dispose()
        
        print("\n" + "=" * 60)
        print("Database initialization complete!")
        print("=" * 60)
        return 0
        
    except Exception as e:
        print(f"\n Error creating tables: {e}")
        import traceback
        traceback.print_exc()
        return 1

if __name__ == "__main__":
    from sqlalchemy import text
    sys.exit(asyncio.run(create_tables()))
</file>

<file path="scripts/seed_data.py">
"""
Seed data script for initializing database with default roles, permissions, and admin user.

Run this script after database migrations to set up the initial system state.
"""
import asyncio
import sys
from pathlib import Path

# Add parent directory to path
sys.path.append(str(Path(__file__).resolve().parents[1]))

from sqlalchemy import select
from sqlalchemy.ext.asyncio import AsyncSession

from app.db.base import init_db
from app.models.user import User, Role, Permission
from app.core.security import get_password_hash
from app.config import settings


# Default Permissions
DEFAULT_PERMISSIONS = [
    # User Management
    {"name": "users:create", "description": "Create new users"},
    {"name": "users:read", "description": "View users"},
    {"name": "users:update", "description": "Update users"},
    {"name": "users:delete", "description": "Delete users"},
    
    # Role Management
    {"name": "roles:manage", "description": "Manage roles and permissions"},
    
    # Chat
    {"name": "chat:create", "description": "Create chat sessions"},
    {"name": "chat:read", "description": "View chat history"},
    {"name": "chat:delete", "description": "Delete chat sessions"},
    
    # Tools
    {"name": "tools:execute", "description": "Execute tools"},
    {"name": "tools:manage", "description": "Manage tool registry"},
    {"name": "tools:approve", "description": "Approve tool executions"},
    
    # Documents
    {"name": "documents:upload", "description": "Upload documents"},
    {"name": "documents:read", "description": "Read documents"},
    {"name": "documents:delete", "description": "Delete documents"},
    {"name": "documents:manage", "description": "Manage document settings"},
    
    # Secrets Vault
    {"name": "vault:read", "description": "Read secrets"},
    {"name": "vault:write", "description": "Create/update secrets"},
    {"name": "vault:delete", "description": "Delete secrets"},
    {"name": "vault:manage", "description": "Manage vault settings"},
    
    # Audit
    {"name": "audit:read", "description": "View audit logs"},
    {"name": "audit:export", "description": "Export audit logs"},
    
    # Notifications
    {"name": "notifications:read", "description": "View notifications"},
    {"name": "notifications:broadcast", "description": "Send broadcast notifications"},
    
    # LLM Gateway
    {"name": "llm:use", "description": "Use LLM services"},
    {"name": "llm:manage", "description": "Manage LLM settings"},
]

# Default Roles
DEFAULT_ROLES = {
    "ADMIN": {
        "description": "System administrator with full access",
        "permissions": [
            "users:create", "users:read", "users:update", "users:delete",
            "roles:manage",
            "chat:create", "chat:read", "chat:delete",
            "tools:execute", "tools:manage", "tools:approve",
            "documents:upload", "documents:read", "documents:delete", "documents:manage",
            "vault:read", "vault:write", "vault:delete", "vault:manage",
            "audit:read", "audit:export",
            "notifications:read", "notifications:broadcast",
            "llm:use", "llm:manage",
        ]
    },
    "MANAGER": {
        "description": "Manager with approval and oversight permissions",
        "permissions": [
            "users:read",
            "chat:create", "chat:read",
            "tools:execute", "tools:approve",
            "documents:upload", "documents:read",
            "vault:read", "vault:write",
            "audit:read",
            "notifications:read",
            "llm:use",
        ]
    },
    "ANALYST": {
        "description": "Standard analyst with core functionality access",
        "permissions": [
            "chat:create", "chat:read",
            "tools:execute",
            "documents:upload", "documents:read",
            "vault:read",
            "notifications:read",
            "llm:use",
        ]
    },
    "VIEWER": {
        "description": "Read-only access for viewing data",
        "permissions": [
            "chat:read",
            "documents:read",
            "audit:read",
            "notifications:read",
        ]
    }
}


async def create_permissions(db: AsyncSession) -> dict[str, Permission]:
    """Create default permissions."""
    print("Creating permissions...")
    permissions_map = {}
    
    for perm_data in DEFAULT_PERMISSIONS:
        # Check if permission exists
        result = await db.execute(
            select(Permission).where(Permission.name == perm_data["name"])
        )
        permission = result.scalar_one_or_none()
        
        if permission is None:
            # Parse resource and action from name (e.g., "users:create" -> resource="users", action="create")
            parts = perm_data["name"].split(":")
            resource = parts[0] if len(parts) > 0 else "unknown"
            action = parts[1] if len(parts) > 1 else "execute"
            
            permission = Permission(
                name=perm_data["name"],
                description=perm_data["description"],
                resource=resource,
                action=action
            )
            db.add(permission)
            print(f"   Created permission: {perm_data['name']}")
        else:
            print(f"   Permission already exists: {perm_data['name']}")
        
        permissions_map[perm_data["name"]] = permission
    
    await db.commit()
    return permissions_map


async def create_roles(db: AsyncSession, permissions_map: dict[str, Permission]) -> dict[str, Role]:
    """Create default roles and assign permissions."""
    print("\nCreating roles...")
    roles_map = {}
    
    for role_name, role_data in DEFAULT_ROLES.items():
        # Check if role exists
        result = await db.execute(
            select(Role).where(Role.name == role_name)
        )
        role = result.unique().scalar_one_or_none()
        
        if role is None:
            role = Role(
                name=role_name,
                description=role_data["description"]
            )
            db.add(role)
            print(f"   Created role: {role_name}")
        else:
            print(f"   Role already exists: {role_name}")
        
        # Assign permissions to role
        role.permissions = []
        for perm_name in role_data["permissions"]:
            if perm_name in permissions_map:
                role.permissions.append(permissions_map[perm_name])
        
        roles_map[role_name] = role
    
    await db.commit()
    return roles_map


async def create_admin_user(db: AsyncSession, roles_map: dict[str, Role]):
    """Create default admin user."""
    print("\nCreating admin user...")
    
    admin_email = "admin@cdsa.local"
    admin_username = "admin"
    
    # Check if admin user exists
    result = await db.execute(
        select(User).where(User.email == admin_email)
    )
    admin_user = result.scalar_one_or_none()
    
    if admin_user is None:
        # Create admin user
        admin_user = User(
            email=admin_email,
            username=admin_username,
            hashed_password=get_password_hash("admin123"),  # Default password
            full_name="System Administrator",
            is_active=True,
            is_superuser=True,
            is_verified=True
        )
        
        # Assign ADMIN role
        if "ADMIN" in roles_map:
            admin_user.roles.append(roles_map["ADMIN"])
        
        db.add(admin_user)
        await db.commit()
        
        print(f"   Created admin user: {admin_email}")
        print(f"    Username: {admin_username}")
        print(f"    Password: admin123")
        print(f"      IMPORTANT: Change this password immediately!")
    else:
        print(f"   Admin user already exists: {admin_email}")


async def seed_database():
    """Main function to seed the database."""
    print("=" * 60)
    print("CDSA Database Seeding")
    print("=" * 60)
    
    try:
        # Initialize database
        print("\nInitializing database connection...")
        engine, session_factory = init_db()
        print(" Database connection established")
        
        # Create session
        async with session_factory() as db:
            # Create permissions
            permissions_map = await create_permissions(db)
            
            # Create roles
            roles_map = await create_roles(db, permissions_map)
            
            # Create admin user
            await create_admin_user(db, roles_map)
        
        print("\n" + "=" * 60)
        print(" Database seeding completed successfully!")
        print("=" * 60)
        
        print("\n Summary:")
        print(f"   Permissions: {len(DEFAULT_PERMISSIONS)}")
        print(f"   Roles: {len(DEFAULT_ROLES)}")
        print(f"   Admin user: admin@cdsa.local")
        
        print("\n Default Credentials:")
        print("  Email: admin@cdsa.local")
        print("  Username: admin")
        print("  Password: admin123")
        print("\n    IMPORTANT: Change the admin password immediately!")
        
    except Exception as e:
        print(f"\n Error seeding database: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)


if __name__ == "__main__":
    asyncio.run(seed_database())
</file>

<file path="scripts/setup_database.sh">
#!/bin/bash

# CDSA Database Setup Script
# This script initializes the database, runs migrations, and seeds initial data

set -e  # Exit on error

echo "=========================================="
echo "CDSA Database Setup"
echo "=========================================="
echo ""

# Colors for output
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
NC='\033[0m' # No Color

# Check if .env file exists
if [ ! -f ".env" ]; then
    echo -e "${RED} Error: .env file not found${NC}"
    echo "Please create a .env file with your database configuration"
    exit 1
fi

# Load environment variables
export $(cat .env | grep -v '^#' | xargs)

echo " Configuration:"
echo "  Database: $DATABASE_URL"
echo ""

# Step 1: Check database connection
echo "1  Checking database connection..."
if python -c "
import psycopg2
from urllib.parse import urlparse
url = urlparse('$DATABASE_URL')
try:
    conn = psycopg2.connect(
        dbname=url.path[1:],
        user=url.username,
        password=url.password,
        host=url.hostname,
        port=url.port
    )
    conn.close()
    print(' Database connection successful')
except Exception as e:
    print(f' Database connection failed: {e}')
    exit(1)
" 2>/dev/null; then
    echo -e "${GREEN} Database connection verified${NC}"
else
    echo -e "${RED} Database connection failed${NC}"
    echo "Please check your DATABASE_URL in .env file"
    exit 1
fi

echo ""

# Step 2: Run Alembic migrations
echo "2  Running database migrations..."
if alembic upgrade head; then
    echo -e "${GREEN} Migrations completed successfully${NC}"
else
    echo -e "${RED} Migration failed${NC}"
    exit 1
fi

echo ""

# Step 3: Seed initial data
echo "3  Seeding initial data..."
if python scripts/seed_data.py; then
    echo -e "${GREEN} Data seeding completed successfully${NC}"
else
    echo -e "${RED} Data seeding failed${NC}"
    exit 1
fi

echo ""
echo "=========================================="
echo -e "${GREEN} Database setup completed!${NC}"
echo "=========================================="
echo ""
echo " Next steps:"
echo "  1. Start the backend server: ./run.sh"
echo "  2. Access API docs: http://localhost:8000/docs"
echo "  3. Login with default admin credentials:"
echo "     Email: admin@cdsa.local"
echo "     Password: admin123"
echo ""
echo -e "${YELLOW}  IMPORTANT: Change the admin password immediately!${NC}"
echo ""
</file>

<file path="scripts/validate_setup.py">
#!/usr/bin/env python3
"""Validate backend setup and configuration."""
import sys
from pathlib import Path

# Add app to path
sys.path.insert(0, str(Path(__file__).parent.parent))

def test_imports():
    """Test that all modules can be imported."""
    print("Testing imports...")
    try:
        from app.config import settings
        print(" Config module imported successfully")
        
        from app.main import app
        print(" Main app module imported successfully")
        
        from app.core.logging import setup_logging
        print(" Logging module imported successfully")
        
        return True
    except Exception as e:
        print(f" Import failed: {e}")
        return False

def test_config():
    """Test configuration."""
    print("\nTesting configuration...")
    try:
        from app.config import settings
        
        print(f"  App Name: {settings.APP_NAME}")
        print(f"  Version: {settings.APP_VERSION}")
        print(f"  Environment: {settings.ENVIRONMENT}")
        print(f"  API Port: {settings.API_PORT}")
        print(f"  Debug Mode: {settings.DEBUG}")
        
        # Check required settings
        assert settings.APP_NAME, "APP_NAME not set"
        assert settings.DATABASE_URL, "DATABASE_URL not set"
        assert settings.REDIS_URL, "REDIS_URL not set"
        
        print(" Configuration validated successfully")
        return True
    except Exception as e:
        print(f" Configuration validation failed: {e}")
        return False

def test_app_structure():
    """Test that all required directories exist."""
    print("\nTesting project structure...")
    
    required_dirs = [
        "app",
        "app/api",
        "app/api/v1",
        "app/core",
        "app/models",
        "app/schemas",
        "app/services",
        "app/tools",
        "app/db",
        "app/middleware",
        "app/utils",
        "tests",
        "tests/unit",
        "tests/integration",
        "tests/e2e",
        "scripts",
        "alembic",
        "alembic/versions",
    ]
    
    base_path = Path(__file__).parent.parent
    missing = []
    
    for dir_path in required_dirs:
        full_path = base_path / dir_path
        if not full_path.exists():
            missing.append(dir_path)
            print(f"   Missing: {dir_path}")
        else:
            print(f"   Found: {dir_path}")
    
    if missing:
        print(f"\n Missing {len(missing)} directories")
        return False
    
    print("\n All required directories exist")
    return True

def test_files():
    """Test that all required files exist."""
    print("\nTesting required files...")
    
    required_files = [
        "pyproject.toml",
        "docker-compose.yml",
        "Dockerfile",
        ".env.example",
        ".gitignore",
        "README.md",
        "app/__init__.py",
        "app/main.py",
        "app/config.py",
        "app/core/logging.py",
    ]
    
    base_path = Path(__file__).parent.parent
    missing = []
    
    for file_path in required_files:
        full_path = base_path / file_path
        if not full_path.exists():
            missing.append(file_path)
            print(f"   Missing: {file_path}")
        else:
            print(f"   Found: {file_path}")
    
    if missing:
        print(f"\n Missing {len(missing)} files")
        return False
    
    print("\n All required files exist")
    return True

def main():
    """Run all validation tests."""
    print("=" * 60)
    print("CDSA Backend Setup Validation")
    print("=" * 60)
    
    tests = [
        ("Project Structure", test_app_structure),
        ("Required Files", test_files),
        ("Python Imports", test_imports),
        ("Configuration", test_config),
    ]
    
    results = []
    for name, test_func in tests:
        print(f"\n{'=' * 60}")
        print(f"Running: {name}")
        print("=" * 60)
        results.append(test_func())
    
    print("\n" + "=" * 60)
    print("VALIDATION SUMMARY")
    print("=" * 60)
    
    for i, (name, _) in enumerate(tests):
        status = " PASSED" if results[i] else " FAILED"
        print(f"{status}: {name}")
    
    if all(results):
        print("\n All validation tests passed!")
        print("\nNext steps:")
        print("1. Start Docker services: docker-compose up -d")
        print("2. Run the API: uvicorn app.main:app --reload")
        print("3. Access docs: http://localhost:8000/docs")
        return 0
    else:
        print("\n Some validation tests failed. Please fix the issues above.")
        return 1

if __name__ == "__main__":
    sys.exit(main())
</file>

<file path=".env.example">
# Application
APP_NAME="CDSA Backend"
APP_VERSION="1.0.0"
ENVIRONMENT="development"
DEBUG=true
LOG_LEVEL="INFO"

# API
API_HOST="0.0.0.0"
API_PORT=8000
API_RELOAD=true
CORS_ORIGINS="http://localhost:3000,http://localhost:5173"

# Database
DATABASE_URL="postgresql+asyncpg://cdsa:changeme@postgres:5432/cdsa_db"
DB_POOL_SIZE=20
DB_MAX_OVERFLOW=0
DB_ECHO=false

# Redis
REDIS_URL="redis://redis:6379/0"
REDIS_CACHE_DB=1
REDIS_SESSION_DB=2
REDIS_NOTIFICATION_DB=4

# Security
SECRET_KEY="your-secret-key-here-change-in-production-min-32-chars"
JWT_SECRET_KEY="your-jwt-secret-key-change-in-production"
JWT_ALGORITHM="HS256"
JWT_EXPIRATION_MINUTES=60
ENCRYPTION_KEY="your-encryption-key-base64-encoded-change-me"

# Rate Limiting
RATE_LIMIT_REQUESTS=100
RATE_LIMIT_PERIOD=60

# LLM Configuration
# OpenAI
OPENAI_API_KEY=""
OPENAI_MODEL="gpt-4-turbo-preview"
OPENAI_TEMPERATURE=0.7

# Anthropic
ANTHROPIC_API_KEY=""
ANTHROPIC_MODEL="claude-3-opus-20240229"

# Google (Gemini)
GEMINI_API_KEY=""

# Local Models (Ollama)
OLLAMA_BASE_URL="http://ollama:11434"
OLLAMA_DEFAULT_MODEL="llama3:8b"

# Embeddings
EMBEDDING_MODEL="text-embedding-3-small"
EMBEDDING_DIMENSIONS=1536

# RAG Configuration
RAG_CHUNK_SIZE=1000
RAG_CHUNK_OVERLAP=200
RAG_TOP_K=5

# Celery
CELERY_BROKER_URL="redis://redis:6379/3"
CELERY_RESULT_BACKEND="redis://redis:6379/3"

# Context Window
MAX_CONTEXT_WINDOW=4096
CONTEXT_PRUNING_THRESHOLD=3500

# Notifications
NOTIFICATION_RETENTION_DAYS=30
NOTIFICATION_BATCH_SIZE=100

# File Storage
UPLOAD_DIR="./uploads"
MAX_UPLOAD_SIZE=10485760

# Monitoring (Optional)
SENTRY_DSN=""
ENABLE_METRICS=true
</file>

<file path=".gitignore">
# Python
__pycache__/
*.py[cod]
*$py.class
*.so
.Python
build/
develop-eggs/
dist/
downloads/
eggs/
.eggs/
lib/
lib64/
parts/
sdist/
var/
wheels/
*.egg-info/
.installed.cfg
*.egg
MANIFEST

# Virtual Environment
venv/
ENV/
env/
.venv

# IDEs
.vscode/
.idea/
*.swp
*.swo
*~
.DS_Store

# Environment variables
.env
.env.local
.env.*.local

# Testing
.pytest_cache/
.coverage
htmlcov/
.tox/
.hypothesis/

# Logs
*.log
logs/

# Database
*.db
*.sqlite
*.sqlite3

# Uploads
uploads/
temp/

# Alembic
alembic/versions/*.pyc

# Documentation
site/
docs/_build/

# Distribution
*.tar.gz
*.whl

# Jupyter Notebook
.ipynb_checkpoints

# mypy
.mypy_cache/
.dmypy.json
dmypy.json

# Celery
celerybeat-schedule
celerybeat.pid

# UV
.uv/
uv.lock
</file>

<file path=".wnv">
# Application
APP_NAME="CDSA Backend"
APP_VERSION="1.0.0"
ENVIRONMENT="development"
DEBUG=true
LOG_LEVEL="INFO"

# API
API_HOST="0.0.0.0"
API_PORT=8000
API_RELOAD=true
CORS_ORIGINS="http://localhost:3000,http://localhost:5173"

# Database
DATABASE_URL="postgresql+asyncpg://cdsa:changeme@postgres:5432/cdsa_db"
DB_POOL_SIZE=20
DB_MAX_OVERFLOW=0
DB_ECHO=false

# Redis
REDIS_URL="redis://redis:6379/0"
REDIS_CACHE_DB=1
REDIS_SESSION_DB=2
REDIS_NOTIFICATION_DB=4

# Security
SECRET_KEY="your-secret-key-here-change-in-production-min-32-chars"
JWT_SECRET_KEY="your-jwt-secret-key-change-in-production"
JWT_ALGORITHM="HS256"
JWT_EXPIRATION_MINUTES=60
ENCRYPTION_KEY="your-encryption-key-base64-encoded-change-me"

# Rate Limiting
RATE_LIMIT_REQUESTS=100
RATE_LIMIT_PERIOD=60

# LLM Configuration
# OpenAI
OPENAI_API_KEY=""
OPENAI_MODEL="gpt-4-turbo-preview"
OPENAI_TEMPERATURE=0.7

# Anthropic
ANTHROPIC_API_KEY=""
ANTHROPIC_MODEL="claude-3-opus-20240229"

# Google (Gemini)
GEMINI_API_KEY=""

# Local Models (Ollama)
OLLAMA_BASE_URL="http://ollama:11434"
OLLAMA_DEFAULT_MODEL="llama3:8b"

# Embeddings
EMBEDDING_MODEL="text-embedding-3-small"
EMBEDDING_DIMENSIONS=1536

# RAG Configuration
RAG_CHUNK_SIZE=1000
RAG_CHUNK_OVERLAP=200
RAG_TOP_K=5

# Celery
CELERY_BROKER_URL="redis://redis:6379/3"
CELERY_RESULT_BACKEND="redis://redis:6379/3"

# Context Window
MAX_CONTEXT_WINDOW=4096
CONTEXT_PRUNING_THRESHOLD=3500

# Notifications
NOTIFICATION_RETENTION_DAYS=30
NOTIFICATION_BATCH_SIZE=100

# File Storage
UPLOAD_DIR="./uploads"
MAX_UPLOAD_SIZE=10485760

# Monitoring (Optional)
SENTRY_DSN=""
ENABLE_METRICS=true
</file>

<file path="alembic.ini">
# Alembic configuration file

[alembic]
# Path to migration scripts
script_location = alembic

# Template used to generate migration files
file_template = %%(year)d%%(month).2d%%(day).2d_%%(hour).2d%%(minute).2d_%%(rev)s_%%(slug)s

# Timezone to use when rendering the date within the migration file
# as well as the filename. If specified, requires the python-dateutil library
# that can be installed by adding `alembic[tz]` to the pip requirements
# string value is passed to dateutil.tz.gettz()
# timezone =

# Max length of characters to apply to the "slug" field
truncate_slug_length = 40

# Set to 'true' to run the environment during
# the 'revision' command, regardless of autogenerate
# revision_environment = false

# Set to 'true' to allow .pyc and .pyo files without
# a source .py file to be detected as revisions in the versions/ directory
# sourceless = false

# Version location specification; This defaults to alembic/versions
# When providing multiple version locations, in the form of a list of
# version_locations = %(here)s/bar %(here)s/bat alembic/versions

# Version path separator; As mentioned above, this is the character used to split
# version_locations = %(here)s/foo:%(here)s/bar:alembic/versions

# The output encoding used when revision files
# are written from script.py.mako
# output_encoding = utf-8

# Database URL - loaded from environment variable
sqlalchemy.url = driver://user:pass@localhost/dbname

[post_write_hooks]
# Post-write hooks allow you to define additional actions to be taken
# after a migration file has been generated
# hooks = black
# black.type = console_scripts
# black.entrypoint = black

# Logging configuration
[loggers]
keys = root,sqlalchemy,alembic

[handlers]
keys = console

[formatters]
keys = generic

[logger_root]
level = WARN
handlers = console
qualname =

[logger_sqlalchemy]
level = WARN
handlers =
qualname = sqlalchemy.engine

[logger_alembic]
level = INFO
handlers =
qualname = alembic

[handler_console]
class = StreamHandler
args = (sys.stderr,)
level = NOTSET
formatter = generic

[formatter_generic]
format = %(levelname)-5.5s [%(name)s] %(message)s
datefmt = %H:%M:%S
</file>

<file path="create_tables.py">
#!/usr/bin/env python3
"""Direct table creation script - bypasses alembic model imports."""
import sys
from pathlib import Path

# Add app to path
sys.path.insert(0, str(Path(__file__).parent))

from sqlalchemy import create_engine, text
from app.config import settings

# Use sync psycopg2 driver
database_url = settings.DATABASE_URL.replace("postgresql+asyncpg://", "postgresql://")

print(f"Connecting to database: {database_url}")

engine = create_engine(database_url, echo=True)

# Read and execute the migration SQL
migration_file = Path(__file__).parent / "alembic" / "versions" / "001_initial_schema.py"

print(f"\nReading migration from: {migration_file}")

# Import the migration module
import importlib.util
spec = importlib.util.spec_from_file_location("migration", migration_file)
migration = importlib.util.module_from_spec(spec)

# Execute upgrade function
print("\nExecuting database migrations...")

try:
    # Import necessary alembic components
    from alembic import op
    from alembic.runtime.migration import MigrationContext
    from alembic.operations import Operations
    
    with engine.begin() as connection:
        ctx = MigrationContext.configure(connection)
        op_obj = Operations(ctx)
        
        # Load the migration module properly
        spec.loader.exec_module(migration)
        
        # Execute the upgrade
        migration.upgrade()
        
    print("\n Database tables created successfully!")
    
    # Verify tables
    with engine.connect() as conn:
        result = conn.execute(text("""
            SELECT tablename FROM pg_tables 
            WHERE schemaname = 'public' 
            ORDER BY tablename
        """))
        tables = [row[0] for row in result]
        print(f"\n Created {len(tables)} tables:")
        for table in tables:
            print(f"  - {table}")
            
except Exception as e:
    print(f"\n Error creating tables: {e}")
    import traceback
    traceback.print_exc()
    sys.exit(1)
</file>

<file path="docker-compose.yml">
version: '3.8'

services:
  # PostgreSQL Database with pgvector
  postgres:
    image: pgvector/pgvector:pg16
    container_name: cdsa-postgres
    environment:
      POSTGRES_DB: cdsa_db
      POSTGRES_USER: cdsa
      POSTGRES_PASSWORD: changeme
    volumes:
      - postgres_data:/var/lib/postgresql/data
    ports:
      - "5433:5432"
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U cdsa"]
      interval: 10s
      timeout: 5s
      retries: 5
    networks:
      - cdsa-network

  # Redis Cache
  redis:
    image: redis:7-alpine
    container_name: cdsa-redis
    command: redis-server --appendonly yes
    volumes:
      - redis_data:/data
    ports:
      - "6379:6379"
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 5s
      retries: 5
    networks:
      - cdsa-network

  # FastAPI Application
  api:
    build: .
    container_name: cdsa-api
    command: uvicorn app.main:app --host 0.0.0.0 --port 8000 --reload
    environment:
      DATABASE_URL: postgresql+asyncpg://cdsa:changeme@postgres:5432/cdsa_db
      REDIS_URL: redis://redis:6379/0
    env_file:
      - .env
    volumes:
      - ./app:/app/app
      - ./uploads:/app/uploads
    ports:
      - "8000:8000"
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
    networks:
      - cdsa-network

  # Celery Worker
  celery_worker:
    build: .
    container_name: cdsa-celery-worker
    command: celery -A app.tasks worker --loglevel=info
    environment:
      DATABASE_URL: postgresql+asyncpg://cdsa:changeme@postgres:5432/cdsa_db
      REDIS_URL: redis://redis:6379/0
    env_file:
      - .env
    depends_on:
      - postgres
      - redis
    networks:
      - cdsa-network

  # Ollama (Local LLM) - Optional
  ollama:
    image: ollama/ollama:latest
    container_name: cdsa-ollama
    volumes:
      - ollama_data:/root/.ollama
    ports:
      - "11434:11434"
    # Uncomment for GPU support
    # deploy:
    #   resources:
    #     reservations:
    #       devices:
    #         - driver: nvidia
    #           count: 1
    #           capabilities: [gpu]
    networks:
      - cdsa-network

volumes:
  postgres_data:
  redis_data:
  ollama_data:

networks:
  cdsa-network:
    driver: bridge
</file>

<file path="Dockerfile">
FROM python:3.11-slim

WORKDIR /app

# Install system dependencies
RUN apt-get update && apt-get install -y \
    gcc \
    g++ \
    postgresql-client \
    curl \
    && rm -rf /var/lib/apt/lists/*

# Install uv
RUN curl -LsSf https://astral.sh/uv/install.sh | sh
ENV PATH="/root/.cargo/bin:$PATH"

# Copy project files
COPY pyproject.toml ./

# Install Python dependencies using uv
RUN uv pip install --system --no-cache -e .

# Copy application code
COPY ./app ./app
COPY ./alembic ./alembic
COPY ./alembic.ini ./

# Create non-root user
RUN useradd -m -u 1000 cdsa && chown -R cdsa:cdsa /app
USER cdsa

# Expose port
EXPOSE 8000

# Health check
HEALTHCHECK --interval=30s --timeout=10s --start-period=40s --retries=3 \
    CMD curl -f http://localhost:8000/health || exit 1

# Run migrations and start server
CMD ["sh", "-c", "alembic upgrade head && uvicorn app.main:app --host 0.0.0.0 --port 8000"]
</file>

<file path="migration.sql">
INFO  [alembic.runtime.migration] Context impl PostgresqlImpl.
INFO  [alembic.runtime.migration] Generating static SQL
INFO  [alembic.runtime.migration] Will assume transactional DDL.
BEGIN;

CREATE TABLE alembic_version (
    version_num VARCHAR(32) NOT NULL, 
    CONSTRAINT alembic_version_pkc PRIMARY KEY (version_num)
);

INFO  [alembic.runtime.migration] Running upgrade  -> 001, initial schema
-- Running upgrade  -> 001

CREATE TABLE roles (
    id SERIAL NOT NULL, 
    name VARCHAR(100) NOT NULL, 
    description VARCHAR(500), 
    created_at TIMESTAMP WITHOUT TIME ZONE NOT NULL, 
    updated_at TIMESTAMP WITHOUT TIME ZONE NOT NULL, 
    PRIMARY KEY (id), 
    UNIQUE (name)
);

CREATE INDEX ix_roles_id ON roles (id);

CREATE TABLE permissions (
    id SERIAL NOT NULL, 
    name VARCHAR(100) NOT NULL, 
    description VARCHAR(500), 
    created_at TIMESTAMP WITHOUT TIME ZONE NOT NULL, 
    PRIMARY KEY (id), 
    UNIQUE (name)
);

CREATE INDEX ix_permissions_id ON permissions (id);

CREATE TABLE users (
    id SERIAL NOT NULL, 
    email VARCHAR(255) NOT NULL, 
    username VARCHAR(100) NOT NULL, 
    hashed_password VARCHAR(255) NOT NULL, 
    full_name VARCHAR(255), 
    is_active BOOLEAN NOT NULL, 
    is_superuser BOOLEAN NOT NULL, 
    is_verified BOOLEAN NOT NULL, 
    created_at TIMESTAMP WITHOUT TIME ZONE NOT NULL, 
    updated_at TIMESTAMP WITHOUT TIME ZONE NOT NULL, 
    last_login TIMESTAMP WITHOUT TIME ZONE, 
    PRIMARY KEY (id)
);

CREATE UNIQUE INDEX ix_users_email ON users (email);

CREATE INDEX ix_users_id ON users (id);

CREATE UNIQUE INDEX ix_users_username ON users (username);

CREATE TABLE user_roles (
    user_id INTEGER NOT NULL, 
    role_id INTEGER NOT NULL, 
    PRIMARY KEY (user_id, role_id), 
    FOREIGN KEY(role_id) REFERENCES roles (id) ON DELETE CASCADE, 
    FOREIGN KEY(user_id) REFERENCES users (id) ON DELETE CASCADE
);

CREATE TABLE role_permissions (
    role_id INTEGER NOT NULL, 
    permission_id INTEGER NOT NULL, 
    PRIMARY KEY (role_id, permission_id), 
    FOREIGN KEY(permission_id) REFERENCES permissions (id) ON DELETE CASCADE, 
    FOREIGN KEY(role_id) REFERENCES roles (id) ON DELETE CASCADE
);

CREATE TABLE sessions (
    id SERIAL NOT NULL, 
    user_id INTEGER NOT NULL, 
    token VARCHAR(500) NOT NULL, 
    refresh_token VARCHAR(500), 
    ip_address VARCHAR(50), 
    user_agent VARCHAR(500), 
    is_active BOOLEAN NOT NULL, 
    expires_at TIMESTAMP WITHOUT TIME ZONE NOT NULL, 
    created_at TIMESTAMP WITHOUT TIME ZONE NOT NULL, 
    last_activity TIMESTAMP WITHOUT TIME ZONE NOT NULL, 
    PRIMARY KEY (id), 
    FOREIGN KEY(user_id) REFERENCES users (id) ON DELETE CASCADE
);

CREATE INDEX ix_sessions_id ON sessions (id);

CREATE UNIQUE INDEX ix_sessions_token ON sessions (token);

CREATE INDEX ix_sessions_user_active ON sessions (user_id, is_active);

CREATE TABLE chat_sessions (
    id SERIAL NOT NULL, 
    user_id INTEGER NOT NULL, 
    title VARCHAR(500) NOT NULL, 
    model VARCHAR(100), 
    temperature VARCHAR(10) NOT NULL, 
    context_window_size INTEGER NOT NULL, 
    is_active BOOLEAN NOT NULL, 
    created_at TIMESTAMP WITHOUT TIME ZONE NOT NULL, 
    updated_at TIMESTAMP WITHOUT TIME ZONE NOT NULL, 
    last_message_at TIMESTAMP WITHOUT TIME ZONE NOT NULL, 
    PRIMARY KEY (id), 
    FOREIGN KEY(user_id) REFERENCES users (id) ON DELETE CASCADE
);

CREATE INDEX ix_chat_sessions_id ON chat_sessions (id);

CREATE INDEX ix_chat_sessions_user_active ON chat_sessions (user_id, is_active);

CREATE TABLE chat_messages (
    id SERIAL NOT NULL, 
    session_id INTEGER NOT NULL, 
    user_id INTEGER NOT NULL, 
    role VARCHAR(20) NOT NULL, 
    content TEXT NOT NULL, 
    tokens INTEGER, 
    model VARCHAR(100), 
    meta_data JSON, 
    tool_execution_id INTEGER, 
    created_at TIMESTAMP WITHOUT TIME ZONE NOT NULL, 
    PRIMARY KEY (id), 
    FOREIGN KEY(session_id) REFERENCES chat_sessions (id) ON DELETE CASCADE, 
    FOREIGN KEY(user_id) REFERENCES users (id) ON DELETE CASCADE
);

CREATE INDEX ix_chat_messages_id ON chat_messages (id);

CREATE INDEX ix_chat_messages_session ON chat_messages (session_id, created_at);

CREATE TABLE context_windows (
    id SERIAL NOT NULL, 
    session_id INTEGER NOT NULL, 
    max_tokens INTEGER NOT NULL, 
    current_tokens INTEGER NOT NULL, 
    message_count INTEGER NOT NULL, 
    strategy VARCHAR(50) NOT NULL, 
    updated_at TIMESTAMP WITHOUT TIME ZONE NOT NULL, 
    PRIMARY KEY (id), 
    FOREIGN KEY(session_id) REFERENCES chat_sessions (id) ON DELETE CASCADE
);

CREATE INDEX ix_context_windows_id ON context_windows (id);

CREATE UNIQUE INDEX ix_context_windows_session_id ON context_windows (session_id);

CREATE TABLE tools (
    id SERIAL NOT NULL, 
    name VARCHAR(100) NOT NULL, 
    display_name VARCHAR(200) NOT NULL, 
    description TEXT NOT NULL, 
    category VARCHAR(50) NOT NULL, 
    version VARCHAR(20) NOT NULL, 
    python_function VARCHAR(200) NOT NULL, 
    parameters_schema JSON NOT NULL, 
    return_schema JSON, 
    status VARCHAR(20) NOT NULL, 
    requires_approval BOOLEAN NOT NULL, 
    required_permission VARCHAR(100), 
    timeout_seconds INTEGER NOT NULL, 
    max_retries INTEGER NOT NULL, 
    execution_count INTEGER NOT NULL, 
    success_count INTEGER NOT NULL, 
    failure_count INTEGER NOT NULL, 
    avg_execution_time FLOAT, 
    last_executed_at TIMESTAMP WITHOUT TIME ZONE, 
    created_at TIMESTAMP WITHOUT TIME ZONE NOT NULL, 
    updated_at TIMESTAMP WITHOUT TIME ZONE NOT NULL, 
    PRIMARY KEY (id)
);

CREATE INDEX ix_tools_id ON tools (id);

CREATE UNIQUE INDEX ix_tools_name ON tools (name);

CREATE INDEX ix_tools_category_status ON tools (category, status);

CREATE TABLE tool_executions (
    id SERIAL NOT NULL, 
    tool_id INTEGER NOT NULL, 
    user_id INTEGER NOT NULL, 
    session_id INTEGER, 
    status VARCHAR(20) NOT NULL, 
    input_data JSON NOT NULL, 
    output_data JSON, 
    error_message TEXT, 
    started_at TIMESTAMP WITHOUT TIME ZONE, 
    completed_at TIMESTAMP WITHOUT TIME ZONE, 
    execution_time FLOAT, 
    retry_count INTEGER NOT NULL, 
    requires_approval BOOLEAN NOT NULL, 
    created_at TIMESTAMP WITHOUT TIME ZONE NOT NULL, 
    PRIMARY KEY (id), 
    FOREIGN KEY(session_id) REFERENCES chat_sessions (id) ON DELETE SET NULL, 
    FOREIGN KEY(tool_id) REFERENCES tools (id) ON DELETE CASCADE, 
    FOREIGN KEY(user_id) REFERENCES users (id) ON DELETE CASCADE
);

CREATE INDEX ix_tool_executions_id ON tool_executions (id);

CREATE INDEX ix_tool_executions_tool_status ON tool_executions (tool_id, status);

CREATE INDEX ix_tool_executions_user_created ON tool_executions (user_id, created_at);

CREATE TABLE tool_approvals (
    id SERIAL NOT NULL, 
    execution_id INTEGER NOT NULL, 
    user_id INTEGER NOT NULL, 
    status VARCHAR(20) NOT NULL, 
    reason TEXT, 
    notes TEXT, 
    requested_at TIMESTAMP WITHOUT TIME ZONE NOT NULL, 
    responded_at TIMESTAMP WITHOUT TIME ZONE, 
    expires_at TIMESTAMP WITHOUT TIME ZONE, 
    PRIMARY KEY (id), 
    FOREIGN KEY(execution_id) REFERENCES tool_executions (id) ON DELETE CASCADE, 
    FOREIGN KEY(user_id) REFERENCES users (id) ON DELETE CASCADE
);

CREATE INDEX ix_tool_approvals_id ON tool_approvals (id);

CREATE INDEX ix_tool_approvals_status_requested ON tool_approvals (status, requested_at);

CREATE INDEX ix_tool_approvals_user_status ON tool_approvals (user_id, status);

CREATE TABLE tool_cache (
    id SERIAL NOT NULL, 
    tool_id INTEGER NOT NULL, 
    input_hash VARCHAR(64) NOT NULL, 
    input_data JSON NOT NULL, 
    output_data JSON NOT NULL, 
    execution_time FLOAT NOT NULL, 
    hit_count INTEGER NOT NULL, 
    created_at TIMESTAMP WITHOUT TIME ZONE NOT NULL, 
    last_accessed TIMESTAMP WITHOUT TIME ZONE NOT NULL, 
    expires_at TIMESTAMP WITHOUT TIME ZONE, 
    PRIMARY KEY (id), 
    FOREIGN KEY(tool_id) REFERENCES tools (id) ON DELETE CASCADE
);

CREATE INDEX ix_tool_cache_id ON tool_cache (id);

CREATE UNIQUE INDEX ix_tool_cache_tool_hash ON tool_cache (tool_id, input_hash);

CREATE TABLE documents (
    id SERIAL NOT NULL, 
    title VARCHAR(500) NOT NULL, 
    source VARCHAR(1000) NOT NULL, 
    source_type VARCHAR(50) NOT NULL, 
    file_type VARCHAR(50), 
    file_size INTEGER, 
    tags VARCHAR[], 
    meta_data JSON, 
    is_processed BOOLEAN NOT NULL, 
    is_indexed BOOLEAN NOT NULL, 
    processing_error TEXT, 
    uploaded_by INTEGER, 
    is_public BOOLEAN NOT NULL, 
    required_permission VARCHAR(100), 
    created_at TIMESTAMP WITHOUT TIME ZONE NOT NULL, 
    updated_at TIMESTAMP WITHOUT TIME ZONE NOT NULL, 
    last_accessed TIMESTAMP WITHOUT TIME ZONE, 
    PRIMARY KEY (id), 
    FOREIGN KEY(uploaded_by) REFERENCES users (id) ON DELETE SET NULL
);

CREATE INDEX ix_documents_id ON documents (id);

CREATE INDEX ix_documents_indexed_public ON documents (is_indexed, is_public);

CREATE INDEX ix_documents_source_type ON documents (source_type, created_at);

CREATE TABLE embedding_models (
    id SERIAL NOT NULL, 
    name VARCHAR(100) NOT NULL, 
    display_name VARCHAR(200) NOT NULL, 
    provider VARCHAR(50) NOT NULL, 
    model_id VARCHAR(200) NOT NULL, 
    dimension INTEGER NOT NULL, 
    max_tokens INTEGER NOT NULL, 
    cost_per_1k_tokens FLOAT, 
    is_active BOOLEAN NOT NULL, 
    is_default BOOLEAN NOT NULL, 
    avg_latency_ms FLOAT, 
    created_at TIMESTAMP WITHOUT TIME ZONE NOT NULL, 
    PRIMARY KEY (id)
);

CREATE INDEX ix_embedding_models_id ON embedding_models (id);

CREATE UNIQUE INDEX ix_embedding_models_name ON embedding_models (name);

CREATE TABLE document_chunks (
    id SERIAL NOT NULL, 
    document_id INTEGER NOT NULL, 
    chunk_index INTEGER NOT NULL, 
    content TEXT NOT NULL, 
    embedding FLOAT[], 
    embedding_model_id INTEGER, 
    token_count INTEGER, 
    char_count INTEGER, 
    meta_data JSON, 
    search_keywords VARCHAR[], 
    created_at TIMESTAMP WITHOUT TIME ZONE NOT NULL, 
    PRIMARY KEY (id), 
    FOREIGN KEY(document_id) REFERENCES documents (id) ON DELETE CASCADE, 
    FOREIGN KEY(embedding_model_id) REFERENCES embedding_models (id) ON DELETE SET NULL
);

CREATE INDEX ix_document_chunks_id ON document_chunks (id);

CREATE UNIQUE INDEX ix_document_chunks_doc_index ON document_chunks (document_id, chunk_index);

CREATE TABLE search_results (
    id SERIAL NOT NULL, 
    chunk_id INTEGER NOT NULL, 
    query TEXT NOT NULL, 
    similarity_score FLOAT NOT NULL, 
    search_type VARCHAR(50) NOT NULL, 
    user_id INTEGER, 
    created_at TIMESTAMP WITHOUT TIME ZONE NOT NULL, 
    PRIMARY KEY (id), 
    FOREIGN KEY(chunk_id) REFERENCES document_chunks (id) ON DELETE CASCADE, 
    FOREIGN KEY(user_id) REFERENCES users (id) ON DELETE SET NULL
);

CREATE INDEX ix_search_results_id ON search_results (id);

CREATE INDEX ix_search_results_user_created ON search_results (user_id, created_at);

CREATE TABLE secrets (
    id SERIAL NOT NULL, 
    name VARCHAR(200) NOT NULL, 
    description TEXT, 
    secret_type VARCHAR(50) NOT NULL, 
    encrypted_value TEXT NOT NULL, 
    encryption_key_id VARCHAR(100) NOT NULL, 
    version INTEGER NOT NULL, 
    is_active BOOLEAN NOT NULL, 
    created_by INTEGER NOT NULL, 
    tags VARCHAR[], 
    meta_data JSON, 
    rotation_enabled BOOLEAN NOT NULL, 
    rotation_interval_days INTEGER, 
    last_rotated_at TIMESTAMP WITHOUT TIME ZONE, 
    next_rotation_at TIMESTAMP WITHOUT TIME ZONE, 
    expires_at TIMESTAMP WITHOUT TIME ZONE, 
    created_at TIMESTAMP WITHOUT TIME ZONE NOT NULL, 
    updated_at TIMESTAMP WITHOUT TIME ZONE NOT NULL, 
    PRIMARY KEY (id), 
    FOREIGN KEY(created_by) REFERENCES users (id) ON DELETE RESTRICT
);

CREATE INDEX ix_secrets_id ON secrets (id);

CREATE UNIQUE INDEX ix_secrets_name ON secrets (name);

CREATE INDEX ix_secrets_active_type ON secrets (is_active, secret_type);

CREATE TABLE secret_versions (
    id SERIAL NOT NULL, 
    secret_id INTEGER NOT NULL, 
    version INTEGER NOT NULL, 
    encrypted_value TEXT NOT NULL, 
    encryption_key_id VARCHAR(100) NOT NULL, 
    is_active BOOLEAN NOT NULL, 
    created_by INTEGER NOT NULL, 
    created_at TIMESTAMP WITHOUT TIME ZONE NOT NULL, 
    expires_at TIMESTAMP WITHOUT TIME ZONE, 
    PRIMARY KEY (id), 
    FOREIGN KEY(created_by) REFERENCES users (id) ON DELETE RESTRICT, 
    FOREIGN KEY(secret_id) REFERENCES secrets (id) ON DELETE CASCADE
);

CREATE INDEX ix_secret_versions_id ON secret_versions (id);

CREATE UNIQUE INDEX ix_secret_versions_secret_version ON secret_versions (secret_id, version);

CREATE TABLE secret_access_logs (
    id SERIAL NOT NULL, 
    secret_id INTEGER NOT NULL, 
    user_id INTEGER NOT NULL, 
    access_type VARCHAR(50) NOT NULL, 
    ip_address VARCHAR(50), 
    user_agent VARCHAR(500), 
    accessed_at TIMESTAMP WITHOUT TIME ZONE NOT NULL, 
    PRIMARY KEY (id), 
    FOREIGN KEY(secret_id) REFERENCES secrets (id) ON DELETE CASCADE, 
    FOREIGN KEY(user_id) REFERENCES users (id) ON DELETE CASCADE
);

CREATE INDEX ix_secret_access_logs_id ON secret_access_logs (id);

CREATE INDEX ix_secret_access_logs_secret_accessed ON secret_access_logs (secret_id, accessed_at);

CREATE INDEX ix_secret_access_logs_user_accessed ON secret_access_logs (user_id, accessed_at);

CREATE TABLE audit_logs (
    id SERIAL NOT NULL, 
    user_id INTEGER, 
    action VARCHAR(100) NOT NULL, 
    resource_type VARCHAR(100) NOT NULL, 
    resource_id INTEGER, 
    details JSON, 
    ip_address VARCHAR(50), 
    user_agent VARCHAR(500), 
    session_id INTEGER, 
    tool_execution_id INTEGER, 
    created_at TIMESTAMP WITHOUT TIME ZONE NOT NULL, 
    PRIMARY KEY (id), 
    FOREIGN KEY(session_id) REFERENCES sessions (id) ON DELETE SET NULL, 
    FOREIGN KEY(tool_execution_id) REFERENCES tool_executions (id) ON DELETE SET NULL, 
    FOREIGN KEY(user_id) REFERENCES users (id) ON DELETE SET NULL
);

CREATE INDEX ix_audit_logs_id ON audit_logs (id);

CREATE INDEX ix_audit_logs_action_created ON audit_logs (action, created_at);

CREATE INDEX ix_audit_logs_resource ON audit_logs (resource_type, resource_id);

CREATE INDEX ix_audit_logs_user_created ON audit_logs (user_id, created_at);

CREATE TABLE system_metrics (
    id SERIAL NOT NULL, 
    metric_name VARCHAR(100) NOT NULL, 
    metric_value FLOAT NOT NULL, 
    metric_type VARCHAR(50) NOT NULL, 
    tags JSON, 
    recorded_at TIMESTAMP WITHOUT TIME ZONE NOT NULL, 
    PRIMARY KEY (id)
);

CREATE INDEX ix_system_metrics_id ON system_metrics (id);

CREATE INDEX ix_system_metrics_name_recorded ON system_metrics (metric_name, recorded_at);

CREATE TABLE notifications (
    id SERIAL NOT NULL, 
    user_id INTEGER NOT NULL, 
    type VARCHAR(100) NOT NULL, 
    title VARCHAR(500) NOT NULL, 
    message TEXT NOT NULL, 
    data JSON, 
    priority VARCHAR(20) NOT NULL, 
    is_read BOOLEAN NOT NULL, 
    read_at TIMESTAMP WITHOUT TIME ZONE, 
    expires_at TIMESTAMP WITHOUT TIME ZONE, 
    created_at TIMESTAMP WITHOUT TIME ZONE NOT NULL, 
    PRIMARY KEY (id), 
    FOREIGN KEY(user_id) REFERENCES users (id) ON DELETE CASCADE
);

CREATE INDEX ix_notifications_id ON notifications (id);

CREATE INDEX idx_notifications_priority ON notifications (priority, created_at);

CREATE INDEX idx_notifications_type ON notifications (type);

CREATE INDEX idx_notifications_unread ON notifications (user_id, is_read, created_at);

CREATE INDEX idx_notifications_user ON notifications (user_id);

CREATE TABLE notification_preferences (
    id SERIAL NOT NULL, 
    user_id INTEGER NOT NULL, 
    notification_type VARCHAR(100) NOT NULL, 
    enabled BOOLEAN NOT NULL, 
    delivery_method VARCHAR(50) NOT NULL, 
    created_at TIMESTAMP WITHOUT TIME ZONE NOT NULL, 
    updated_at TIMESTAMP WITHOUT TIME ZONE NOT NULL, 
    PRIMARY KEY (id), 
    FOREIGN KEY(user_id) REFERENCES users (id) ON DELETE CASCADE
);

CREATE INDEX ix_notification_preferences_id ON notification_preferences (id);

CREATE UNIQUE INDEX idx_notification_prefs_user_type ON notification_preferences (user_id, notification_type);

INSERT INTO alembic_version (version_num) VALUES ('001') RETURNING alembic_version.version_num;

COMMIT;
</file>

<file path="pyproject.toml">
[project]
name = "cdsa-backend"
version = "0.1.0"
description = "Confidential Data Steward Agent - Backend API"
readme = "README.md"
requires-python = ">=3.11"
dependencies = [
    # Web Framework
    "fastapi>=0.109.0",
    "uvicorn[standard]>=0.27.0",
    "python-multipart>=0.0.6",
    "sse-starlette>=1.8.2",
    
    # Database
    "sqlalchemy>=2.0.25",
    "psycopg2-binary>=2.9.9",
    "alembic>=1.13.1",
    "asyncpg>=0.29.0",
    
    # Redis & Caching
    "redis>=5.0.1",
    "hiredis>=2.3.2",
    
    # Background Tasks
    "celery>=5.3.6",
    
    # Authentication
    "python-jose[cryptography]>=3.3.0",
    "passlib[bcrypt]>=1.7.4",
    
    # AI/ML Framework
    "langchain>=0.1.6",
    "langchain-openai>=0.0.5",
    "langchain-anthropic>=0.0.1",
    "llama-index>=0.9.48",
    "openai>=1.12.0",
    "anthropic>=0.8.1",
    "httpx>=0.25.0",
    
    # Local LLM
    "ollama>=0.1.6",
    
    # Vector Database
    "pgvector>=0.2.4",
    
    # Document Processing (Docling)
    "docling>=2.0.0",
    "docling-core>=2.0.0",
    "pypdf>=4.0.1",
    "python-docx>=1.1.0",
    "python-pptx>=0.6.23",
    "beautifulsoup4>=4.12.3",
    "lxml>=5.1.0",
    
    # Embeddings
    "sentence-transformers>=2.3.1",
    "torch>=2.2.0",
    
    # Data Validation
    "pydantic>=2.6.0",
    "pydantic-settings>=2.1.0",
    "email-validator>=2.1.0",
    
    # Security & Encryption
    "cryptography>=42.0.2",
    "bcrypt>=4.1.2",
    
    # Utilities
    "python-dateutil>=2.8.2",
    "pytz>=2024.1",
    "httpx>=0.26.0",
    "aiofiles>=23.2.1",
    
    # Monitoring & Logging
    "structlog>=24.1.0",
    "python-json-logger>=2.0.7",
    
    # Configuration
    "python-dotenv>=1.0.1",
    "pyyaml>=6.0.1",
]

[project.optional-dependencies]
dev = [
    # Testing
    "pytest>=8.0.0",
    "pytest-asyncio>=0.23.4",
    "pytest-cov>=4.1.0",
    "pytest-mock>=3.12.0",
    
    # Code Quality
    "black>=24.1.1",
    "isort>=5.13.2",
    "flake8>=7.0.0",
    "mypy>=1.8.0",
    "pylint>=3.0.3",
    
    # Debugging
    "ipython>=8.20.0",
    "ipdb>=0.13.13",
]

[build-system]
requires = ["hatchling"]
build-backend = "hatchling.build"

[tool.hatch.build.targets.wheel]
packages = ["app"]

[tool.black]
line-length = 100
target-version = ['py311']
include = '\.pyi?$'

[tool.isort]
profile = "black"
line_length = 100

[tool.pytest.ini_options]
asyncio_mode = "auto"
testpaths = ["tests"]
python_files = ["test_*.py"]
python_classes = ["Test*"]
python_functions = ["test_*"]
addopts = "-v --cov=app --cov-report=term-missing --cov-report=html"

[tool.mypy]
python_version = "3.11"
warn_return_any = true
warn_unused_configs = true
disallow_untyped_defs = true
</file>

<file path="QUICKSTART.md">
# CDSA Backend - Quick Start

##  Backend is Starting!

The backend is currently installing dependencies and starting up.

### What's Happening

1.  Installing FastAPI and dependencies
2.  Starting Uvicorn server on http://localhost:8000
3.  Server should be available in a few seconds...

### Once Started, Test It:

```bash
# Check health
curl http://localhost:8000/health

# View root endpoint
curl http://localhost:8000/

# Open interactive docs
open http://localhost:8000/docs
```

### Access Points

- **API**: http://localhost:8000
- **Swagger Docs**: http://localhost:8000/docs  
- **ReDoc**: http://localhost:8000/redoc
- **Health Check**: http://localhost:8000/health

### What You Can Do Now

1. **Browse API Docs** - Visit http://localhost:8000/docs
2. **Test Health Endpoint** - `curl http://localhost:8000/health`
3. **Check Root Endpoint** - `curl http://localhost:8000/`

### Current Status

 Project structure complete
 Configuration loaded  
 FastAPI application initialized
 CORS middleware configured
 Health check endpoint ready
 Server starting...

### Next Development Steps

Once the server is running and tested:

1. **Add Database** - Set up PostgreSQL with Alembic migrations
2. **Add Authentication** - JWT auth system
3. **Add API Endpoints** - Chat, tools, approvals, notifications
4. **Add Services** - Business logic implementation
5. **Add Tests** - Comprehensive test suite

---

**Current Command Running:**
```bash
python3 -m uvicorn app.main:app --host 0.0.0.0 --port 8000
```

The server will auto-reload on file changes (development mode).
</file>

<file path="README.md">
# CDSA Backend - Confidential Data Steward Agent

Backend API for the Confidential Data Steward Agent, built with FastAPI, PostgreSQL, Redis, and AI/ML frameworks.

##  Quick Start

### Prerequisites

- Docker & Docker Compose
- Python 3.11+ (for local development)
- [uv](https://github.com/astral-sh/uv) package manager (optional, for faster installs)

### Setup with Docker (Recommended)

1. **Clone and navigate to backend:**
```bash
cd backend
```

2. **Copy environment variables:**
```bash
cp .env.example .env
# Edit .env with your API keys and secrets
```

3. **Start services:**
```bash
docker-compose up -d
```

4. **Run database migrations:**
```bash
docker-compose exec api alembic upgrade head
```

5. **Access the API:**
- API: http://localhost:8000
- Documentation: http://localhost:8000/docs
- Health Check: http://localhost:8000/health

### Local Development Setup

1. **Install uv (if not installed):**
```bash
curl -LsSf https://astral.sh/uv/install.sh | sh
```

2. **Create virtual environment and install dependencies:**
```bash
uv venv
source .venv/bin/activate  # On Windows: .venv\Scripts\activate
uv pip install -e ".[dev]"
```

3. **Set up environment variables:**
```bash
cp .env.example .env
# Edit .env file with your configuration
```

4. **Start PostgreSQL and Redis (via Docker):**
```bash
docker-compose up -d postgres redis
```

5. **Run migrations:**
```bash
alembic upgrade head
```

6. **Start development server:**
```bash
uvicorn app.main:app --reload --host 0.0.0.0 --port 8000
```

##  Project Structure

```
backend/
 app/
    api/              # API endpoints
       v1/           # API version 1
    core/             # Core functionality (auth, security, logging)
    models/           # SQLAlchemy models
    schemas/          # Pydantic schemas
    services/         # Business logic
    tools/            # Tool implementations
    db/               # Database configuration
    middleware/       # Custom middleware
    utils/            # Utility functions
 tests/                # Test suite
 alembic/              # Database migrations
 scripts/              # Utility scripts
 docker-compose.yml    # Docker services
 Dockerfile            # Application container
 pyproject.toml        # Python dependencies
```

##  Configuration

### Environment Variables

Key configuration variables (see `.env.example` for full list):

```bash
# Database
DATABASE_URL=postgresql+asyncpg://cdsa:password@localhost:5432/cdsa_db

# Redis
REDIS_URL=redis://localhost:6379/0

# Security
SECRET_KEY=your-secret-key-here
JWT_SECRET_KEY=your-jwt-secret-here
ENCRYPTION_KEY=your-encryption-key-base64

# LLM APIs (optional)
OPENAI_API_KEY=sk-...
ANTHROPIC_API_KEY=sk-ant-...
GEMINI_API_KEY=...

# Local LLM (Ollama)
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_DEFAULT_MODEL=llama3:8b
```

##  Database

### Migrations

```bash
# Create a new migration
alembic revision --autogenerate -m "Description"

# Apply migrations
alembic upgrade head

# Rollback one migration
alembic downgrade -1

# Show current version
alembic current
```

### Schema

The database includes tables for:
- Users & Authentication
- Chat Messages & Sessions
- Tools Registry & Executions
- Approval Workflows
- Audit Events
- Secrets Vault
- Documents & Embeddings
- Notifications
- LLM Models Registry

##  AI/ML Integration

### Supported LLM Providers

1. **OpenAI** (GPT-4, GPT-3.5)
2. **Anthropic** (Claude 3)
3. **Google** (Gemini)
4. **Local Models** (via Ollama)

### Setting up Local Models

```bash
# Pull a model with Ollama
docker-compose exec ollama ollama pull llama3:8b

# List available models
docker-compose exec ollama ollama list
```

##  Security Features

- JWT-based authentication
- Role-Based Access Control (RBAC)
- Field-level encryption for sensitive data
- Complete audit logging
- Rate limiting
- CORS protection
- Request validation

##  Monitoring & Logging

### Logs

```bash
# View application logs
docker-compose logs -f api

# View all service logs
docker-compose logs -f
```

### Health Checks

```bash
# Basic health check
curl http://localhost:8000/health

# Database connection check
curl http://localhost:8000/health/db
```

##  Testing

```bash
# Run all tests
pytest

# Run with coverage
pytest --cov=app --cov-report=html

# Run specific test file
pytest tests/unit/test_auth.py

# Run with verbose output
pytest -v
```

##  Deployment

### Production Build

```bash
# Build production image
docker build -t cdsa-backend:latest .

# Run with production settings
docker-compose -f docker-compose.prod.yml up -d
```

### Environment-Specific Configs

- Development: `.env`
- Production: `.env.production`
- Staging: `.env.staging`

##  API Documentation

### Interactive Documentation

- Swagger UI: http://localhost:8000/docs
- ReDoc: http://localhost:8000/redoc

### API Endpoints

#### Authentication
- `POST /api/v1/auth/register` - User registration
- `POST /api/v1/auth/login` - User login
- `GET /api/v1/auth/me` - Get current user

#### Chat
- `POST /api/v1/chat/stream` - Streaming chat (SSE)
- `POST /api/v1/chat/continue` - Continue after approval

#### Notifications
- `GET /api/v1/notifications/stream` - Real-time notifications (SSE)
- `GET /api/v1/notifications` - Get notification history

#### Tools
- `GET /api/v1/tools` - List available tools
- `POST /api/v1/tools/{id}/execute` - Execute tool

##  Development

### Code Quality

```bash
# Format code
black app tests

# Sort imports
isort app tests

# Lint code
flake8 app tests

# Type checking
mypy app
```

### Adding a New Tool

1. Create tool class in `app/tools/`
2. Inherit from `BaseTool`
3. Implement `execute()` method
4. Register in tool registry
5. Add tests

### Adding a New API Endpoint

1. Create router in `app/api/v1/`
2. Define schemas in `app/schemas/`
3. Implement business logic in `app/services/`
4. Add tests
5. Register router in `app/main.py`

##  Troubleshooting

### Common Issues

**Database connection error:**
```bash
# Check if PostgreSQL is running
docker-compose ps postgres

# View PostgreSQL logs
docker-compose logs postgres
```

**Redis connection error:**
```bash
# Check if Redis is running
docker-compose ps redis

# Test Redis connection
docker-compose exec redis redis-cli ping
```

**Migration errors:**
```bash
# Reset database (WARNING: deletes all data)
alembic downgrade base
alembic upgrade head
```

##  Additional Resources

- [FastAPI Documentation](https://fastapi.tiangolo.com/)
- [SQLAlchemy Documentation](https://docs.sqlalchemy.org/)
- [LangChain Documentation](https://python.langchain.com/)
- [Architecture Plan](../backend-architecture-plan.md)
- [Notification System](../NOTIFICATION_SYSTEM_ENHANCEMENT.md)

##  Contributing

1. Create a feature branch
2. Make your changes
3. Add tests
4. Run code quality checks
5. Submit a pull request

##  License

Proprietary - All rights reserved

##  Team

- Architecture: AI Architect
- Development: [Your Team]

---

**Version:** 1.0.0  
**Last Updated:** 2025-11-12
</file>

<file path="run.sh">
#!/bin/bash
# Simple startup script for CDSA Backend

echo " Starting CDSA Backend..."
echo ""

# Check if .env exists
if [ ! -f .env ]; then
    echo " Creating .env file from template..."
    cp .env.example .env
    echo " .env created - please update with your API keys if needed"
    echo ""
fi

# Check if running in Docker or locally
if command -v docker &> /dev/null && docker info &> /dev/null; then
    echo " Docker is available"
    echo "Starting services with Docker Compose..."
    echo ""
    docker-compose up -d
    echo ""
    echo " Services started!"
    echo ""
    echo " Access points:"
    echo "   API:      http://localhost:8000"
    echo "   Docs:     http://localhost:8000/docs"
    echo "   Health:   http://localhost:8000/health"
    echo ""
    echo " View logs: docker-compose logs -f api"
    echo " Stop: docker-compose down"
else
    echo "  Docker not available, trying local Python..."
    echo ""
    
    # Install minimal dependencies if needed
    python3 -m pip install fastapi uvicorn[standard] pydantic pydantic-settings structlog python-dotenv --quiet --break-system-packages 2>/dev/null || \
    python3 -m pip install fastapi uvicorn[standard] pydantic pydantic-settings structlog python-dotenv --quiet --user
    
    echo " Starting with uvicorn..."
    echo ""
    python3 -m uvicorn app.main:app --host 0.0.0.0 --port 8000 --reload
fi
</file>

<file path="test_bcrypt_direct.py">
"""Direct bcrypt test to isolate the issue."""
from passlib.context import CryptContext

pwd_context = CryptContext(schemes=["bcrypt"], deprecated="auto")

# The actual hash from the database
stored_hash = "$2b$12$ms4TLSnVueN0WVpXQ3vem.Yo0IGyL38K3qD.JgJitK/XmpPdESfMK"
password = "admin123"

print(f"Password: {password}")
print(f"Password length: {len(password)}")
print(f"Hash: {stored_hash}")
print(f"Hash length: {len(stored_hash)}")

try:
    result = pwd_context.verify(password, stored_hash)
    print(f" Verification result: {result}")
except Exception as e:
    print(f" Error: {e}")
    import traceback
    traceback.print_exc()
</file>

<file path="test_login.py">
"""Test login endpoint with detailed error handling."""
import asyncio
import sys
from sqlalchemy.ext.asyncio import AsyncSession
from sqlalchemy import select, or_
from sqlalchemy.orm import selectinload

# Add parent directory to path
sys.path.insert(0, '/Users/charleshoward/Applications/Secure App/backend')

from app.db.base import init_db
from app.models.user import User
from app.core.security import verify_password, create_access_token, create_refresh_token

async def test_login():
    """Test the login process step by step."""
    print("=" * 60)
    print("Testing Login Process")
    print("=" * 60)
    
    # Initialize database
    print("\nInitializing database connection...")
    engine, AsyncSessionLocal = init_db()
    print(" Database initialized")
    
    username = "admin"
    password = "admin123"
    
    async with AsyncSessionLocal() as db:
        try:
            print(f"\n1. Looking up user: {username}")
            result = await db.execute(
                select(User)
                .options(selectinload(User.roles))
                .where(
                    or_(User.username == username, User.email == username)
                )
            )
            user = result.scalar_one_or_none()
            
            if not user:
                print(f" User not found: {username}")
                return
            
            print(f" User found: {user.email}")
            print(f"  - ID: {user.id}")
            print(f"  - Username: {user.username}")
            print(f"  - Is Active: {user.is_active}")
            print(f"  - Is Superuser: {user.is_superuser}")
            
            print(f"\n2. Verifying password...")
            password_valid = verify_password(password, user.hashed_password)
            if not password_valid:
                print(" Invalid password")
                return
            print(" Password verified")
            
            print(f"\n3. Checking if user is active...")
            if not user.is_active:
                print(" User is not active")
                return
            print(" User is active")
            
            print(f"\n4. Getting user ID...")
            user_id = user.id
            print(f" User ID: {user_id}")
            
            print(f"\n5. Creating access token...")
            access_token = create_access_token(subject=user_id)
            print(f" Access token created: {access_token[:50]}...")
            
            print(f"\n6. Creating refresh token...")
            refresh_token = create_refresh_token(subject=user_id)
            print(f" Refresh token created: {refresh_token[:50]}...")
            
            print("\n" + "=" * 60)
            print(" LOGIN TEST SUCCESSFUL!")
            print("=" * 60)
            print(f"\nAccess Token: {access_token}")
            print(f"\nRefresh Token: {refresh_token}")
            
        except Exception as e:
            print(f"\n ERROR: {str(e)}")
            import traceback
            print("\nFull traceback:")
            traceback.print_exc()

if __name__ == "__main__":
    asyncio.run(test_login())
</file>

<file path="test_server.py">
#!/usr/bin/env python3
"""Quick server verification script"""
import sys
try:
    import requests
    
    print("Testing CDSA Backend Server on http://localhost:8001")
    print("=" * 50)
    
    # Test health endpoint
    try:
        r = requests.get('http://localhost:8001/health', timeout=5)
        print(f"\n Health Check: Status {r.status_code}")
        print(f"   Response: {r.json()}")
    except Exception as e:
        print(f"\n Health Check Failed: {e}")
        sys.exit(1)
    
    # Test root endpoint
    try:
        r = requests.get('http://localhost:8001/', timeout=5)
        print(f"\n Root Endpoint: Status {r.status_code}")
        print(f"   Response: {r.json()}")
    except Exception as e:
        print(f"\n Root Endpoint Failed: {e}")
    
    # Test docs
    try:
        r = requests.get('http://localhost:8001/docs', timeout=5)
        print(f"\n API Docs: Status {r.status_code}")
        print(f"   Available at: http://localhost:8001/docs")
    except Exception as e:
        print(f"\n API Docs Failed: {e}")
    
    print("\n" + "=" * 50)
    print(" Server is running successfully!")
    print("\nAccess the API at:")
    print("  - Health: http://localhost:8001/health")
    print("  - Root: http://localhost:8001/")
    print("  - Docs: http://localhost:8001/docs")
    print("  - ReDoc: http://localhost:8001/redoc")
    
except ImportError:
    print("Installing requests library...")
    import subprocess
    subprocess.run([sys.executable, "-m", "pip", "install", "requests", "-q", "--user"])
    print("Please run this script again")
    sys.exit(1)
</file>

</files>
